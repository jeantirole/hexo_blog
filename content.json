{"pages":[],"posts":[{"title":"Cos Pro Python 1급 1차 01번 문제풀이","text":"Contents of table: 문제내용 정답코드 Topic:배달음식점 주문메뉴 만들기 Description:파이썬의 전형적인 메뉴판 만들기 문제를, 클래스를 활용해서 작성할 수 있는가를테스트 하는 문제이다. 해당 문제는 특히 추상클래스에 대한 이해 여부가 포인트이다.논리구조는 특별할 것이 없다. 문제내용 DeliveryStore : DeliveryStore는 배달 음식점의 인터페이스입니다. 배달 음식점은 set_order_list와 get_total_price 함수를 구현해야 합니다. set_order_list 함수는 주문 메뉴의 리스트를 매개변수로 받아 저장합니다. get_total_price 함수는 주문받은 음식 가격의 총합을 return 합니다. Food : Food는 음식을 나타내는 클래스입니다. 음식은 이름(name)과 가격(price)으로 구성되어있습니다. PizzaStore PizzaStore는 피자 배달 전문점을 나타내는 클래스이며 DeliveryStore 인터페이스를 구현합니다. menu_list는 피자 배달 전문점에서 주문 할 수 있는 음식의 리스트를 저장합니다. order_list는 주문 받은 음식들의 이름을 저장합니다. set_order_list 함수는 주문 메뉴를 받아 order_list에 저장합니다. get_total_price 함수는 order_list에 들어있는 음식 가격의 총합을 return 합니다. 주문 메뉴가 들어있는 리스트 order_list가 매개변수로 주어질 때, 주문한 메뉴의 전체 가격을 return 하도록 solution 함수를 작성하려고 합니다. 위의 클래스 구조를 참고하여 주어진 코드의 빈칸을 적절히 채워 전체 코드를 완성해주세요. 매개변수 설명주문 메뉴가 들어있는 리스트 order_list가 solution 함수의 매개변수로 주어집니다. order_list의 길이는 1 이상 5이하입니댜. order_list에는 주문하려는 메뉴의 이름들이 문자열 형태로 들어있습니다. order_list에는 같은 메뉴의 이름이 중복해서 들어있지 않습니다. 메뉴의 이름과 가격은 PizzaStore의 생성자에서 초기화해줍니다. return 값 설명주문한 메뉴의 전체 가격을 return 해주세요. 예시 order_list return [“Cheese”, “Pineapple”, “Meatball”] 51600 정답코드 1from abc import * # 추상클래스 import 12345678class DeliveryStore(metaclass=ABCMeta): # 추상클래스에 앞으로 만들 함수목록 지정 @abstractmethod def set_order_list(self, order_list): pass @abstractmethod def get_total_price(self): pass 1234class Food: # name, price 변수를 만들어줄 class 작성 def __init__(self, name, price): self.name = name self.price = price 123456789101112131415161718192021class PizzaStore(DeliveryStore): # (name, price)가 리스트의 요소로 이루어진 menu_list def __init__(self): menu_names = [&quot;Cheese&quot;, &quot;Potato&quot;, &quot;Shrimp&quot;, &quot;Pineapple&quot;, &quot;Meatball&quot;] menu_prices = [11100, 12600, 13300, 21000, 19500]; self.menu_list = [] for i in range(5): self.menu_list.append(Food(menu_names[i], menu_prices[i])) self.order_list = [] def set_order_list(self, order_list): # order_list 변수작성 for order in order_list: self.order_list.append(order) def get_total_price(self): # menu_list의 name변수와 order_list의 요소를 비교하여 동일 요소일 경우, price 값을 total_price에 누적합 return total_price = 0 for order in self.order_list: for menu in self.menu_list: if order == menu.name: total_price += menu.price return total_price 123456def solution(order_list): #order_list를 넣었을 때, total price가 나올 수 있도록 마무리 delivery_store = PizzaStore() delivery_store.set_order_list(order_list) total_price = delivery_store.get_total_price() return total_price","link":"/2020/09/18/1%EA%B8%89-1%EC%B0%A8-01%EB%B2%88/"},{"title":"Cos Pro Python 1급 1차 01번 문제풀이","text":"Contents of table: 문제내용 정답코드 Topic:배달음식점 주문메뉴 만들기 Description:파이썬의 전형적인 메뉴판 만들기 문제를, 클래스를 활용해서 작성할 수 있는가를테스트 하는 문제이다. 해당 문제는 특히 추상클래스에 대한 이해 여부가 포인트이다.논리구조는 특별할 것이 없다. 문제내용 DeliveryStore : DeliveryStore는 배달 음식점의 인터페이스입니다. 배달 음식점은 set_order_list와 get_total_price 함수를 구현해야 합니다. set_order_list 함수는 주문 메뉴의 리스트를 매개변수로 받아 저장합니다. get_total_price 함수는 주문받은 음식 가격의 총합을 return 합니다. Food : Food는 음식을 나타내는 클래스입니다. 음식은 이름(name)과 가격(price)으로 구성되어있습니다. PizzaStore PizzaStore는 피자 배달 전문점을 나타내는 클래스이며 DeliveryStore 인터페이스를 구현합니다. menu_list는 피자 배달 전문점에서 주문 할 수 있는 음식의 리스트를 저장합니다. order_list는 주문 받은 음식들의 이름을 저장합니다. set_order_list 함수는 주문 메뉴를 받아 order_list에 저장합니다. get_total_price 함수는 order_list에 들어있는 음식 가격의 총합을 return 합니다. 주문 메뉴가 들어있는 리스트 order_list가 매개변수로 주어질 때, 주문한 메뉴의 전체 가격을 return 하도록 solution 함수를 작성하려고 합니다. 위의 클래스 구조를 참고하여 주어진 코드의 빈칸을 적절히 채워 전체 코드를 완성해주세요. 매개변수 설명주문 메뉴가 들어있는 리스트 order_list가 solution 함수의 매개변수로 주어집니다. order_list의 길이는 1 이상 5이하입니댜. order_list에는 주문하려는 메뉴의 이름들이 문자열 형태로 들어있습니다. order_list에는 같은 메뉴의 이름이 중복해서 들어있지 않습니다. 메뉴의 이름과 가격은 PizzaStore의 생성자에서 초기화해줍니다. return 값 설명주문한 메뉴의 전체 가격을 return 해주세요. 예시 order_list return [“Cheese”, “Pineapple”, “Meatball”] 51600 정답코드 1from abc import * # 추상클래스 import 12345678class DeliveryStore(metaclass=ABCMeta): # 추상클래스에 앞으로 만들 함수목록 지정 @abstractmethod def set_order_list(self, order_list): pass @abstractmethod def get_total_price(self): pass 1234class Food: # name, price 변수를 만들어줄 class 작성 def __init__(self, name, price): self.name = name self.price = price 123456789101112131415161718192021class PizzaStore(DeliveryStore): # (name, price)가 리스트의 요소로 이루어진 menu_list def __init__(self): menu_names = [&quot;Cheese&quot;, &quot;Potato&quot;, &quot;Shrimp&quot;, &quot;Pineapple&quot;, &quot;Meatball&quot;] menu_prices = [11100, 12600, 13300, 21000, 19500]; self.menu_list = [] for i in range(5): self.menu_list.append(Food(menu_names[i], menu_prices[i])) self.order_list = [] def set_order_list(self, order_list): # order_list 변수작성 for order in order_list: self.order_list.append(order) def get_total_price(self): # menu_list의 name변수와 order_list의 요소를 비교하여 동일 요소일 경우, price 값을 total_price에 누적합 return total_price = 0 for order in self.order_list: for menu in self.menu_list: if order == menu.name: total_price += menu.price return total_price 123456def solution(order_list): #order_list를 넣었을 때, total price가 나올 수 있도록 마무리 delivery_store = PizzaStore() delivery_store.set_order_list(order_list) total_price = delivery_store.get_total_price() return total_price","link":"/2020/09/17/1%EA%B8%89-1%EC%B0%A8-01%EB%B2%88_dummy/"},{"title":"Cos Pro Python 1급 1차 02번 문제풀이","text":"Contents of table: 문제내용 정답코드 Topic:해밍거리 구하기 Review:주어진 두 개의 문자열 리스트의 길이를 활용하고 인덱스값을 이용해서 두 리스트의 요소들을 비교할 수 있는가?라는 것을 묻는 문제이다. 문제내용#문제2해밍 거리(Hamming distance)란 같은 길이를 가진 두 개의 문자열에서 같은 위치에 있지만 서로 다른 문자의 개수를 뜻합니다. 예를 들어 두 2진수 문자열이 “10010”과 “110”이라면, 먼저 두 문자열의 자릿수를 맞추기 위해 “110”의 앞에 0 두개를 채워 “00110”으로 만들어 줍니다. 두 2진수 문자열은 첫 번째와 세 번째 문자가 서로 다르므로 해밍 거리는 2입니다. 1001 0 0011 0 두 2진수 문자열 binaryA, binaryB의 해밍 거리를 구하려 합니다. 이를 위해 다음과 같이 간단히 프로그램 구조를 작성했습니다 12341단계. 길이가 더 긴 2진수 문자열의 길이를 구합니다.2단계. 첫 번째 2진수 문자열의 길이가 더 짧다면 문자열의 앞에 0을 채워넣어 길이를 맞춰줍니다.3단계. 두 번째 2진수 문자열의 길이가 더 짧다면 문자열의 앞에 0을 채워넣어 길이를 맞춰줍니다.4단계. 길이가 같은 두 2진수 문자열의 해밍 거리를 구합니다. 두 2진수 문자열 binaryA와 binaryB가 매개변수로 주어질 때, 두 2진수의 해밍 거리를 return 하도록 solution 함수를 작성했습니다. 이때, 위 구조를 참고하여 중복되는 부분은 func_a라는 함수로 작성했습니다. 코드가 올바르게 동작할 수 있도록 빈칸을 알맞게 채워 전체 코드를 완성해주세요. 매개변수 설명두 2진수 문자열 binaryA와 binaryB가 solution 함수의 매개변수로 주어집니다. binaryA의 길이는 1 이상 10 이하입니다. binaryA는 0 또는 1로만 이루어진 문자열이며, 0으로 시작하지 않습니다. binaryB의 길이는 1 이상 10 이하입니다. binaryB는 0 또는 1로만 이루어진 문자열이며, 0으로 시작하지 않습니다. return 값 설명두 2진수 문자열의 해밍 거리를 return 해주세요. 예시 binaryA binaryB return “10010” “110” 2 예시 설명두 2진수의 자릿수는 각각 5와 3입니다. 자릿수를 맞추기 위해 “110” 앞에 0 두 개를 채워주면 “00110”이 됩니다. 이제 두 2진수 문자열의 해밍 거리를 구하면 다음과 같습니다. 1001 0 0011 0 위와 같이 첫 번째와 세 번째 문자가 서로 다르므로, 해밍 거리는 2가 됩니다. 정답코드123456def func_a(bstr, bstr_len): # 문자열의 부족한 길이만큼의 0문자열을 추가하다 padZero = &quot;&quot; #빈 문자열 padSize = bstr_len - len(bstr) #매개변수로 주어진 문자열과, 주어진 문자열 만큼의 차이 for i in range(padSize): #위의 차이 만큼의 문자열 &quot;0&quot;을 빈 문자열 안에 넣어준다. padZero += &quot;0&quot; return padZero + bstr # 변수로 주어진 문자열에, &quot;0&quot;으로 채워진 문자열을 추가한다. 123456789101112def solution(binaryA, binaryB): max_length = max(len(binaryA), len(binaryB)) #두 문자열의 길이 중 더 큰 것을 고른다. if max_length &gt; len(binaryA): #binaryA가 더 작을 경우에, func_a를 이용해서 &quot;0&quot; 문자열을 추가해서 반환해준다. binaryA = func_a(binaryA, max_length) if max_length &gt; len(binaryB): #binaryB가 더 작을 경우. 위와동일. binaryB = fucn_a(binaryB, max_length) hamming_distance = 0 for i in range(max_length): #두 매개변수의 요소들을 훑기 위한 for문. if binaryA[i] != binaryB[i]: hamming_distance += 1 #해밍거리 count return hamming_distance","link":"/2020/09/19/1%EA%B8%89-1%EC%B0%A8-02%EB%B2%88/"},{"title":"Cos Pro Python 1급 1차 03번 문제풀이","text":"Contents of table: 문제내용 정답코드 Topic:계산기 by 문자열 Review:‘enumerate()’ 함수를 적극적으로 활용함. 개인적으로 func_c의 slicing 아이디어가 좋았다. 문제내용문자열 형태의 식을 계산하려 합니다. 식은 2개의 자연수와 1개의 연산자(‘+’, ‘-‘, ‘*’ 중 하나)로 이루어져 있습니다. 예를 들어 주어진 식이 “123+12”라면 이를 계산한 결과는 135입니다. 문자열로 이루어진 식을 계산하기 위해 다음과 같이 간단히 프로그램 구조를 작성했습니다. 1단계. 주어진 식에서 연산자의 위치를 찾습니다.2단계. 연산자의 앞과 뒤에 있는 문자열을 각각 숫자로 변환합니다.3단계. 주어진 연산자에 맞게 연산을 수행합니다. 문자열 형태의 식 expression이 매개변수로 주어질 때, 식을 계산한 결과를 return 하도록 solution 함수를 작성하려 합니다. 위 구조를 참고하여 코드가 올바르게 동작할 수 있도록 빈칸에 주어진 func_a, func_b, func_c 함수와 매개변수를 알맞게 채워주세요. 매개변수 설명문자열 형태의 식 expression이 solution 함수의 매개변수로 주어집니다. expression은 연산자 1개와 숫자 2개가 결합한 형태입니다. 연산자는 ‘+’, ‘-‘, ‘*’만 사용됩니다. 숫자는 1 이상 10,000 이하의 자연수입니다. return 값 설명expression을 계산한 결과를 return 해주세요. 계산 결과는 문자열로 변환하지 않아도 됩니다. 예시 expression return “123+12” 135 예시 설명‘+’를 기준으로 앞의 숫자는 123이고 뒤의 숫자는 12이므로 두 숫자를 더하면 135가 됩니다. 정답코드12345678910111213def func_a(numA, numB, exp): #주어진 문자열에서 사칙연산 기호를 찾아서, 해당 연산실행 if exp == '+': return numA + numB elif exp == '-': return numA - numB elif exp == '*': return numA * numB~~~ ~~~pythondef func_b(exp): #리스트 변수를 idx 와 val로 나누고, 연산기호의 idx 값을 return for index, value in enumerate(exp): if value == '+' or value == '-' or value == '*': return index 1234def func_c(exp, idx): #주어진 연산자의 앞과 뒤의 문자열을 숫자로 반환 numA = int(exp[:idx]) numB = int(exp[idx + 1:]) return numA, numB 12345def solution(expression): exp_index = func_b(expression) #연산 기호의 idx 값을 exp_index 변수에 저장 first_num, second_num = func_c(expression, exp_index) result = func_a(first_num, second_num, expression[exp_index]) return result","link":"/2020/09/20/1%EA%B8%89-1%EC%B0%A8-03%EB%B2%88/"},{"title":"Cos Pro Python 1급 1차 04번 문제풀이","text":"Contents of table: 문제내용 정답코드 Topic:타임머신 Review:#point 1) 연산자의 활용능력. 기호 //, % 을 활용할 수 있는가.#point 2) 숫자에 0을 포함하지 않도록 하는 로직. 정답코드에서는 num // digit % 10 == 0 으로 판단기준을 잡았다. 0이 포함된 숫자는 10의 배수라는 아이디어를 기본으로 num +=1, digit *= 10 을 반복하며 각 자릿수마다 0 이 없도록 while 문을 반복시킴. 문제내용어느 누군가가 타임머신을 타고 과거로 가서 숫자 0이 없는 수 체계를 전파했습니다. 역사가 바뀌어 이제 사람들의 의식 속엔 0이란 숫자가 사라졌습니다. 따라서, 현재의 수 체계는 1, 2, 3, …, 8, 9, 11, 12, …와 같이 0이 없게 바뀌었습니다. 0을 포함하지 않은 자연수 num이 매개변수로 주어질 때, 이 수에 1을 더한 수를 return 하도록 solution 함수를 완성해주세요. 매개변수 설명자연수 num이 solution 함수의 매개변수로 주어집니다. num은 1 이상 999,999,999,999,999,999 이하의 0을 포함하지 않는 자연수입니다. return 값 설명자연수 num에 1을 더한 수를 return 해주세요. 예시 num return 9949999 9951111 예시 설명9,949,999에 1을 더하면 9,950,000이지만 0은 존재하지 않으므로 9,951,111이 됩니다. 정답코드 def solution(num): num += 1 #100을 넣었다고 가정해 볼 때, 101 즉 끝자리 수의 0을 1로 바꿔준다. digit = 1 #임의의 digit 값, 아래 함수 참조하면, 1 =&gt; 10 =&gt; 100, 10의 거듭제곱 형태. while num // digit % 10 == 0: # num 값이 10의 배수라고 가정 했을 때, num += digit digit *= 10 return num ~~~python ## **부가설명** num에 99라는 값을 임의로 넣었다고 가정했을 때, 아래와 같은 연산이 진행이 된다. num = 99 digit =1 num += 1 =&gt; num = 100 #99라는 숫자에 1을 더했을 때, 10의 배수가 되는데, 10의 배수가 되지 않도록 만드는 것이 목표. num // digit % 10 == 0 (true) #100은 10의 배수 이므로 (num//digit) 값의 나머지가 0이 된다. 100 + 1 = 101 = num #100에서 바꾸어야 할 숫자는 10**0자리와 10**1자리 둘. 두 개중 하나가 해결되었다. 1 * 10 = 10 = digit #10**0의 자리수가 해결이 되었으므로 10**1자리로 포인트 이동. 101 // 10 % 10 == 0 (true) #101은 여전히 10의 배수, 몫의 정수 부분이 10이다. 101 + 10 = 111 = num #한 자리 수 올라간 digit 부분이 10*1 부분에 1을 삽입. 10 * 10 = 100 = digit 111 // 10 % 10 == 0 (false) #루프 break","link":"/2020/09/21/1%EA%B8%89-1%EC%B0%A8-04%EB%B2%88/"},{"title":"Cos Pro Python 1급 1차 05번 문제풀이","text":"Contents of table: 문제내용 정답코드 Topic:소용돌이 수 Review:유명한 소용돌이 알고리즘 문제이다. 2차원 리스트 안에서, 원하는 대로 point 이동 알고리즘을 짤 수 있는가를 테스트하는 문제이다. 정답코드 12def in_range(i, j, n): # i와 j값이 0보다크고 n보다 작은지 확인 return 0 &lt;= i and i &lt; n and 0 &lt;= j and j &lt; n 12345678910111213141516171819202122def solution(n): pane = [[0 for j in range(n)] for i in range(n)] # n * n 크기의 panel 을 만든다. dy = [0, 1, 0, -1] # 2차원 리스트 panel 에서, (dy,dx)의 움직임은, (right, down, left, up)을 의미한다. dx = [1, 0, -1, 0] ci, cj = 0, 0 # 시작 포인트는 (0,0) num = 1 while in_range(ci, cj, n) and pane[ci][cj] == 0: # 1) in_range 함수를 활용, current point가 panel을 벗어나지 않음. 2) 이동하는 칸에 숫자가 채워져 있지 않음. for k in range(4): # dy, dx의 이동순서 if not in_range(ci, cj, n) or pane[ci][cj] != 0: break while True: # 1) pannel 범위안 2) 다음칸의 숫자가 비어있을 때, 무한루프 pane[ci][cj] = num # k = 0 일 때, panel의 범위 안에서 num +=1 만큼의 숫자를 각 칸에 입력 num += 1 ni = ci + dy[k] # next point는 current point 에서 (dy,dx) 만큼의 이동값을 가져간 결과이다. nj = cj + dx[k] if not in_range(ni, nj, n) or pane[ni][nj] != 0: # next point 가 pannel 을 벗어났을 경우, (down, left, up, right) 순서로 이동. ci += dy[(k + 1) % 4] # (k+1) % 4 는 1 2 3 0 cj += dx[(k + 1) % 4] break ci = ni cj = nj return pane 문제내용#문제5다음과 같이 n x n 크기의 격자에 1부터 n x n까지의 수가 하나씩 있습니다. 이때 수가 다음과 같은 순서로 배치되어있다면 이것을 n-소용돌이 수라고 부릅니다. 소용돌이 수에서 1행 1열부터 n 행 n 열까지 대각선상에 존재하는 수들의 합을 구해야 합니다. 위의 예에서 대각선상에 존재하는 수의 합은 15입니다.격자의 크기 n이 주어질 때 n-소용돌이 수의 대각선상에 존재하는 수들의 합을 return 하도록 solution 함수를 완성해주세요. 매개변수 설명격자의 크기 n이 solution 함수의 매개변수로 주어집니다. n은 1 이상 100 이하의 자연수입니다. return 값 설명n-소용돌이 수의 대각선상에 존재하는 수들의 합을 return 해주세요. 예시 n return 3 15 2 4 예시 설명예시 #1문제의 예와 같습니다. 예시 #21과 3을 더하여 4가 됩니다.","link":"/2020/09/21/1%EA%B8%89-1%EC%B0%A8-05%EB%B2%88/"},{"title":"Cos Pro Python 1급 1차 06번 문제풀이","text":"Contents of table: 문제내용 정답코드 Topic:체스의 나이트 Review:2차원 panel 위에서 포인트 이동문제.소용돌이 알고리즘의 해결방법과 유사하게, direction 리스트를 정하고, point 를 정해진 조건에따라서 이동시켜서 결과값을 얻어낸다. 정답코드 12345678910111213def solution(pos): dx = [1,1,-1,-1,2,2,-2,-2] #체스말이 움직이는 거리와 방향을 나타낸다. dy = [2,-2,-2,2,1,-1,-1,1] #ex) (x,y) =&gt; (x+dx, y+dy) cx = ord(pos[0]) - ord(&quot;A&quot;) #ord()함수는 문자열의 unicode 값을 리턴해준다. #체스판 구조상, A와 1을 기준으로 잡고, unicode의 차이만큼을 current position으로 보겠다는 의미. cy = ord(pos[1]) - ord(&quot;0&quot;) - 1 ans = 0 for i in range(8): #나이트가 움직일 수 있는 전방위는 8가지이다. nx = cx + dx[i] #ex) (nx,ny) =&gt; (x+dx, y+dy) ny = cy + dy[i] if nx &gt;= 0 and nx &lt; 8 and ny &gt;= 0 and ny &lt; 8: # 0 &lt;= x,y &lt; 8 이어야지 체스판을 벗어나지 않는다. ans += 1 # nx, ny 가 체스판을 벗어나지 않을 경우, ans 변수에 카운트누적. return ans 문제내용체스에서 나이트(knight)는 아래 그림과 같이 동그라미로 표시된 8개의 방향중 한 곳으로 한 번에 이동이 가능합니다. 단, 나이트는 체스판 밖으로는 이동할 수 없습니다. 체스판의 각 칸의 위치는 다음과 같이 표기합니다.예를 들어, A번줄과 1번줄이 겹치는 부분은 ‘A1’이라고 합니다. 나이트의 위치 pos가 매개변수로 주어질 때, 나이트를 한 번 움직여서 이동할 수 있는 칸은 몇개인지 return 하도록 solution 함수를 완성해주세요. 매개변수 설명나이트의 위치 pos가 solution 함수의 매개변수로 주어집니다. pos는 A부터 H까지의 대문자 알파벳 하나와 1 이상 8이하의 정수 하나로 이루어진 두 글자 문자열입니다. 잘못된 위치가 주어지는 경우는 없습니다. return 값 설명나이트를 한 번 움직여서 이동할 수 있는 칸의 개수를 return 해주세요. 예시 pos return “A7” 3 예시 설명나이트가 A7 위치에 있으면 아래 그림과 같이 왼쪽으로는 이동하지 못하고, 오른쪽으로는 맨 위를 제외한 나머지 세 칸으로 이동 가능합니다.따라서, 3을 return 하면 됩니다.","link":"/2020/09/21/1%EA%B8%89-1%EC%B0%A8-06%EB%B2%88/"},{"title":"Cos Pro Python 1급 1차 09번 문제풀이","text":"Contents of table: 문제내용 정답코드 Topic: 계단 게임 Review:잘못된 한 줄 코드를 찾아서 수정하는 형태의 문제. 이런 경우는 대입으로 잘못된 코드를 찾아내는 것이 가장 빠르다.A,B 가 비기는 케이스 ex) (0,0) 일 때, cnt -=1 이 되면은 안되므로 max(0, cnt-1) 로 고쳐준다. 정답코드 1234567891011121314151617def func(record): if record == 0: return 1 elif record == 1: return 2 return 0def solution(recordA, recordB): cnt = 0 for i in range(len(recordA)): if recordA[i] == recordB[i]: continue elif recordA[i] == func(recordB[i]): cnt += 3 else: cnt = max(0, cnt - 1) return cnt 문제내용#문제9두 학생 A와 B는 계단 게임을 하였습니다.계단 게임의 규칙은 아래와 같습니다. 계단 제일 아래에서 게임을 시작합니다. (0번째 칸) 가위바위보를 합니다. 이기면 계단 세 칸을 올라가고, 지면 한 칸을 내려가고, 비기면 제자리에 있습니다. 계단 제일 아래에서 지면 제자리에 있습니다. 2~4 과정을 열 번 반복합니다. A와 B가 계단 게임을 완료한 후에, A가 계단 위 몇 번째 칸에 있는지 파악하려고 합니다. A와 B가 낸 가위바위보 기록이 순서대로 들어있는 리스트 recordA와 recordB가 매개변수로 주어질 때, 게임을 마친 후의 A의 위치를 return 하도록 solution 함수를 작성했습니다. 그러나, 코드 일부분이 잘못되어있기 때문에, 몇몇 입력에 대해서는 올바르게 동작하지 않습니다. 주어진 코드에서 _한 줄_만 변경해서 모든 입력에 대해 올바르게 동작하도록 수정하세요. 매개변수 설명A와 B가 낸 가위바위보 기록이 순서대로 들어있는 리스트 recordA와 recordB가 매개변수로 주어집니다. recordA와 recordB의 원소는 0, 1, 2중 하나이고 순서대로 가위, 바위, 보를 의미합니다. recordA와 recordB의 길이는 10입니다. return 값 설명solution 함수는 계단 게임을 마친 후에 A가 계단 위 몇 번째 칸에 위치하는지를 return 합니다. 계단 제일 아래 칸은 0번째 칸입니다. 예시 recordA recordB return [2,0,0,0,0,0,1,1,0,0] [0,0,0,0,2,2,0,2,2,2] 14 예시 설명 recordA 보 가위 가위 가위 가위 가위 바위 바위 가위 가위 recordB 가위 가위 가위 가위 보 보 가위 보 보 보 result 0 0 0 0 +3 +6 +9 +8 +11 +14","link":"/2020/09/27/1%EA%B8%89-1%EC%B0%A8-09%EB%B2%88/"},{"title":"Cos Pro Python 1급 2차 01번 문제풀이","text":"Contents of table: 문제내용 정답코드 Topic: 주식으로 최대 수익을 내세요 Review: 정답코드 1234567891011121314151617181920212223242526272829class ComicBook(): : cost = 500 day -= 2 if day &gt; 0: cost += return costclass Novel(): : cost = 1000 day -= 3 if day &gt; 0: cost += return costdef solution(book_types, day): books = [] for types in book_types: if types == &quot;comic&quot;: books.append(ComicBook()) elif types == &quot;novel&quot;: books.append(Novel()) total_price = 0 for book in books: total_price += book.get_rental_price(day) return total_price ### **문제내용** ### **문제10** 지난 연속된 n일 동안의 주식 가격이 순서대로 들어있는 리스트가 있습니다. 이때, 다음 규칙에 따라 주식을 사고 팔았을 때의 최대 수익을 구하려 합니다. * n일 동안 주식을 단 한 번 살 수 있습니다. * n일 동안 주식을 단 한 번 팔 수 있습니다. * 주식을 산 날에 바로 팔 수는 없으며, 최소 하루가 지나야 팔 수 있습니다. * 적어도 한 번은 주식을 사야하며, 한 번은 팔아야 합니다. 주식을 팔 때는 반드시 이전에 주식을 샀어야 하며, 최대 수익은 양수가 아닐 수도 있습니다. 연속된 n 일 동안의 주식 가격이 순서대로 들어있는 리스트 prices가 매개변수로 주어질 때, 주식을 규칙에 맞게 한 번만 사고팔았을 때 얻을 수 있는 최대 수익을 return 하도록 solution 함수를 작성했습니다. 그러나, 코드 일부분이 잘못되어있기 때문에, 코드가 올바르게 동작하지 않습니다. 주어진 코드에서 _**한 줄**_만 변경해서 모든 입력에 대해 올바르게 동작하도록 수정해주세요. --- ### **매개변수 설명** 연속된 n 일 동안의 주식 가격이 순서대로 들어있는 리스트 prices가 solution 함수의 매개변수로 주어집니다. * prices의 길이는 2 이상 1,000,000 이하입니다. * prices의 각 원소는 1 이상 1,000 이하의 자연수입니다. --- #####return 값 설명 주식을 규칙에 맞게 한 번만 사고팔았을 때 얻을 수 있는 최대 수익을 return 해주세요. --- ### **예시** | prices | return | |--------- |-------- | | [1,2,3] | 2 | | [3,1] | -2 | ### **예시 설명** 예시 #1 연속된 3일의 주가가 차례로 [1, 2, 3] 이며, 첫째 날에 주식을 사서 셋째 날에 팔면 수익은 2이고, 이때가 최대입니다. 예시 #2 문제에서 설명한 것처럼 무조건 한 번은 매수하고, 한 번은 매도해야 합니다. 첫째 날에 매수하여 둘째 날에 매도하는 방법밖에 없기 때문에 수익은 -2, 즉 2만큼 손실을 보게 됩니다.","link":"/2020/09/27/1%EA%B8%89-2%EC%B0%A8-01%EB%B2%88/"},{"title":"Cos Pro Python 1급 1차 10번 문제풀이","text":"Contents of table: 문제내용 정답코드 Topic: 주식으로 최대 수익을 내세요 Review:Solution 함수를 역으로 훑어오면은, price=3 일때, mn 값이 1여야 하므로, 해당 아이디어를 가지고 수식을 훑으면수익률을 return 하는 부분인, mn - price가 아닌, price-mn 이 되어야 함을 알 수 있다. 정답코드 1234567891011def solution(prices): inf = 1000000001; mn = inf ans = -inf for price in prices: #price 1 , price2, mn = 1 , price3, mn = 1 if mn != inf: #mn =1001 , inf = 1001 // mn =1, inf = 1001 // mn=1, inf =1001 ans = max(ans, price - mn) # 문제 코드에서는 mn - price 로 되어있다. ans = max( -1001, 2-1)= 1 // ans = max(1, 3 - 1) = 2 mn = min(mn, price) #price 1, mn = 1 return ans 문제내용문제10지난 연속된 n일 동안의 주식 가격이 순서대로 들어있는 리스트가 있습니다. 이때, 다음 규칙에 따라 주식을 사고 팔았을 때의 최대 수익을 구하려 합니다. n일 동안 주식을 단 한 번 살 수 있습니다. n일 동안 주식을 단 한 번 팔 수 있습니다. 주식을 산 날에 바로 팔 수는 없으며, 최소 하루가 지나야 팔 수 있습니다. 적어도 한 번은 주식을 사야하며, 한 번은 팔아야 합니다. 주식을 팔 때는 반드시 이전에 주식을 샀어야 하며, 최대 수익은 양수가 아닐 수도 있습니다. 연속된 n 일 동안의 주식 가격이 순서대로 들어있는 리스트 prices가 매개변수로 주어질 때, 주식을 규칙에 맞게 한 번만 사고팔았을 때 얻을 수 있는 최대 수익을 return 하도록 solution 함수를 작성했습니다. 그러나, 코드 일부분이 잘못되어있기 때문에, 코드가 올바르게 동작하지 않습니다. 주어진 코드에서 _한 줄_만 변경해서 모든 입력에 대해 올바르게 동작하도록 수정해주세요. 매개변수 설명연속된 n 일 동안의 주식 가격이 순서대로 들어있는 리스트 prices가 solution 함수의 매개변수로 주어집니다. prices의 길이는 2 이상 1,000,000 이하입니다. prices의 각 원소는 1 이상 1,000 이하의 자연수입니다. #####return 값 설명주식을 규칙에 맞게 한 번만 사고팔았을 때 얻을 수 있는 최대 수익을 return 해주세요. 예시 prices return [1,2,3] 2 [3,1] -2 예시 설명예시 #1연속된 3일의 주가가 차례로 [1, 2, 3] 이며, 첫째 날에 주식을 사서 셋째 날에 팔면 수익은 2이고, 이때가 최대입니다. 예시 #2문제에서 설명한 것처럼 무조건 한 번은 매수하고, 한 번은 매도해야 합니다. 첫째 날에 매수하여 둘째 날에 매도하는 방법밖에 없기 때문에 수익은 -2, 즉 2만큼 손실을 보게 됩니다.","link":"/2020/10/10/1%EA%B8%89-1%EC%B0%A8-10%EB%B2%88/"},{"title":"Kaggle M5 Competition Part 1 -EDA","text":"Contents of table: Kaggle M5 Competition Part 1 -EDA 1. Fetch the data 2. Downcasting 3. Exploratory Data Analysis 4. Melting the data Kaggle M5 Competition Part 1 -EDA 미국 Wal-Mart 에서 주최한 매출예측 대회이다. #ref:https://mofc.unic.ac.cy/m5-competition/https://www.kaggle.com/c/m5-forecasting-accuracy #아래 코드는 Kaggle Grandmaster Rob Mulla 의 모델링을 기반으로 재구성하였습니다. 대회설명:M5는 월마트에서 제공하는 계층적 판매 데이터를 사용하여, 향후 28 일 동안의 일일 판매를 예측하고 분포를 추정하는 것이 목표이다.데이터에는 가격, 프로모션, 요일 및 특별 이벤트와 같은 설명 변수가 포함된다. 데이터셋:calendar.csv - 제품 판매 날짜에 대한 정보를 포함.sales_train_validation.csv - 제품 및 매장 별 일일 판매량 기록 데이터 포함 [d_1-d_1913]sample_submission.csv - 제출 양식.sell_prices.csv - 상점 및 날짜별로 판매 된 제품의 가격에 대한 정보를 포함.sales_train_evaluation.cs - 제품 판매 포함 [d_1-d_1941] 12345678910111213import osimport pandas as pdimport numpy as npimport plotly_express as pximport plotly.graph_objects as gofrom plotly.subplots import make_subplotsimport matplotlib.pyplot as pltimport seaborn as snsimport gcimport warningswarnings.filterwarnings('ignore')from lightgbm import LGBMRegressorimport joblib 1. Fetch the data123456sales = pd.read_csv('C:\\\\Eric\\\\Projects\\\\Kaggle_M5\\Dataset\\\\sales_train_evaluation.csv')sales.name = 'sales'calendar = pd.read_csv('C:\\\\Eric\\\\Projects\\\\Kaggle_M5\\Dataset\\\\calendar.csv')calendar.name = 'calendar'prices = pd.read_csv('C:\\\\Eric\\\\Projects\\\\Kaggle_M5\\Dataset\\\\sell_prices.csv')prices.name = 'prices' 1sales.columns Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd_1', 'd_2', 'd_3', 'd_4', ... 'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937', 'd_1938', 'd_1939', 'd_1940', 'd_1941'], dtype='object', length=1947) 12345#빈 칸 처리되어있는 d 1942 ~ 1969 col들에 0 입력for d in range(1942,1970): col = 'd_' + str(d) sales[col] = 0 sales[col] = sales[col].astype(np.int16) 2. Downcasting1234#기본 데이터셋의 용량이 큰 만큼, 메모리 다운이 필요. sales_bd = np.round(sales.memory_usage().sum()/(1024*1024),1)calendar_bd = np.round(calendar.memory_usage().sum()/(1024*1024),1)prices_bd = np.round(prices.memory_usage().sum()/(1024*1024),1) 123456789101112131415161718192021222324252627#캐글의 memory downcasting 코드를 참고하여 아래와 같이 메모리 다운. def downcast(df): cols = df.dtypes.index.tolist() types = df.dtypes.values.tolist() for i,t in enumerate(types): if 'int' in str(t): if df[cols[i]].min() &gt; np.iinfo(np.int8).min and df[cols[i]].max() &lt; np.iinfo(np.int8).max: df[cols[i]] = df[cols[i]].astype(np.int8) elif df[cols[i]].min() &gt; np.iinfo(np.int16).min and df[cols[i]].max() &lt; np.iinfo(np.int16).max: df[cols[i]] = df[cols[i]].astype(np.int16) elif df[cols[i]].min() &gt; np.iinfo(np.int32).min and df[cols[i]].max() &lt; np.iinfo(np.int32).max: df[cols[i]] = df[cols[i]].astype(np.int32) else: df[cols[i]] = df[cols[i]].astype(np.int64) elif 'float' in str(t): if df[cols[i]].min() &gt; np.finfo(np.float16).min and df[cols[i]].max() &lt; np.finfo(np.float16).max: df[cols[i]] = df[cols[i]].astype(np.float16) elif df[cols[i]].min() &gt; np.finfo(np.float32).min and df[cols[i]].max() &lt; np.finfo(np.float32).max: df[cols[i]] = df[cols[i]].astype(np.float32) else: df[cols[i]] = df[cols[i]].astype(np.float64) elif t == np.object: if cols[i] == 'date': df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d') else: df[cols[i]] = df[cols[i]].astype('category') return df 123sales = downcast(sales)prices = downcast(prices)calendar = downcast(calendar) 1234#메모리 다운 후의 메모리 사용량 체크. sales_ad = np.round(sales.memory_usage().sum()/(1024*1024),1)calendar_ad = np.round(calendar.memory_usage().sum()/(1024*1024),1)prices_ad = np.round(prices.memory_usage().sum()/(1024*1024),1) 123456789101112#다운 캐스팅이 DataFrame의 메모리 사용량에 얼마나 많은 영향을 미쳤는지 시각화.1/4 미만으로 줄일 수 있음. dic = {'DataFrame':['sales','calendar','prices'], 'Before downcasting':[sales_bd,calendar_bd,prices_bd], 'After downcasting':[sales_ad,calendar_ad,prices_ad]}memory = pd.DataFrame(dic)memory = pd.melt(memory, id_vars='DataFrame', var_name='Status', value_name='Memory (MB)')memory.sort_values('Memory (MB)',inplace=True)fig = px.bar(memory, x='DataFrame', y='Memory (MB)', color='Status', barmode='group', text='Memory (MB)')fig.update_traces(texttemplate='%{text} MB', textposition='outside')fig.update_layout(template='seaborn', title='Effect of Downcasting')fig.show() 3. Exploratory Data Analysis123456walmart 에서 제공하는 세일즈 데이터는, wrt, 즉 with respect to [ cols ]State: CA, WI, TX (3)Store: CA_1, CA_2, TX_1, WI_1, ... (10)Category: FOOD, HOBBIES, HOUSEHOLD (3) Department:FOOD_1,2,3 , HOBBIES_1,2, ... (7)item_id:: each unique id # (3,049) 1234567891011#plotly_express 에서 제공하는 treemap 을 활용해서, 각 제품 id 를 count var로 잡고, data col 들의 관계를 directory 형태로 시각화.group = sales.groupby(['state_id','store_id','cat_id','dept_id'],as_index=False)['item_id'].count().dropna()group['USA'] = 'United States of America'group.rename(columns={'state_id':'State','store_id':'Store','cat_id':'Category','dept_id':'Department','item_id':'Count'},inplace=True)fig = px.treemap(group, path=['USA', 'State', 'Store', 'Category', 'Department'], values='Count', color='Count', color_continuous_scale= px.colors.sequential.Sunset, title='Walmart: Distribution of items')fig.update_layout(template='seaborn')fig.show() 4. Melting the data#4.1 Convert from wide to long format 1머신러닝 포맷에 적합시키기 위해서는 와이드 형식의 판매 데이터 프레임을 긴 형식으로 변환이 필요하다. sales 데이터셋의 row 는 30490(== # of items), 데이터셋을 melt하게되면은 sales, calendar 30490 x 1969 = 60034810 개의 row 를 가지게 된다. 1df = pd.melt(sales, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='sold').dropna() 12df = pd.merge(df, calendar, on='d', how='left')df = pd.merge(df, prices, on=['store_id','item_id','wm_yr_wk'], how='left') 1234567#Store 별로 매출액합계를 violin plot 을 활용해서 시각화. group = df.groupby(['year','date','state_id','store_id'], as_index=False)['sold'].sum().dropna()fig = px.violin(group, x='store_id', color='state_id', y='sold',box=True)fig.update_xaxes(title_text='Store')fig.update_yaxes(title_text='Total items sold')fig.update_layout(template='seaborn',title='Distribution of Items sold wrt Stores',legend_title_text='State')fig.show()","link":"/2020/09/22/project_kaggle_m5_0922/"},{"title":"Kaggle M5 Competition Part 2 -Modeling","text":"Contents of table: Kaggle M5 Competition Part 2 -Modeling 5. Feature Engineering 6. Modeling and Prediction Kaggle M5 Competition Part 2 -Modeling 5. Feature Engineering1#5.1 Label Encoding 1234567#id, department, category, store, state 를 코드값으로 저장 d_id = dict(zip(df.id.cat.codes, df.id))d_item_id = dict(zip(df.item_id.cat.codes, df.item_id))d_dept_id = dict(zip(df.dept_id.cat.codes, df.dept_id))d_cat_id = dict(zip(df.cat_id.cat.codes, df.cat_id))d_store_id = dict(zip(df.store_id.cat.codes, df.store_id))d_state_id = dict(zip(df.state_id.cat.codes, df.state_id)) 12345678910111213#1gc.collect()#2df.d = df['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)cols = df.dtypes.index.tolist()types = df.dtypes.values.tolist()for i,type in enumerate(types): if type.name == 'category': df[cols[i]] = df[cols[i]].cat.codes #3df.drop('date',axis=1,inplace=True) 1import time 1#5.2 introduce lags 1234#lag col들을 추가lags = [1,2,3,6,12,24,36]for lag in lags: df['sold_lag_'+str(lag)] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['sold'].shift(lag).astype(np.float16) 1#5.3 Mean Encoding 1234567891011121314%time#판매량 평균을 wrt item, state, store, category, department 별로 col 생성 df['iteam_sold_avg'] = df.groupby('item_id')['sold'].transform('mean').astype(np.float16)df['state_sold_avg'] = df.groupby('state_id')['sold'].transform('mean').astype(np.float16)df['store_sold_avg'] = df.groupby('store_id')['sold'].transform('mean').astype(np.float16)df['cat_sold_avg'] = df.groupby('cat_id')['sold'].transform('mean').astype(np.float16)df['dept_sold_avg'] = df.groupby('dept_id')['sold'].transform('mean').astype(np.float16)df['cat_dept_sold_avg'] = df.groupby(['cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)df['store_item_sold_avg'] = df.groupby(['store_id','item_id'])['sold'].transform('mean').astype(np.float16)df['cat_item_sold_avg'] = df.groupby(['cat_id','item_id'])['sold'].transform('mean').astype(np.float16)df['dept_item_sold_avg'] = df.groupby(['dept_id','item_id'])['sold'].transform('mean').astype(np.float16)df['state_store_sold_avg'] = df.groupby(['state_id','store_id'])['sold'].transform('mean').astype(np.float16)df['state_store_cat_sold_avg'] = df.groupby(['state_id','store_id','cat_id'])['sold'].transform('mean').astype(np.float16)df['store_cat_dept_sold_avg'] = df.groupby(['store_id','cat_id','dept_id'])['sold'].transform('mean').astype(np.float16) Wall time: 1 ms 1#5.4 Rolling Window Statistics 1df['rolling_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.rolling(window=7).mean()).astype(np.float16) 1#5.5 Expanding Window Statistics 1df['expanding_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.expanding(2).mean()).astype(np.float16) 1#5.6 Trends 12345#Selling Trend는 간단하게, 평균보다 큰지 작은지 만을 비교. df['daily_avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','d'])['sold'].transform('mean').astype(np.float16)df['avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform('mean').astype(np.float16)df['selling_trend'] = (df['daily_avg_sold'] - df['avg_sold']).astype(np.float16)df.drop(['daily_avg_sold','avg_sold'],axis=1,inplace=True) 1#5.7 Save the data 12#lag 추가로 인해서, d 35까지 빈 row 들이 많이 발생했으므로 해당기간을 제외. df = df[df['d']&gt;=36] 1df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; Int64Index: 58967660 entries, 1067150 to 60034809 Data columns (total 43 columns): id int16 item_id int16 dept_id int8 cat_id int8 store_id int8 state_id int8 d int16 sold int16 wm_yr_wk int16 weekday int8 wday int8 month int8 year int16 event_name_1 int8 event_type_1 int8 event_name_2 int8 event_type_2 int8 snap_CA int8 snap_TX int8 snap_WI int8 sell_price float16 sold_lag_1 float16 sold_lag_2 float16 sold_lag_3 float16 sold_lag_6 float16 sold_lag_12 float16 sold_lag_24 float16 sold_lag_36 float16 iteam_sold_avg float16 state_sold_avg float16 store_sold_avg float16 cat_sold_avg float16 dept_sold_avg float16 cat_dept_sold_avg float16 store_item_sold_avg float16 cat_item_sold_avg float16 dept_item_sold_avg float16 state_store_sold_avg float16 state_store_cat_sold_avg float16 store_cat_dept_sold_avg float16 rolling_sold_mean float16 expanding_sold_mean float16 selling_trend float16 dtypes: float16(23), int16(6), int8(14) memory usage: 4.4 GB 123df.to_pickle('data.pkl')del dfgc.collect() 6. Modeling and Prediction1import time 123456%time data = pd.read_pickle('data.pkl') # FE후에 pickle 형태로 저장시켰던 데이터를 로드. valid = data[(data['d']&gt;=1914) &amp; (data['d']&lt;1942)][['id','d','sold']] # 1914 ~ 1942 validation periodtest = data[data['d']&gt;=1942][['id','d','sold']] # d &gt;= 1942 test and eval period eval_preds = test['sold'] # eval = testvalid_preds = valid['sold'] # val = val Wall time: 0 ns 12345678910111213141516171819202122232425262728293031#Get the store idsstores = sales.store_id.cat.codes.unique().tolist()for store in stores: #store 별로 나눠서 prediction 진행 df = data[data['store_id']==store] #Split the data X_train, y_train = df[df['d']&lt;1914].drop('sold',axis=1), df[df['d']&lt;1914]['sold'] X_valid, y_valid = df[(df['d']&gt;=1914) &amp; (df['d']&lt;1942)].drop('sold',axis=1), df[(df['d']&gt;=1914) &amp; (df['d']&lt;1942)]['sold'] X_test = df[df['d']&gt;=1942].drop('sold',axis=1) #Train and validate model = LGBMRegressor( n_estimators=1000, learning_rate=0.3, subsample=0.8, colsample_bytree=0.8, max_depth=8, num_leaves=50, min_child_weight=300 ) print('*****Prediction for Store: {}*****'.format(d_store_id[store])) model.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_valid,y_valid)], eval_metric='rmse', verbose=20, early_stopping_rounds=20) valid_preds[X_valid.index] = model.predict(X_valid) eval_preds[X_test.index] = model.predict(X_test) filename = 'model'+str(d_store_id[store])+'.pkl' # save model joblib.dump(model, filename) del model, X_train, y_train, X_valid, y_valid gc.collect() *****Prediction for Store: CA_1***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.843923 training's l2: 0.712206 valid_1's rmse: 0.556612 valid_1's l2: 0.309817 [40] training's rmse: 0.805702 training's l2: 0.649156 valid_1's rmse: 0.536648 valid_1's l2: 0.287992 [60] training's rmse: 0.782521 training's l2: 0.612339 valid_1's rmse: 0.529075 valid_1's l2: 0.27992 [80] training's rmse: 0.765509 training's l2: 0.586004 valid_1's rmse: 0.519001 valid_1's l2: 0.269362 [100] training's rmse: 0.746824 training's l2: 0.557746 valid_1's rmse: 0.516391 valid_1's l2: 0.26666 [120] training's rmse: 0.736669 training's l2: 0.542682 valid_1's rmse: 0.512239 valid_1's l2: 0.262389 [140] training's rmse: 0.725183 training's l2: 0.52589 valid_1's rmse: 0.507517 valid_1's l2: 0.257574 [160] training's rmse: 0.71879 training's l2: 0.516659 valid_1's rmse: 0.503054 valid_1's l2: 0.253063 [180] training's rmse: 0.713246 training's l2: 0.508719 valid_1's rmse: 0.501668 valid_1's l2: 0.25167 Early stopping, best iteration is: [177] training's rmse: 0.713815 training's l2: 0.509531 valid_1's rmse: 0.501194 valid_1's l2: 0.251195 *****Prediction for Store: CA_2***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.509193 training's l2: 0.259277 valid_1's rmse: 0.488679 valid_1's l2: 0.238808 [40] training's rmse: 0.476985 training's l2: 0.227515 valid_1's rmse: 0.481392 valid_1's l2: 0.231738 [60] training's rmse: 0.459124 training's l2: 0.210795 valid_1's rmse: 0.469844 valid_1's l2: 0.220753 [80] training's rmse: 0.446454 training's l2: 0.199321 valid_1's rmse: 0.466131 valid_1's l2: 0.217278 [100] training's rmse: 0.44062 training's l2: 0.194146 valid_1's rmse: 0.465138 valid_1's l2: 0.216353 [120] training's rmse: 0.435579 training's l2: 0.189729 valid_1's rmse: 0.462275 valid_1's l2: 0.213698 [140] training's rmse: 0.433312 training's l2: 0.187759 valid_1's rmse: 0.46174 valid_1's l2: 0.213204 [160] training's rmse: 0.430487 training's l2: 0.185319 valid_1's rmse: 0.461825 valid_1's l2: 0.213283 Early stopping, best iteration is: [149] training's rmse: 0.431706 training's l2: 0.18637 valid_1's rmse: 0.461223 valid_1's l2: 0.212727 *****Prediction for Store: CA_3***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 1.31768 training's l2: 1.73629 valid_1's rmse: 0.620532 valid_1's l2: 0.38506 [40] training's rmse: 1.25016 training's l2: 1.56289 valid_1's rmse: 0.599518 valid_1's l2: 0.359422 [60] training's rmse: 1.21357 training's l2: 1.47275 valid_1's rmse: 0.583401 valid_1's l2: 0.340357 [80] training's rmse: 1.18962 training's l2: 1.41519 valid_1's rmse: 0.580415 valid_1's l2: 0.336882 [100] training's rmse: 1.16704 training's l2: 1.36198 valid_1's rmse: 0.573824 valid_1's l2: 0.329274 Early stopping, best iteration is: [83] training's rmse: 1.18341 training's l2: 1.40046 valid_1's rmse: 0.571149 valid_1's l2: 0.326211 *****Prediction for Store: CA_4***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.379545 training's l2: 0.144055 valid_1's rmse: 0.306421 valid_1's l2: 0.0938936 [40] training's rmse: 0.362723 training's l2: 0.131568 valid_1's rmse: 0.296737 valid_1's l2: 0.0880528 [60] training's rmse: 0.352526 training's l2: 0.124275 valid_1's rmse: 0.286469 valid_1's l2: 0.0820644 [80] training's rmse: 0.347152 training's l2: 0.120515 valid_1's rmse: 0.283419 valid_1's l2: 0.0803261 [100] training's rmse: 0.342128 training's l2: 0.117052 valid_1's rmse: 0.279012 valid_1's l2: 0.0778477 [120] training's rmse: 0.339248 training's l2: 0.115089 valid_1's rmse: 0.27756 valid_1's l2: 0.0770398 [140] training's rmse: 0.336076 training's l2: 0.112947 valid_1's rmse: 0.27745 valid_1's l2: 0.0769786 Early stopping, best iteration is: [129] training's rmse: 0.337326 training's l2: 0.113789 valid_1's rmse: 0.276789 valid_1's l2: 0.0766123 *****Prediction for Store: TX_1***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.779231 training's l2: 0.607202 valid_1's rmse: 0.495078 valid_1's l2: 0.245102 [40] training's rmse: 0.734945 training's l2: 0.540143 valid_1's rmse: 0.477927 valid_1's l2: 0.228414 [60] training's rmse: 0.715 training's l2: 0.511225 valid_1's rmse: 0.474993 valid_1's l2: 0.225618 [80] training's rmse: 0.700945 training's l2: 0.491324 valid_1's rmse: 0.471686 valid_1's l2: 0.222487 [100] training's rmse: 0.688138 training's l2: 0.473534 valid_1's rmse: 0.469721 valid_1's l2: 0.220638 [120] training's rmse: 0.671506 training's l2: 0.45092 valid_1's rmse: 0.468799 valid_1's l2: 0.219772 Early stopping, best iteration is: [111] training's rmse: 0.678168 training's l2: 0.459912 valid_1's rmse: 0.466017 valid_1's l2: 0.217172 *****Prediction for Store: TX_2***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.949797 training's l2: 0.902115 valid_1's rmse: 0.519843 valid_1's l2: 0.270237 [40] training's rmse: 0.901254 training's l2: 0.812259 valid_1's rmse: 0.50753 valid_1's l2: 0.257587 [60] training's rmse: 0.860935 training's l2: 0.741208 valid_1's rmse: 0.496691 valid_1's l2: 0.246702 [80] training's rmse: 0.837279 training's l2: 0.701036 valid_1's rmse: 0.500869 valid_1's l2: 0.25087 Early stopping, best iteration is: [60] training's rmse: 0.860935 training's l2: 0.741208 valid_1's rmse: 0.496691 valid_1's l2: 0.246702 *****Prediction for Store: TX_3***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.741642 training's l2: 0.550033 valid_1's rmse: 0.569192 valid_1's l2: 0.323979 [40] training's rmse: 0.71047 training's l2: 0.504767 valid_1's rmse: 0.557032 valid_1's l2: 0.310284 [60] training's rmse: 0.68682 training's l2: 0.471721 valid_1's rmse: 0.546532 valid_1's l2: 0.298697 [80] training's rmse: 0.672727 training's l2: 0.452562 valid_1's rmse: 0.541006 valid_1's l2: 0.292688 [100] training's rmse: 0.66163 training's l2: 0.437754 valid_1's rmse: 0.539347 valid_1's l2: 0.290895 [120] training's rmse: 0.650395 training's l2: 0.423014 valid_1's rmse: 0.534985 valid_1's l2: 0.286208 [140] training's rmse: 0.645165 training's l2: 0.416238 valid_1's rmse: 0.532259 valid_1's l2: 0.2833 Early stopping, best iteration is: [132] training's rmse: 0.646645 training's l2: 0.418149 valid_1's rmse: 0.531403 valid_1's l2: 0.28239 *****Prediction for Store: WI_1***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.40387 training's l2: 0.163111 valid_1's rmse: 0.351971 valid_1's l2: 0.123884 [40] training's rmse: 0.379547 training's l2: 0.144056 valid_1's rmse: 0.339714 valid_1's l2: 0.115405 [60] training's rmse: 0.370228 training's l2: 0.137069 valid_1's rmse: 0.338534 valid_1's l2: 0.114605 [80] training's rmse: 0.362681 training's l2: 0.131537 valid_1's rmse: 0.335793 valid_1's l2: 0.112757 Early stopping, best iteration is: [75] training's rmse: 0.363574 training's l2: 0.132186 valid_1's rmse: 0.335287 valid_1's l2: 0.112418 *****Prediction for Store: WI_2***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.798844 training's l2: 0.638151 valid_1's rmse: 0.99757 valid_1's l2: 0.995147 [40] training's rmse: 0.75986 training's l2: 0.577388 valid_1's rmse: 0.979328 valid_1's l2: 0.959084 [60] training's rmse: 0.729671 training's l2: 0.53242 valid_1's rmse: 0.968394 valid_1's l2: 0.937787 Early stopping, best iteration is: [57] training's rmse: 0.732588 training's l2: 0.536685 valid_1's rmse: 0.967836 valid_1's l2: 0.936707 *****Prediction for Store: WI_3***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.803068 training's l2: 0.644919 valid_1's rmse: 0.580289 valid_1's l2: 0.336735 [40] training's rmse: 0.762335 training's l2: 0.581154 valid_1's rmse: 0.573159 valid_1's l2: 0.328512 [60] training's rmse: 0.739142 training's l2: 0.546331 valid_1's rmse: 0.566164 valid_1's l2: 0.320541 Early stopping, best iteration is: [51] training's rmse: 0.748455 training's l2: 0.560184 valid_1's rmse: 0.563976 valid_1's l2: 0.318069 123456789101112131415161718192021222324actual = Falseif actual == False: #대회 종료 1달 전에, validation data 를 추가로 제공하기 때문에, 그 전에 training data 로만 생성한 valid 를 쓸지, 아니면 추가 제공 valid 를 쓸지 결정 validation = sales[['id']+['d_' + str(i) for i in range(1914,1942)]] validation['id']=pd.read_csv('C:\\\\Eric\\\\Projects\\\\Kaggle_M5\\Dataset\\\\sales_train_validation.csv').id validation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]else: valid['sold'] = valid_preds validation = valid[['id','d','sold']] validation = pd.pivot(validation, index='id', columns='d', values='sold').reset_index() validation.columns=['id'] + ['F' + str(i + 1) for i in range(28)] validation.id = validation.id.map(d_id).str.replace('evaluation','validation')#predictio data 생성 test['sold'] = eval_predsevaluation = test[['id','d','sold']]evaluation = pd.pivot(evaluation, index='id', columns='d', values='sold').reset_index()evaluation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]evaluation.id = evaluation.id.map(d_id)#Submission 파일 생성 submit = pd.concat([validation,evaluation]).reset_index(drop=True)submit.to_csv('M5_submission.csv',index=False) 1","link":"/2020/09/21/project_kaggle_m5_0922%20Modeling/"},{"title":"Project_DACON_상점 신용카드 매출 예측 경진대회","text":"- Portfolio for Time-Series 개인적인 시계열 분석 포트폴리오 작성을 위하여, 아래의 DACON 대회 자료를 활용하였습니다. (현재종료 2019년 대회) 대회유형: 시계열 예측 대회목적: 핀테크 기업인 ‘FUNDA(펀다)’는 상환 기간의 매출을 예측하여 신용 점수가 낮거나 담보를 가지지 못하는 우수 상점들에 금융 기회를 제공하려 합니다. 이번 대회에서는 2년 전 부터 2019년 2월 28일까지의 카드 거래 데이터를 이용해 2019-03-01부터 2019-05-31까지의 각 상점별 3개월 총 매출을 예측하는 것입니다. 데이터제공기간: 2016-06-01 ~ 2019-2-28 (1002 days) 데이터예측기간: 2019-03-01 ~ 2019-05-31 (92 days) 데이터사이즈: 6,556,613 entries , 9 columns, memory 450MBLink: https://dacon.io/competitions/official/140472/overview/ Data Descriptionstore_id : 상점의 고유 아이디card_id : 사용한 카드의 고유 아이디card_company : 비식별화된 카드 회사trasacted_date : 거래 날짜transacted_time : 거래 시간( 시:분 )installment_term : 할부 개월 수( 포인트 사용 시 (60개월 + 실제할부개월)을 할부개월수에 기재한다. )region : 상점의 지역type_of_business : 상점의 업종amount : 거래액(단위는 원이 아닙니다) - Project Summary 프로젝트 진행 일정2020-10-07 ~ 2020-10-16 (9 days) 개발환경OS: WindowIDE: Visual Studio Code, Jupyter NotebookVCS: Git, SourceTree 프로젝트 최종 결과 (9등, 상위권) Model MAE(Mean Absolute Error,평균절대오차) Public Board Rank Exponential_Moving Average(5) 765,887 9등(최종) Exponential_Moving Average(3) 784,219 18등 Simple_Moving_Average(3) 810,333 25등 ARIMA_n_LGBM 1,768,874 93등 ARIMA 1,062,635 50등 LGBM 2,481,832 102등 *우측 CATALOGUE 에서 Conclusion 링크를 클릭하시면 프로젝트 결과에 관한 설명 및 요약 리포트를 확인하실 수 있습니다. 프로젝트 목차 -1. Portfolio for Time-Series-2. Project Summary-3. Data_Load-4. EDA_Part_1-5. EDA_Part_2-6. Data Preprocessing-7. LGBM Modeling-8. Modeling_MA&amp;EPMA&amp;ARIMA-9. Moving Average-10. Exponential Moving Average-11. ARIMA-12. Conclusion Data_Load12345678910111213import numpy as npimport pandas as pdfrom tqdm.autonotebook import tqdmimport datetimefrom datetime import datefrom datetime import timedeltafrom lightgbm import LGBMRegressorfrom sklearn.preprocessing import LabelEncoderimport plotly_express as pximport matplotlib.pylab as pltplt.rcParams['font.family'] = 'Malgun Gothic'import seaborn as snsfrom statsmodels.tsa.seasonal import seasonal_decompose C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console) This is separate from the ipykernel package so we can avoid doing imports until 12# raw data load sales = pd.read_csv('C:\\Archaon\\projects\\Card_Sales\\Dataset\\copy_train.csv') EDA_Part_112# 1. raw datasales.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 6556613 entries, 0 to 6556612 Data columns (total 9 columns): # Column Dtype --- ------ ----- 0 store_id int64 1 card_id int64 2 card_company object 3 transacted_date object 4 transacted_time object 5 installment_term int64 6 region object 7 type_of_business object 8 amount float64 dtypes: float64(1), int64(3), object(5) memory usage: 450.2+ MB 12# 1.1 missing values sales.isnull().sum() store_id 0 card_id 0 card_company 0 transacted_date 0 transacted_time 0 installment_term 0 region 2042766 type_of_business 3952609 amount 0 dtype: int64 123# Before Downcasting sales_bd = np.round(sales.memory_usage().sum()/(1024*1024),1)sales_bd #450 450.2 1234567891011121314151617181920212223242526272829# Memory Downcast# 데이터 메모리 축소를 위한 downcast 함수 생성 및 적용 def downcast(df): cols = df.dtypes.index.tolist() types = df.dtypes.values.tolist() for i,t in enumerate(types): if 'int' in str(t): if df[cols[i]].min() &gt; np.iinfo(np.int8).min and df[cols[i]].max() &lt; np.iinfo(np.int8).max: df[cols[i]] = df[cols[i]].astype(np.int8) elif df[cols[i]].min() &gt; np.iinfo(np.int16).min and df[cols[i]].max() &lt; np.iinfo(np.int16).max: df[cols[i]] = df[cols[i]].astype(np.int16) elif df[cols[i]].min() &gt; np.iinfo(np.int32).min and df[cols[i]].max() &lt; np.iinfo(np.int32).max: df[cols[i]] = df[cols[i]].astype(np.int32) else: df[cols[i]] = df[cols[i]].astype(np.int64) elif 'float' in str(t): if df[cols[i]].min() &gt; np.finfo(np.float16).min and df[cols[i]].max() &lt; np.finfo(np.float16).max: df[cols[i]] = df[cols[i]].astype(np.float16) elif df[cols[i]].min() &gt; np.finfo(np.float32).min and df[cols[i]].max() &lt; np.finfo(np.float32).max: df[cols[i]] = df[cols[i]].astype(np.float32) else: df[cols[i]] = df[cols[i]].astype(np.float64) elif t == np.object: if cols[i] == 'date': df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d') else: df[cols[i]] = df[cols[i]].astype('category') return df 123456# Downcast Datasales = downcast(sales)# After Downcasting sales_ad = np.round(sales.memory_usage().sum()/(1024*1024),1) #125.2sales_ad 125.2 1234567891011121314import plotly_express as pxdic = {'DataFrame':['sales'], 'Before downcasting':[sales_bd], 'After downcasting':[sales_ad]}memory = pd.DataFrame(dic)memory = pd.melt(memory, id_vars='DataFrame', var_name='Status', value_name='Memory (MB)')memory.sort_values('Memory (MB)',inplace=True)fig = px.bar(memory, x='DataFrame', y='Memory (MB)', color='Status', barmode='group', text='Memory (MB)')fig.update_traces(texttemplate='%{text} MB', textposition='outside')fig.update_layout(template='seaborn', title='Effect of Downcasting')fig.show() 12345678910111213141516# 1.2 columns# 1.2.1 ['store_id']len(sales['store_id'].unique()) #1967## 1967개의 store_id 가 존재한다. store_counts = sales['store_id'].value_counts().reset_index()store_counts = pd.DataFrame(store_counts )store_counts.columns = ['store_id','counts']plt.figure(figsize=(12,8))ax = sns.boxplot(x=&quot;counts&quot;, data =store_counts)ax = sns.swarmplot(x=&quot;counts&quot;, data =store_counts, palette=&quot;ocean&quot;)ax.set( Xlabel=&quot;결제횟수 by Store ID&quot;, ylabel=&quot;Box Plot&quot;)ax.set_title(&quot;&lt;Figure1&gt;, 상점 Store ID 별 결제횟수 분포&quot;)plt.tight_layout()plt.show() 1234567891011121314151617181920212223# 1.2.2 ['card_company]len(sales) # = 6,556,613len(sales['card_id'].unique()) # = 3,950,001## sales 데이터의 entry 는 card_id를 기준으로 작성되었다.## 따라서, 1 entry = 1 card transaction 으로 이해할 수 있다. ## 하지만 데이터상의 고유 card_id 의 수는 3,950,001 개로, 전체 거래량인 6,556,613 에 비해서 40% 정도 작다. ## 그러므로 전체 거래량의 40%는 동일한 카드의 중복거래란 것을 확인할 수 있다. sales.card_company ## Categories (8, object): [a, b, c, d, e, f, g, h]card_company = sales['card_company'].value_counts()card_company = pd.DataFrame(card_company)plt.figure(figsize=(12,8))ax = sns.barplot(x=card_company.index , y = &quot;card_company&quot;, palette=&quot;CMRmap_r&quot;, data =card_company )ax.set( Xlabel=&quot;Card Company&quot;, ylabel=&quot;결제횟수 합계&quot;)ax.set_title(&quot;&lt;Figure2&gt;, 카드회사별 결제횟수 합계&quot;)plt.tight_layout()plt.show() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 1.2.4 ['transacted_date']## 카드 거래일자는 2016년 6월1일부터, 2019년 2월 28일까지이며, day 기준으로 1002 일이다. ## 본 대회의 경우, 1002일 간의 카드거래 데이터로, 향후 3달후의 상점별 매출액을 예상하는 대회이다. ## 따라서 거래일자는 아래와 같은 구간으로 나눠진다. ## train period : 2016.6.1 ~ 2019.2.28 (1002 days) ## test period : 2019.3.1 ~ 2019.5.31 (91 days)sales['transacted_date']# 0 2016-06-01# 1 2016-06-01# 2 2016-06-01# 3 2016-06-01# 4 2016-06-02# ...# 6556608 2019-02-28# 6556609 2019-02-28# 6556610 2019-02-28# 6556611 2019-02-28# 6556612 2019-02-28# 1.2.5 ['transacted_time'] &amp; ['installment_term]## transacted_time 과 installment_term 각각 카드거래 시각과 할부 개월수를 의미한다. ## 이러한 분포는 상점의 사업종류에 따라서 편차가 클 것으로 예상된다. ## 상점 하나(store_id == 1000)를 샘플로 골라서, 분포를 살펴보도록 하겠다. ## 선택된 상점의 경우에, type of business 가 '의복 소매업'이다. 따라서 사람들이 많이 쇼핑하는 ## 주말 오후 시간대 14 ~ 18 시 사이에, 카드결제 시간이 몰려 있는 것으로 확인된다. store_1000 = sales[ sales['store_id'] == 1000 ]###len(store_1000)blank = []for i in range(0,len(store_1000)): str1 = int(store_1000['transacted_time'].iloc[i][0:2]) blank.append(str1)blankblank = pd.DataFrame(blank, columns=['Count'])time_count = pd.DataFrame(blank['Count'].value_counts())###plt.figure(figsize=(12,8))ax = sns.barplot(x=time_count.index , y = &quot;Count&quot;, data =time_count )ax.set( Xlabel=&quot;카드결제시각&quot;, ylabel=&quot;카드결제횟수&quot;)ax.set_title(&quot;&lt;Figure3&gt;, Store_id = 1000, 카드결제시각 당 카드결제횟수&quot;)plt.tight_layout()plt.show() 12345678910111213141516171819# 1.2.6 ['region'] &amp; ['type of business']## region 과 type of business 의 경우에 결측치가 많다. ## region 의 경우에는 총 데이터의 31%가 결측치이며, business의 경우에 60%가 결측치이다. null_result = sales.isnull().sum()null_result = pd.DataFrame(null_result,columns=['count'])null_result = null_result.reset_index()plt.figure(figsize=(12,8))ax = sns.barplot(x=&quot;index&quot; , y = &quot;count&quot;, data =null_result , palette= &quot;ocean&quot; )ax.set( Xlabel=&quot;Data Columns&quot;, ylabel=&quot;Null Counts&quot;)ax.set_title(&quot;&lt;Figure4&gt;, 결측치확인 &quot;)plt.tight_layout()# 1.2.7 ['amount']## 주어진 데이터의 거래액은 카드결제 당 금액으로 입력되어있다. 하지만 예측이 필요한 3개월의 데이터는 ## 상점별 매출이므로, amount 금액 또한 store_id 를 기준으로 groupby 한 금액을 본다. plt.show() 1234567891011121314151617181920# 1.2.7 ['amount']## 주어진 데이터의 거래액은 카드결제 당 금액으로 입력되어있다. 하지만 예측이 필요한 3개월의 데이터는 ## 상점별 매출이므로, amount 금액 또한 store_id 를 기준으로 groupby 한 금액을 본다. ## 상점별 매출의 경우 피라미드 형태의 매출을 보여주고 있으며, 특정 상점이 이상치에 가까운 매출액을 보여주고 있다. s1 = sales.groupby('store_id').sum().reset_index()s1['amount'] = s1['amount'].astype(int)plt.figure(figsize=(12,8))ax = sns.boxplot(x=&quot;amount&quot;, data =s1)ax = sns.swarmplot(x=&quot;amount&quot;, data=s1, palette=&quot;ocean&quot;)ax.set( Xlabel=&quot;&quot;, ylabel=&quot;Card Transaction Amount&quot;)ax.set_title(&quot;&lt;Figure5&gt;, 상점별 카드매출액 분포 &quot;)plt.tight_layout()plt.show() EDA_Part_212345678910111213141516171819202122232425# sido 데이터 뽑아내기 #region &amp; type of business 가 있는 데이터로 그룹 나누기 sales_region = sales[['store_id','region']]sales_region_null = sales_region.isnull() region_index_list = sales_region_null[ sales_region_null['region']== 0 ].index #length 2042766 #4513847 False # Sales 데이터에서 region 값이 있는 데이터들을 찾기 sales1 = sales.iloc[ region_index_list, :]sales1.type_of_business.isnull().sum()#결측치행 drop sales_c = sales.copy()group1 = sales_c.dropna(subset = ['region', 'type_of_business'] )group1 #[2318410 rows x 9 columns] # group1 = 35% 전체데이터에서 sido_list = group1['region'].str[0:2] sido_list = pd.DataFrame(sido_list)sido_list.columns = ['region_sido']group1_sido = pd.concat([group1, sido_list] ,axis = 1 )#### group 구분 4단계 ## group1 둘다 있음 group1 = 35% 전체데이터에서 ## group2 비지니스만 ## group3 지역만## group4 둘다없음 123456789101112# Group1 시도별, amount 금액 합계 ra = group1_sido.groupby('region_sido')['amount'].sum()ras = ra.reset_index()ras = ras.sort_values(by=['amount'], axis=0,ascending=False)plt.figure(figsize=(12,8))ax = sns.barplot(x='region_sido' , y = &quot;amount&quot;, data =ras )ax.set( Xlabel=&quot;시도&quot;, ylabel=&quot;카드매출금액합계&quot;)ax.set_title(&quot;&lt;Figure6&gt;, 시도별 카드매출금액 합계 &quot;)plt.tight_layout()plt.show() 1234567891011# 시도별, 결제 횟수 합계 rs = group1_sido['region_sido'].value_counts()res = rs.reset_index()resplt.figure(figsize=(12,8))ax = sns.barplot(x='index' , y = &quot;region_sido&quot;, data =res )ax.set( Xlabel=&quot;카드결제횟수 합계&quot;, ylabel=&quot;counts&quot;)ax.set_title(&quot;&lt;Figure7&gt;, 시도별 카드결제횟수 합계 &quot;)plt.tight_layout()plt.show() 123456789101112# 업종별 결제금액 합계 tba = group1_sido.groupby('type_of_business')['amount'].sum()tba = tba.reset_index()tbas = tba.sort_values(by=['amount'], axis=0,ascending=False)plt.figure(figsize=(12,8))ax = sns.barplot(y='type_of_business' , x=&quot;amount&quot;, data =tbas, order=tbas.sort_values('amount',ascending = False).type_of_business )ax.set( Xlabel=&quot;결제금액합&quot;, ylabel=&quot;type_of_business&quot;, )ax.set_title(&quot;&lt;Figure8&gt;, 업종별 카드결제금액 합계 &quot;)ax.set_xticklabels(ax.get_xticklabels(), rotation=90)plt.tight_layout()plt.show() 123## 결제금액 top 10, Business type new_tb= tbas[0:9]new_tb .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } type_of_business amount 139 한식 음식점업 4.355464e+09 99 의복 소매업 1.954874e+09 101 의약품 도매업 1.568540e+09 105 일반 교과 학원 1.168244e+09 54 두발 미용업 8.859701e+08 108 일식 음식점업 8.170945e+08 46 기타 주점업 6.550956e+08 37 기타 스포츠 교육기관 6.052595e+08 82 안경 및 렌즈 소매업 5.064083e+08 12345678## 매출 top 10 업종이 전체 매출에서 차지하는 비중 ## group1['amount'].sum() #25,657,748,130tbas['amount'][0:10].sum() #1,071,393,938#13020321268 / 25657748130 # 상위 10개가 전체 매출의 50 %tbas['amount'][0:5].sum() #9,933,091,985#9933091985 / 25657748130 # 상위 5개가 전체 매출의 38 %tbas['amount'][0:1].sum() #4,355,463,710#4355463710 / 25657748130 # top 1(한식음식점업)이 전체 매출의 16% 12345678910111213141516171819202122232425262728293031323334## top3 지역별 매출액 합계 group1_sido['transacted_date'] = pd.to_datetime(group1_sido['transacted_date'] )group1_gy = group1_sido[group1_sido['region_sido'] == &quot;경기&quot;]group1_gy_new = group1_gy.groupby( 'transacted_date' )['amount'].sum()group1_gy_new = group1_gy_new.reset_index()group1_se = group1_sido[group1_sido['region_sido'] == &quot;서울&quot;]group1_se_new = group1_se.groupby( 'transacted_date' )['amount'].sum()group1_se_new = group1_se_new.reset_index()group1_pu = group1_sido[group1_sido['region_sido'] == &quot;부산&quot;]group1_pu_new = group1_pu.groupby( 'transacted_date' )['amount'].sum()group1_pu_new = group1_pu_new.reset_index()fig, ax = plt.subplots(nrows=3, figsize = (18, 12))#f,axes = plt.subplots(3,1, figsize = (18,12), sharex=True)sns.lineplot(y=&quot;amount&quot; , x=&quot;transacted_date&quot;, data =group1_gy_new, ax= ax[0])sns.lineplot(y=&quot;amount&quot; , x=&quot;transacted_date&quot;, data =group1_se_new, ax= ax[1]) sns.lineplot(y=&quot;amount&quot; , x=&quot;transacted_date&quot;, data =group1_pu_new, ax= ax[2])ax[0].set( Xlabel=&quot;&quot;, ylabel=&quot;매출액&quot;, )ax[0].set_title(&quot;&lt;Figure9&gt;, 경기지역 매출액 합계 &quot;)ax[1].set( Xlabel=&quot;&quot;, ylabel=&quot;매출액&quot;, )ax[1].set_title(&quot;&lt;Figure10&gt;, 서울지역 매출액 합계 &quot;)ax[2].set( Xlabel=&quot;&quot;, ylabel=&quot;매출액&quot;, )ax[2].set_title(&quot;&lt;Figure11&gt;, 부산지역 매출액 합계 &quot;)plt.show() 12345678910111213141516171819202122232425262728293031323334## top3 업종별 매출액 합계 group1_sido['transacted_date'] = pd.to_datetime(group1_sido['transacted_date'] )group1_gyr = group1_sido[group1_sido['type_of_business'] == &quot;한식 음식점업&quot;]group1_gyr_new = group1_gyr.groupby( 'transacted_date' )['amount'].sum()group1_gyr_new = group1_gyr_new.reset_index()group1_ser = group1_sido[group1_sido['type_of_business'] == &quot;의복 소매업&quot;]group1_ser_new = group1_ser.groupby( 'transacted_date' )['amount'].sum()group1_ser_new = group1_ser_new.reset_index()group1_pur = group1_sido[group1_sido['type_of_business'] == &quot;의약품 도매업&quot;]group1_pur_new = group1_pur.groupby( 'transacted_date' )['amount'].sum()group1_pur_new = group1_pur_new.reset_index()fig, ax = plt.subplots(nrows=3, figsize = (18, 12))#f,axes = plt.subplots(3,1, figsize = (18,12), sharex=True)sns.lineplot(y=&quot;amount&quot; , x=&quot;transacted_date&quot;, data =group1_gyr_new, ax= ax[0])sns.lineplot(y=&quot;amount&quot; , x=&quot;transacted_date&quot;, data =group1_ser_new, ax= ax[1]) sns.lineplot(y=&quot;amount&quot; , x=&quot;transacted_date&quot;, data =group1_pur_new, ax= ax[2])ax[0].set( Xlabel=&quot;&quot;, ylabel=&quot;매출액&quot;, )ax[0].set_title(&quot;&lt;Figure11&gt;, 한식 음식점업 매출액 합계 &quot;)ax[1].set( Xlabel=&quot;&quot;, ylabel=&quot;매출액&quot;, )ax[1].set_title(&quot;&lt;Figure12&gt;, 의복 소매업 매출액 합계 &quot;)ax[2].set( Xlabel=&quot;&quot;, ylabel=&quot;매출액&quot;, )ax[2].set_title(&quot;&lt;Figure13&gt;, 의약품 도매업 매출액 합계 &quot;)plt.show() 1234### top 매출업종 한식음식점업 결제금액 합계 group1_group = group1_sido.groupby(['type_of_business','transacted_date']).sum().reset_index()group1_group['transacted_date'] = pd.to_datetime(group1_group['transacted_date'])hansik = group1_group[ group1_group['type_of_business'] ==&quot;한식 음식점업&quot; ] 12345plt.figure(figsize=(18,10))ax = sns.lineplot(y='amount' , x=&quot;transacted_date&quot;, data =hansik)ax.set( Xlabel=&quot;&quot;, ylabel=&quot;결제금액&quot;, )ax.set_title(&quot;&lt;Figure14&gt;, 한식음식점업 결제금액 합계&quot;)plt.show() 123456789101112131415### STL 분해 from statsmodels.tsa.seasonal import seasonal_decomposeimport warningswarnings.filterwarnings(action='ignore') # 매출 1위 업종인 한식업종에 대한 STL 분해 seasonal_hansik = hansik[['transacted_date','amount']].set_index('transacted_date')ts = seasonal_hansik.amountresult = seasonal_decompose(x= ts , model='additive', freq=12)plt.rcParams[&quot;figure.figsize&quot;] = (16,12)ax = result.plot()plt.show() 123456789101112### ARIMA 분석을 위한 ACF, PACF 그래프 import matplotlib.pyplot as pltfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf# 한식업종 매출에 대한 ACF, PACF 그래프 hansik_ap = hansik[['transacted_date','amount']]hansik_ap = hansik_ap.set_index('transacted_date')plot_acf(hansik_ap)plot_pacf(hansik_ap)plt.show() Data PreprocessingLGBM Modeling123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# LGBM 모델링을 위한 데이터 전처리 # 전체 데이터를 region 과 business 열에 결측치가 유무로 두 가지의 그룹으로 구분하였다. # group1(35% of all data) : region 과 business 값들에 결측치 없음 # group2(65% of all data) : region 과 business 에 결측치가 존재함# # 보통의 경우에 위와 같이 결측치의 비중이 10~20%를 초과하면은 해당 칼럼들을 사용하지 않지만, # 해당 데이터의 경우에, region과 business 를 제외하면은 효과적으로 데이터를 구분해줄 category 변수가 전혀 존재하지 않는다. # 따라서 tree 기반 모델링을 위해서 data 를 두 그룹으로 나누고, 한쪽 그룹에는 tree 기반 모델링을 하고, 다른 한쪽 그룹에는 # non-tree 기반한 모델링을 하여서 그러한 구분이 결과값에 어떠한 영향을 끼쳤는지와 그 이유를 탐구해 볼 예정이다. # LGBM 모데링을 위한 group1 데이터셋 전처리def group1_make(data): ### group1 dataset making # region과 business에서 결측치가 있는 행들을 제외 group1 = data.dropna(subset = ['region', 'type_of_business'] ) # region 에서, 시/도 추출 sido_list = group1['region'].str[0:2] sido_list = pd.DataFrame(sido_list) sido_list.columns = ['region_sido'] group1_sido = pd.concat([group1, sido_list] ,axis = 1 ) group1_sido = group1_sido.drop('region',axis=1) return group1_sidogroup1_dataset = group1_make(sales)def group1_make_2nd(data): ### group1 transform datetime &amp; groupby year and month dataset = pd.DataFrame([]) for i in tqdm(data.store_id.unique()): datas = data[ data['store_id'] == i ] id_region_sido = datas['region_sido'].unique()[0] id_type_of_business = datas['type_of_business'].unique()[0] # 불필요한 column 들 drop datas = datas.drop(['card_id', 'card_company', 'transacted_time', 'installment_term'], axis=1) # 날짜 데이터 datetime 변환 datas['transacted_date'] = pd.to_datetime( datas['transacted_date']) # month 와 year 생성 datas['month'] = datas['transacted_date'].dt.month datas['year'] = datas['transacted_date'].dt.year datas_grouped = datas.groupby( ['store_id' , 'year', 'month']).sum().reset_index() datas_grouped['sido'] = id_region_sido datas_grouped['business'] = id_type_of_business dataset = pd.concat( [dataset, datas_grouped], axis=0) dataset = dataset.reset_index() dataset = dataset.drop(['index'], axis=1 ) return datasetgroup1 = group1_make_2nd(group1_dataset) Wall time: 0 ns 123456789101112131415161718192021222324252627def make_lags(data): ### Feature Engineering : lag &amp; rolling mean blank = pd.DataFrame([]) for i in tqdm(data.store_id.unique()): store = data[ data.store_id == i] store = store.reset_index() store = store.drop(['index'],axis=1) # lag 1,2,3,6 lags = [1,2,3,6] for lag in lags: store['amount_lag_' + str(lag)] =store.groupby( ['store_id', 'year','month','sido','business'], as_index=False )['amount'].sum().shift(lag).amount # rolling mean 3,6 windows = [3,6] for window in windows: store['rolling_mean_' + str(window)] =store.amount.rolling(window=window).mean() blank = pd.concat([blank,store], axis=0) blank = blank.reset_index() blank = blank.drop( ['index'], axis=1) return blank group1_fin = make_lags(group1) 1234567891011121314151617181920212223242526272829303132333435363738def concat_test_set(data): ### MeanEncoding &amp; Test Dataset # Mean Encoding # 각각 지역평균 data['sido_avg'] = data.groupby('sido')['amount'].transform('mean') # 각각 업종평균 data['business_avg'] = data.groupby('business')['amount'].transform('mean') # 각각 지역 x 업종 평균 data['sido_n_business_avg'] = data.groupby(['sido', 'business'])['amount'].transform('mean') # Test 구간인, 2019년 3월4월5일 치, 빈 데이터셋 만들기 for i in tqdm(data.store_id.unique()): X_test_blank = pd.DataFrame( [[0 for i in range(15)] for i in range(3)], columns=['store_id', 'year', 'month', 'amount', 'sido', 'business', 'amount_lag_1', 'amount_lag_2', 'amount_lag_3', 'amount_lag_6', 'rolling_mean_3', 'rolling_mean_6', 'sido_avg', 'business_avg', 'sido_n_business_avg'] ) store = data[data.store_id == i] X_test_blank['store_id'] = i X_test_blank['sido'] = store.sido.unique()[0] X_test_blank['business'] = store.business.unique()[0] X_test_blank['year'] = 2019 X_test_blank['month'] = [3,4,5] X_test_blank['sido_avg'] = store.sido_avg.unique()[0] X_test_blank['business_avg'] = store.business_avg.unique()[0] X_test_blank['sido_n_business_avg'] = store.sido_n_business_avg.unique()[0] data = pd.concat( [data, X_test_blank], axis=0) new_data = data return new_datagroup1_fin_testadd = concat_test_set(group1_fin) 12345678910cos = group1_fin_testaddcos = cos.reset_index()cos = cos.drop('index', axis=1)cos.columns# ['store_id', 'year', 'month', 'amount', 'sido', 'business',# 'amount_lag_1', 'amount_lag_2', 'amount_lag_3', 'amount_lag_6',# 'rolling_mean_3', 'rolling_mean_6', 'sido_avg', 'business_avg',# 'sido_n_business_avg'] 1234567891011121314151617181920### Data Split# test #2106 #2019년 3,4,5월cos1_test = cos[ (cos['year'] == 2019) &amp; ((cos['month'] == 3) | (cos['month'] == 4) | (cos['month'] == 5)) ]# valid #1366 #2019년 1,2월 cos2_valid = cos[ (cos['year'] == 2019) &amp; ((cos['month'] == 1) | (cos['month'] == 2)) ]# train #20356 #2016년 ~ 2018년cos3_train = cos[ (cos['year'] == 2018)| (cos['year'] == 2017) | (cos['year'] == 2016) ]# Data split X_train = cos3_train.drop(['sido', 'business','amount'], axis=1)y_train = cos3_train['amount']X_valid = cos2_valid.drop(['sido', 'business','amount'], axis=1)y_valid = cos2_valid['amount']X_test = cos1_test.drop(['sido', 'business','amount'], axis=1)y_test = cos1_test['amount'] 1234567891011121314151617181920212223# Train and validatemodel = LGBMRegressor( n_estimators=1000, learning_rate=0.01, subsample=0.8, colsample_bytree=0.8, max_depth=8, num_leaves=50, min_child_weight=300)model.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_valid,y_valid)], eval_metric='mae', verbose=20, early_stopping_rounds=200)eval_preds = model.predict(X_test)X_test_result = X_test.copy()X_test_result['pred'] = eval_preds### LGBM 결과치 group1_submission = X_test_result.groupby('store_id')['pred'].sum()group1_submission = group1_submission.reset_index()group1_submission.columns = ['store_id','amount']group1_submission.to_csv(&quot;submission_LGBM.csv&quot;, mode='w') 123456789101112131415161718192021222324252627282930# LGBM - Grid Searchfrom sklearn.metrics import mean_squared_errorfrom sklearn.metrics import r2_score, mean_squared_error, make_scorerRMSE = mean_squared_error(y_test, pred)**0.5import sklearn.metricssklearn.metrics.SCORERS.keys()# Grid Searchfrom sklearn.model_selection import GridSearchCVgridParams = { 'learning_rate': [0.01, 0.1, 0.2 ], 'n_estimators': [8,32,64,100], 'num_leaves': [6,12,32] , 'min_data_in_leaf' : [8, 20, 36], 'max_bin':[255, 510], 'random_state' : [500], 'colsample_bytree' : [0.64, 0.65, 0.66], 'subsample' : [0.7,0.75], 'reg_alpha' : [1,1.2], 'reg_lambda' : [1,1.2,1.4], }grid_cv = GridSearchCV(model, param_grid = gridParams, n_jobs=-1, scoring='neg_mean_absolute_error')grid_cv.fit(X_train, y_train.values.ravel())grid_pred = grid_cv.predict(X_test)grid_pred.to_csv(&quot;Grid_sub.csv&quot;, mode='w') Modeling_MA&amp;EPMA&amp;ARIMAMoving Average123456789101112131415161718192021222324252627282930313233343536373839404142434445def dataset_month(data): # raw data 를 month 와 year 기준으로 resampling dataset =pd.DataFrame([]) for i in tqdm(data.store_id.unique()): store_zero = data[data.store_id==i] store_zero['transacted_date'] = pd.to_datetime(store_zero['transacted_date']) pd.to_datetime(store_zero['transacted_date']) store_zero['month'] =store_zero['transacted_date'].dt.month store_zero['year'] =store_zero['transacted_date'].dt.year store_zero_1 = store_zero.drop(['card_id','card_company','transacted_date','transacted_time', 'installment_term', 'region','type_of_business'],axis=1) store_zero_group = store_zero_1.groupby(['store_id','year','month'] ).sum().reset_index() store_zero_group['amount'].sum() #24174471 dataset= pd.concat([dataset, store_zero_group],axis=0) return datasetd1 = dataset_month(sales)def MA_dataset(data): # 이동평균(3,5) 변수 추가 blank = pd.DataFrame([]) for i in tqdm(data.store_id.unique() ): store = data[data.store_id == i] #rolling mean 3,5 windows = [3,5] for window in windows: store['rolling_mean_' + str(window)] =store.amount.rolling(window=window).mean() blank = pd.concat([blank,store], axis=0) blank = blank.reset_index() blank = blank.drop( ['index'], axis=1) for j in blank.store_id.unique(): X_test_blank = pd.DataFrame( [[0 for i in range(6)] for i in range(3)], columns=['store_id', 'year', 'month', 'amount','rolling_mean_3','rolling_mean_5' ] ) X_test_blank.store_id = j blank = pd.concat( [blank, X_test_blank], axis=0) return blanknew = MA_dataset(d1) &lt;단순이동평균&gt; 12345678910111213141516171819202122232425def MA_prediction(data): # 이동평균(3) 을 활용한 예측 모델링, 가중치값 1/3 blank = pd.DataFrame([]) for i in tqdm(data.store_id.unique()): store_id = i store = data[data['store_id'] == i] # MA Caculation ma1 = store.rolling_mean_3[-4:-3].values store.amount[-3:-2] = ma1 store.amount[-2:-1] = (ma1 + store.amount[-4:-3].values + store.amount[-5:-4].values )*1/3 store.amount[-1:] = (store.amount[-2:-1].values + ma1 + store.amount[-4:-3].values )*1/3 store_pred = store.amount[-3:].sum() dictA = { 'store_id' : [store_id], 'amount' : [store_pred] } df1 = pd.DataFrame( dictA ) blank = pd.concat([blank,df1],axis=0) return blank MA_result_2 = MA_prediction(new)MA_result_2.to_csv(&quot;submission_MA3_1015.csv&quot;, mode='w') &lt;지수이동평균&gt; Exponential Moving Average1234567891011121314151617181920212223def Exponential_MA_prediction(data,n): # 지수이동평균의 경우 예측모형으로 사용할 때의 가중치(w) 값은 2/(n+1) blank = pd.DataFrame([]) for i in tqdm(data.store_id.unique()): store_id = i store = data[data['store_id'] == i] store['exponential_MA_5'] = store.amount.ewm(span=n, adjust=False).mean() exma = store.exponential_MA_5[-4:-3].values exma_sum = exma*3 dictA = { 'store_id' : [i], 'amount' : exma_sum } df1 = pd.DataFrame( dictA ) blank = pd.concat([blank,df1],axis=0) return blankEX_MA_pred_5 = Exponential_MA_prediction(new,5)EX_MA_pred_3 = Exponential_MA_prediction(new,3)EX_MA_pred_5.to_csv(&quot;submission_EX_MA5_1015_1.csv&quot;, mode='w')EX_MA_pred_3.to_csv(&quot;submission_EX_MA3_1015_1.csv&quot;, mode='w') ARIMA1234567891011121314151617181920212223242526272829303132333435363738394041424344# ARIMA Model # 각각의 상점마다 다른 pdq 값을 적용하기 위해서, autoarima model과 유사한 알고리즘을 사용하였다. import itertoolsp = [0,1,2]d = [0,1,2]q = [0,1,2]pdq = list(itertools.product(p, d, q))from statsmodels.tsa.arima_model import ARIMAdef arima_modeling_5(data): arima_pred_arr = np.array([]) for i in tqdm(data.store_id.unique()): data_set = data[data.store_id == i] best_score = 10000000000 best_param = 0 for param in pdq: try: arima_model = ARIMA(data_set.amount.values, order=param) result = arima_model.fit() if result.aic &lt; best_score: best_score = result.aic best_param = param except: continue arima_model = ARIMA(data_set.amount.values, order=best_param) arima_result = arima_model.fit() arima_pred = arima_result.forecast(3)[0] arima_pred_arr = np.concatenate((arima_pred_arr, np.array([arima_pred.sum()]))) return arima_pred_arrarima_result_month_all_data = arima_modeling_5(d1)arima_result_month_all_data = pd.DataFrame(arima_result_month_all_data)arima_result_month_all_data.to_csv(&quot;arima_result_submission_1015.csv&quot;, mode='w') Conclusion 최종결과 Model MAE(Mean Absolute Error,평균절대오차) Public Board Rank Exponential_Moving Average(5) 765,887 9등(최종) Exponential_Moving Average(3) 784,219 18등 Simple_Moving_Average(3) 810,333 25등 ARIMA_n_LGBM 1,768,874 93등 ARIMA 1,062,635 50등 LGBM 2,481,832 102등 Data size 해당 데이터의 경우 data size 에 관한 issue 가 있었다.처음 제공된 raw data 을 탐색하였을 때, 그 사이즈가 6,556,613 rows 로 머신러닝에 활용하기에 충분한 볼륨이 있어 보였다.하지만 데이터는 개별 카드의 결제 건당 매출을 기준으로 작성되었고, 대회의 최종목표는 상점별 매출을 예측하는 것이었으므로resampling 과정이 불가피 하였다. card_id 는 store_id 기준으로, day 는 month 기준으로 데이터 전처리를 하였을 때,6,556,613 rows 에서 60,232 rows 로 데이터의 사이즈가 약 1/100 가량 축소되었다. Data variables raw data 에서 주어진 column 값들 중, store 를 구분 할 수 있는 categorical variable 은 ‘region’ 과 ‘type_of_business’두 가지가 유일하였다. 하지만 그마저도 결측치의 비율이 65%에 달하여 전체 데이터셋에 일반화하여 적용하기에 무리가 있었다.따라서 데이터셋을 결측치 유무에 따른 두 가지의 그룹으로 나누어서 분석을 진행하였다. Modeling 3.1 LGBM ‘region’ 과 ‘type_of_business’가 존재하는 데이터셋을 group1 로 지정하고 모델링을 진행하였다. group1 의 경우에,146개의 전체 업종 중에서 매출 top10 업종이 전체 매출에서 차지하는 비중은 50% 였으며, top1 인 한식음식점업의경우에 전체 매출의 16%를 차지하였다. 따라서 한식음식점 및 상위 업종의 매출 trend 및 seasonality 를 중심으로모델링 하기 위하여, Feature Engineering 과정에서 이동평균과 lag 을 적극적으로 활용하여 변수에 추가하였다.결과적으로 LGBM 모델링의 MAE 값은 2,481,832 로 대회 Public Board 기준으로는 102등(최하위권)을 기록하였다.tree 모델을 사용하기에는 다소 부족한 데이터의 수와, categorical variable 의 부재로 인한 결과로 추측된다. 3.2 Simple Moving Average, Exponential Moving Average and ARIMA 머신러닝을 활용한 모델링이 힘들기 때문에, 전통적인 방식의 시계열 분석 방법들을 시도해 보았다.결론적으로 Exponential Moving Average(5) 에 의한 결과값이 가장 좋았다. MAE 값은 765,887 로 대회 Public Board 기준으로 9등(최상위권)을 달성하였고, Simple Moving Average(3)가 25등, ARIMA가 50등의 성적을 보여주었다. 현재 데이터의 특성을 전적으로 보여주는 결과라고 생각을 한다. 각각의 모델은 아래와 같은 특징을 지닌다.Simple Moving Average (단순이동평균) 의 경우에, 각각의 시계열에 동일한 가중치를 두고 종속변수의 평균값 및 예측값을 계산한다.따라서 계절성이 적고 추세성이 강한 데이터를 잘 예측할 확률이 높다. Exponential MA (지수이동평균) 의 경우에는 예측시점에 가까운 데이터일수록 가중치를 강하게 가져가고, 예측시점에서 먼 데이터일수록가중치를 낮게 준다. 따라서 단순이동평균에 비해서 최근의 데이터 변화에 민감한 예측을 하게 된다. ARIMA 모델의 경우에는 예측값들이 가지는 관계성에 비중을 두는 모델이다. 따라서 패턴성이 강하고 시계열이 길게 주어질 수록예측력이 높아지는 경향이 있다. 앞서 group1 에 관한 분석과 같이, 한식음식점업이 업종이 드러나지 않은 데이터 전체에서 차지하는 비중이 매우 크다고 가정할 때한식음식점업이 가지는 데이터의 패턴은 데이터 전체에 영향을 끼칠 확률이 높다. 따라서 계절성과 추세성이 약한 요식업의 특성상,Exponential MA (지수이동평균) 모델이 현 데이터를 가장 잘 예측한 것이 아닐까 추론해 보았다.","link":"/2020/10/18/Project_Card_Sales_1019_1/"}],"tags":[{"name":"Python","slug":"Python","link":"/tags/Python/"}],"categories":[{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Cos Pro 1급","slug":"Python/Cos-Pro-1급","link":"/categories/Python/Cos-Pro-1%EA%B8%89/"},{"name":"Kaggle","slug":"Kaggle","link":"/categories/Kaggle/"},{"name":"Time Series","slug":"Time-Series","link":"/categories/Time-Series/"}]}