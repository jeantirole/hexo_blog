{"pages":[],"posts":[{"title":"Project_Dacon_Corona_EDA(benchmark)","text":"제주 신용카드 빅데이터 경진대회▶ 해결해야 하는 문제 2020.04, 2020.07 기간 내 지역, 업종 별 월간 총 카드 사용 금액 예측 ▶ 데이터 요약 사용지역, 업종, 거주지역 등 준식별자로 구성된 신용카드 사용 내역 (출처: BC카드) 2020.07.28 09:00, 2020.04 기간 내 신용카드 사용 내역 데이터 공개 201901-202003.csv (2.07 GB) 2019.01 ~ 2020.03 기간 내 신용카드 사용 내역 데이터 202004.csv (116 MB) 2020.04 기간 내 신용카드 사용 내역 데이터 submission.csv (64 KB) 제출 양식 12import pandas as pd import numpy as np 123456import ospath = &quot;C:\\\\Eric\\\\Projects\\\\Dacon_Corona_TimeSeries&quot;file_list = os.listdir(path)print (&quot;file_list: {}&quot;.format(file_list)) file_list: ['201901-202003.csv', '202004.csv', 'submission.csv'] 123card = pd.read_csv(&quot;C:\\\\Eric\\\\Projects\\\\Dacon_Corona_TimeSeries\\\\201901-202003.csv&quot;)card_new = pd.read_csv(&quot;C:\\\\Eric\\\\Projects\\\\Dacon_Corona_TimeSeries\\\\202004.csv&quot;)sub = pd.read_csv(&quot;C:\\\\Eric\\\\Projects\\\\Dacon_Corona_TimeSeries\\\\submission.csv&quot;) 1card.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 24697792 entries, 0 to 24697791 Data columns (total 12 columns): REG_YYMM int64 CARD_SIDO_NM object CARD_CCG_NM object STD_CLSS_NM object HOM_SIDO_NM object HOM_CCG_NM object AGE object SEX_CTGO_CD int64 FLC int64 CSTMR_CNT int64 AMT int64 CNT int64 dtypes: int64(6), object(6) memory usage: 2.2+ GB 12# 01번째 열 : 2019년 1월부터 2020년 3월까지 월 단위. card['REG_YYMM'].unique() array([201901, 201902, 201903, 201904, 201905, 201906, 201907, 201908, 201909, 201910, 201911, 201912, 202001, 202002, 202003], dtype=int64) 12# 02번째 열 : 전국 시도 17개len(card['CARD_SIDO_NM'].unique()) 17 12# 03번째 열 : 전국 시군구 227개len(card['CARD_CCG_NM'].unique()) 227 12# 04번째 열 : 업종 41개 len(card['STD_CLSS_NM'].unique()) 41 12# 05번째 열 : 17개len(card['HOM_SIDO_NM'].unique()) 17 12# 06번째 열 : 227개 len(card['HOM_CCG_NM'].unique()) 227 12# 07번째 열 : 고객 나이대 card['AGE'].unique() array(['20s', '30s', '40s', '50s', '60s', '70s', '10s'], dtype=object) 12# 08번째 열 : 성별card['SEX_CTGO_CD'].unique() array([1, 2], dtype=int64) 12# 09번째 열 : 생애주기에 따른 구분 card['FLC'].unique() array([1, 2, 3, 4, 5], dtype=int64) 12# 10번째 열 : 이용고객수 (명)card['CSTMR_CNT'].describe() count 2.469779e+07 mean 6.196855e+01 std 3.559175e+02 min 3.000000e+00 25% 4.000000e+00 50% 8.000000e+00 75% 2.400000e+01 max 3.281300e+04 Name: CSTMR_CNT, dtype: float64 1card.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } REG_YYMM CARD_SIDO_NM CARD_CCG_NM STD_CLSS_NM HOM_SIDO_NM HOM_CCG_NM AGE SEX_CTGO_CD FLC CSTMR_CNT AMT CNT 0 201901 강원 강릉시 건강보조식품 소매업 강원 강릉시 20s 1 1 4 311200 4 1 201901 강원 강릉시 건강보조식품 소매업 강원 강릉시 30s 1 2 7 1374500 8 2 201901 강원 강릉시 건강보조식품 소매업 강원 강릉시 30s 2 2 6 818700 6 3 201901 강원 강릉시 건강보조식품 소매업 강원 강릉시 40s 1 3 4 1717000 5 4 201901 강원 강릉시 건강보조식품 소매업 강원 강릉시 40s 1 4 3 1047300 3 1 1 1card_new.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 1350322 entries, 0 to 1350321 Data columns (total 12 columns): REG_YYMM 1350322 non-null int64 CARD_SIDO_NM 1350322 non-null object CARD_CCG_NM 1345164 non-null object STD_CLSS_NM 1350322 non-null object HOM_SIDO_NM 1350322 non-null object HOM_CCG_NM 1342875 non-null object AGE 1350322 non-null object SEX_CTGO_CD 1350322 non-null int64 FLC 1350322 non-null int64 CSTMR_CNT 1350322 non-null int64 AMT 1350322 non-null int64 CNT 1350322 non-null int64 dtypes: int64(6), object(6) memory usage: 123.6+ MB 1sub.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 1394 entries, 0 to 1393 Data columns (total 5 columns): id 1394 non-null int64 REG_YYMM 1394 non-null int64 CARD_SIDO_NM 1394 non-null object STD_CLSS_NM 1394 non-null object AMT 1394 non-null int64 dtypes: int64(3), object(2) memory usage: 54.5+ KB 1 1 1 1 1","link":"/2020/08/24/Project_Dacon_Corona_TimeSeries/"},{"title":"Project_Kaggle_M5_EDA(benchmark)","text":"M5 Forecasting ChallengePart: Exploratory Data Analysis Topic: Wal-Mart Sales Prediction Details: US States( CA, TX, WI) which include item level, department, categories and store details. explanotory variables such as price, promotions, day of the week and speical events. 123456789import pandas as pdimport numpy as npimport matplotlib.pylab as pltimport seaborn as snsfrom itertools import cyclepd.set_option('max_columns', 50)plt.style.use('bmh')color_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color']) 12345678# # Read in the data# INPUT_DIR = '../input/m5-forecasting-accuracy'# cal = pd.read_csv(f'{INPUT_DIR}/calendar.csv')# stv = pd.read_csv(f'{INPUT_DIR}/sales_train_validation.csv')# ss = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')# sellp = pd.read_csv(f'{INPUT_DIR}/sell_prices.csv') 12345stv = pd.read_csv('C:\\\\Eric\\\\Projects\\\\Kaggle-M5\\Dataset\\\\sales_train_validation.csv')cal = pd.read_csv('C:\\\\Eric\\\\Projects\\\\Kaggle-M5\\Dataset\\\\calendar.csv')sellp = pd.read_csv('C:\\\\Eric\\\\Projects\\\\Kaggle-M5\\Dataset\\\\sell_prices.csv') We are given historic sales data in the sales_train_validation dataset. rows exist in this dataset for days d_1 to d_1913. We are given the department, category, state, and store id of the item. d_1914 - d_1941 represents the validation rows which we will predict in stage 1 d_1942 - d_1969 represents the evaluation rows which we will predict for the final competition standings. 1stv.head() Visualizing the data for a single item Lets take a random item that sell a lot and see how it’s sales look across the training data. FOODS_3_090_CA_3_validation sells a lot Note there are days where it appears the item is unavailable and sales flatline 1d_cols = [c for c in stv.columns if 'd_' in c] 12345678910111213141516171819# 아이템(행)들의 dayily sale 가 각 날짜 (열)에 입력되어 있다. d1~ d1941, d로 시작하는 모든 칼럼명을 list형태로 저장하였다. # Below we are chaining the following steps in pandas:# 1. Select the item.# 2. Set the id as the index, Keep only sales data columns# 3. Transform so it's a column# 4. Plot the data# 특정 id를 인덱스로 하고, 각 col이 가로로 정렬. # .T를 통해서, 가로행을 세로행으로 전환# .plot으로 plt 그래프를 그린다. stv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'] \\ .set_index('id')[d_cols] \\ .T \\ .plot(figsize=(15, 5), title='FOODS_3_090_CA_3 sales by &quot;d&quot; number', color=next(color_cycle))plt.legend('')plt.show() Merging the data with real dates We are given a calendar with additional information about past and future dates. The calendar data can be merged with our days data From this we can find weekly and annual trends 1234# Calendar data looks like this (only showing columns we care about for now)# Calendar 파일에서, 해당 리스트에 해당하는 col들만 표시 cal[['d','date','event_name_1','event_name_2', 'event_type_1','event_type_2', 'snap_CA']].head() 1stv.head() 12345678910# Merge calendar on our items' dataexample = stv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'][d_cols].Texample = example.rename(columns={8412:'FOODS_3_090_CA_3'}) # Name it correctlyexample = example.reset_index().rename(columns={'index': 'd'}) # make the index &quot;d&quot;example = example.merge(cal, how='left', validate='1:1')example.set_index('date')['FOODS_3_090_CA_3'] \\ .plot(figsize=(15, 5), color=next(color_cycle), title='FOODS_3_090_CA_3 sales by actual sale dates')plt.show() 123# FOODS_3_090_CA_3_validation id를 선택, col들을 호출, 해당 col 들을 가로에서 세로로 정렬 example = stv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'][d_cols].Texample.head() 12example = example.rename(columns={8412:'FOODS_3_090_CA_3'}) # Name it correctlyexample = example.reset_index().rename(columns={'index': 'd'}) # make the index &quot;d&quot; 1example.head() 12# Calendar 파일을 탐색 d 열과 date 열에 주목 cal.head() 12# Example 에 Calendar 를 Merge example = example.merge(cal, how='left', validate='1:1') 1example.head() 12345# Select more top selling examplesexample2 = stv.loc[stv['id'] == 'HOBBIES_1_234_CA_3_validation'][d_cols].Texample2 = example2.rename(columns={6324:'HOBBIES_1_234_CA_3'}) # Name it correctlyexample2 = example2.reset_index().rename(columns={'index': 'd'}) # make the index &quot;d&quot;example2 = example2.merge(cal, how='left', validate='1:1') 1234example3 = stv.loc[stv['id'] == 'HOUSEHOLD_1_118_CA_3_validation'][d_cols].Texample3 = example3.rename(columns={6776:'HOUSEHOLD_1_118_CA_3'}) # Name it correctlyexample3 = example3.reset_index().rename(columns={'index': 'd'}) # make the index &quot;d&quot;example3 = example3.merge(cal, how='left', validate='1:1') #Sales broken down by time variables Now that we have our example item lets see how it sells by: Day of the week Month Year 1234examples = ['FOODS_3_090_CA_3','HOBBIES_1_234_CA_3','HOUSEHOLD_1_118_CA_3']example_df = [example, example2, example3] 1example.head(5) 1234567891011121314151617181920212223242526272829examples = ['FOODS_3_090_CA_3','HOBBIES_1_234_CA_3','HOUSEHOLD_1_118_CA_3']example_df = [example, example2, example3]for i in [0, 1, 2]: fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3)) example_df[i].groupby('wday').mean()[examples[i]] \\ .plot(kind='line', title='average sale: day of week', lw=5, color=color_pal[0], ax=ax1) example_df[i].groupby('month').mean()[examples[i]] \\ .plot(kind='line', title='average sale: month', lw=5, color=color_pal[4], ax=ax2) example_df[i].groupby('year').mean()[examples[i]] \\ .plot(kind='line', lw=5, title='average sale: year', color=color_pal[2], ax=ax3) fig.suptitle(f'Trends for item: {examples[i]}', size=20, y=1.1) plt.tight_layout() plt.show() #Lets look at a lot of different items! Lets put it all together to plot 20 different items and their sales Some observations from these plots: It is common to see an item unavailable for a period of time. Some items only sell 1 or less in a day, making it very hard to predict. Other items show spikes in their demand (super bowl sunday?) possibly the “events” provided to us could help with these. 12345678twenty_examples = stv.sample(20, random_state=529) \\ .set_index('id')[d_cols] \\ .T \\ .merge(cal.set_index('d')['date'], left_index=True, right_index=True, validate='1:1') \\ .set_index('date') 1stv.sample(20, random_state=529).head(5) # 랜덤으로 샘플 id 고르기 12345678910fig, axs = plt.subplots(10, 2, figsize=(15, 20))axs = axs.flatten()ax_idx = 0for item in twenty_examples.columns: twenty_examples[item].plot(title=item, color=next(color_cycle), ax=axs[ax_idx]) ax_idx += 1plt.tight_layout()plt.show() 1fig, axs = plt.subplots(10,2, figsize=(15,20)) # 10행2열의 subplot 을 표시하기 1twenty_examples.columns # cols 는 현재 id list 와 같다. 1axs = axs.flatten() # axs 의 2차원 행열을 1차원으로 변환시켰다. 12345678ax_idx = 0 for item in twenty_examples.columns: twenty_examples[item].plot(title=item, color=next(color_cycle), ax=axs[ax_idx]) # ax_idx += 1 # ax_idx 를 활용해서 plot 각각에 번호를 부여할 것이다. plt.tight_layout()plt.show() Combined Sales over Time by Type We have several item types: Hobbies Household Foods Lets plot the total demand over time for each type 1stv['cat_id'].unique() 1234stv.groupby('cat_id').count()['id'] \\ .sort_values() \\ .plot(kind='barh', figsize=(15, 5), title='Count of Items by Category')plt.show() 1stv.columns # col 나열 1stv.groupby('cat_id').count() # cat id 종류를 찾아보기로 결정 1stv.groupby('cat_id').count()['id'] # groupby 활용해서 cat id 에 종류별 숫자 카운트 1234stv.groupby('cat_id').count()['id'] \\ .sort_values() \\ .plot(kind='barh', figsize=(15, 5), title='Count of Items by Category')plt.show() 123456789101112131415161718past_sales = stv.set_index('id')[d_cols] \\ .T \\ .merge(cal.set_index('d')['date'], left_index=True, right_index=True, validate='1:1') \\ .set_index('date')for i in stv['cat_id'].unique(): items_col = [c for c in past_sales.columns if i in c] past_sales[items_col] \\ .sum(axis=1) \\ .plot(figsize=(15, 5), alpha=0.8, title='Total Sales by Item Type')plt.legend(stv['cat_id'].unique())plt.show() 1past_sales.head() 123456789for i in stv['cat_id'].unique(): # 순서대로 category id Hobbies, Household, Foods 를 list items_col = [c for c in past_sales.columns if i in c] # past_sales 컬럼명에 있는 id 에 위에서 지정한 cat_id가 포함되어 있는 id만 선택 past_sales[items_col] \\ # 첫번째 카테고리인 Hobbies 가 포함된 id 들만 표시 .sum(axis=1) \\ # Hobbies 를 합계 .plot(figsize=(15, 5), alpha=0.8, title='Total Sales by Item Type')plt.legend(stv['cat_id'].unique())plt.show() Rollout of items being sold We can see the some items come into supply that previously didn’t exist. Similarly some items stop being sold completely. Lets plot the sales, but only count if item is selling or not selling (0 -&gt; not selling, &gt;0 -&gt; selling) This plot shows us that many items are being slowly introduced into inventory, so many of them will not register a sale at the beginning of the provided data. 1234567891011121314# clip() 함수 &lt;= np.clip(배열, 최소값 기준, 최대값 기준) 0보다 작은 수를 0으로 반환 # 아까의 sum 대신에 mean 함수를 넣어서 각 상품별 평균 sales 를 뽑았다. past_sales_clipped = past_sales.clip(0, 1)for i in stv['cat_id'].unique(): items_col = [c for c in past_sales.columns if i in c] (past_sales_clipped[items_col] \\ .mean(axis=1) * 100) \\ .plot(figsize=(15, 5), alpha=0.8, title='Inventory Sale Percentage by Date', style='.')plt.ylabel('% of Inventory with at least 1 sale')plt.legend(stv['cat_id'].unique())plt.show() Sales by StoreWe are provided data for 10 unique stores. What are the total sales by stores? Note that some stores are more steady than others. CA_2 seems to have a big change occur in 2015 1234567891011store_list = sellp['store_id'].unique() #앞서와 같은 방식으로, 이번에는 cat_id 가 아닌, store_id 로 정렬 for s in store_list: store_items = [c for c in past_sales.columns if s in c] past_sales[store_items] \\ .sum(axis=1) \\ .rolling(90).mean() \\ .plot(figsize=(15, 5), alpha=0.8, title='Rolling 90 Day Average Total Sales (10 stores)')plt.legend(store_list)plt.show() Looking at the same data a different way, we can plot a rolling 7 day total demand count by store. Note clearly that some stores have abrupt changes in their demand, it could be that the store expanded or a new competitor was built near by. Either way this is imporant to note when creating predictive models about demand pattern. 12345678910111213141516171819# week 기준으로 rolling mean 을 보기 위해서 rolling 7 을 하였다. fig, axes = plt.subplots(5, 2, figsize=(15, 10), sharex=True)axes = axes.flatten()ax_idx = 0for s in store_list: store_items = [c for c in past_sales.columns if s in c] past_sales[store_items] \\ .sum(axis=1) \\ .rolling(7).mean() \\ .plot(alpha=1, ax=axes[ax_idx], title=s, lw=3, color=next(color_cycle)) ax_idx += 1# plt.legend(store_list)plt.suptitle('Weekly Sale Trends by Store ID')plt.tight_layout()plt.show() Sale Prices We are given historical sale prices of each item. Lets take a look at our example item from before. It looks to me like the price of this item is growing. Different stores have different selling prices. 123456789101112131415fig, ax = plt.subplots(figsize=(15, 5))stores = []for store, d in sellp.query('item_id == &quot;FOODS_3_090&quot;').groupby('store_id'): d.plot(x='wm_yr_wk', y='sell_price', style='.', color=next(color_cycle), figsize=(15, 5), title='FOODS_3_090 sale price over time', ax=ax, legend=store) stores.append(store) plt.legend()plt.legend(stores)plt.show() 1sellp.head() # item 별 가격이 정리되어 있는 파일 1sellp['Category'] = sellp['item_id'].str.split('_', expand=True)[0] # Category 컬럼을 만들어서, 해당 칼럼에 아이템 id 의 unique value 를 부여한다. 1sellp.head() 12345678910111213sellp['Category'] = sellp['item_id'].str.split('_', expand=True)[0]fig, axs = plt.subplots(1, 3, figsize=(15, 4))i = 0for cat, d in sellp.groupby('Category'): ax = d['sell_price'].apply(np.log1p) \\ .plot(kind='hist', bins=20, title=f'Distribution of {cat} prices', ax=axs[i], color=next(color_cycle)) ax.set_xlabel('Log(price)') i += 1plt.tight_layout() 1sellp.groupby('Category').tail() 1sellp.Category.unique() 12for c in sellp.groupby('Category'): # groupby 함수와 for문을 함께 사용하면은 다음과 ('해당열의 고유값', '해당열의 고유값을 만족시키는 데이터의 집합') 으로 출력이 된다. print(c) Ref: https://www.kaggle.com/robikscube/m5-forecasting-starter-data-exploration/data Thanks you, Rob Mulla.","link":"/2020/09/17/Project_Kaggle_M5_EDA(benchmark)_02/"},{"title":"hmm","text":"","link":"/2020/09/18/hmm/"}],"tags":[{"name":"Corona","slug":"Corona","link":"/tags/Corona/"}],"categories":[{"name":"Dacon","slug":"Dacon","link":"/categories/Dacon/"},{"name":"Kaggle","slug":"Kaggle","link":"/categories/Kaggle/"},{"name":"Corona","slug":"Dacon/Corona","link":"/categories/Dacon/Corona/"},{"name":"M5","slug":"Kaggle/M5","link":"/categories/Kaggle/M5/"}]}