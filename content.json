{"pages":[],"posts":[{"title":"Cos Pro Python 1급 1차 01번 문제풀이","text":"Contents of table: 문제내용 정답코드 Topic:배달음식점 주문메뉴 만들기 Description:파이썬의 전형적인 메뉴판 만들기 문제를, 클래스를 활용해서 작성할 수 있는가를테스트 하는 문제이다. 해당 문제는 특히 추상클래스에 대한 이해 여부가 포인트이다.논리구조는 특별할 것이 없다. 문제내용 DeliveryStore : DeliveryStore는 배달 음식점의 인터페이스입니다. 배달 음식점은 set_order_list와 get_total_price 함수를 구현해야 합니다. set_order_list 함수는 주문 메뉴의 리스트를 매개변수로 받아 저장합니다. get_total_price 함수는 주문받은 음식 가격의 총합을 return 합니다. Food : Food는 음식을 나타내는 클래스입니다. 음식은 이름(name)과 가격(price)으로 구성되어있습니다. PizzaStore PizzaStore는 피자 배달 전문점을 나타내는 클래스이며 DeliveryStore 인터페이스를 구현합니다. menu_list는 피자 배달 전문점에서 주문 할 수 있는 음식의 리스트를 저장합니다. order_list는 주문 받은 음식들의 이름을 저장합니다. set_order_list 함수는 주문 메뉴를 받아 order_list에 저장합니다. get_total_price 함수는 order_list에 들어있는 음식 가격의 총합을 return 합니다. 주문 메뉴가 들어있는 리스트 order_list가 매개변수로 주어질 때, 주문한 메뉴의 전체 가격을 return 하도록 solution 함수를 작성하려고 합니다. 위의 클래스 구조를 참고하여 주어진 코드의 빈칸을 적절히 채워 전체 코드를 완성해주세요. 매개변수 설명주문 메뉴가 들어있는 리스트 order_list가 solution 함수의 매개변수로 주어집니다. order_list의 길이는 1 이상 5이하입니댜. order_list에는 주문하려는 메뉴의 이름들이 문자열 형태로 들어있습니다. order_list에는 같은 메뉴의 이름이 중복해서 들어있지 않습니다. 메뉴의 이름과 가격은 PizzaStore의 생성자에서 초기화해줍니다. return 값 설명주문한 메뉴의 전체 가격을 return 해주세요. 예시 order_list return [“Cheese”, “Pineapple”, “Meatball”] 51600 정답코드 1from abc import * # 추상클래스 import 12345678class DeliveryStore(metaclass=ABCMeta): # 추상클래스에 앞으로 만들 함수목록 지정 @abstractmethod def set_order_list(self, order_list): pass @abstractmethod def get_total_price(self): pass 1234class Food: # name, price 변수를 만들어줄 class 작성 def __init__(self, name, price): self.name = name self.price = price 123456789101112131415161718192021class PizzaStore(DeliveryStore): # (name, price)가 리스트의 요소로 이루어진 menu_list def __init__(self): menu_names = [&quot;Cheese&quot;, &quot;Potato&quot;, &quot;Shrimp&quot;, &quot;Pineapple&quot;, &quot;Meatball&quot;] menu_prices = [11100, 12600, 13300, 21000, 19500]; self.menu_list = [] for i in range(5): self.menu_list.append(Food(menu_names[i], menu_prices[i])) self.order_list = [] def set_order_list(self, order_list): # order_list 변수작성 for order in order_list: self.order_list.append(order) def get_total_price(self): # menu_list의 name변수와 order_list의 요소를 비교하여 동일 요소일 경우, price 값을 total_price에 누적합 return total_price = 0 for order in self.order_list: for menu in self.menu_list: if order == menu.name: total_price += menu.price return total_price 123456def solution(order_list): #order_list를 넣었을 때, total price가 나올 수 있도록 마무리 delivery_store = PizzaStore() delivery_store.set_order_list(order_list) total_price = delivery_store.get_total_price() return total_price","link":"/2020/09/18/1%EA%B8%89-1%EC%B0%A8-01%EB%B2%88/"},{"title":"Cos Pro Python 1급 1차 02번 문제풀이","text":"Contents of table: 문제내용 정답코드 Topic:해밍거리 구하기 Review:주어진 두 개의 문자열 리스트의 길이를 활용하고 인덱스값을 이용해서 두 리스트의 요소들을 비교할 수 있는가?라는 것을 묻는 문제이다. 문제내용#문제2해밍 거리(Hamming distance)란 같은 길이를 가진 두 개의 문자열에서 같은 위치에 있지만 서로 다른 문자의 개수를 뜻합니다. 예를 들어 두 2진수 문자열이 “10010”과 “110”이라면, 먼저 두 문자열의 자릿수를 맞추기 위해 “110”의 앞에 0 두개를 채워 “00110”으로 만들어 줍니다. 두 2진수 문자열은 첫 번째와 세 번째 문자가 서로 다르므로 해밍 거리는 2입니다. 1001 0 0011 0 두 2진수 문자열 binaryA, binaryB의 해밍 거리를 구하려 합니다. 이를 위해 다음과 같이 간단히 프로그램 구조를 작성했습니다 12341단계. 길이가 더 긴 2진수 문자열의 길이를 구합니다.2단계. 첫 번째 2진수 문자열의 길이가 더 짧다면 문자열의 앞에 0을 채워넣어 길이를 맞춰줍니다.3단계. 두 번째 2진수 문자열의 길이가 더 짧다면 문자열의 앞에 0을 채워넣어 길이를 맞춰줍니다.4단계. 길이가 같은 두 2진수 문자열의 해밍 거리를 구합니다. 두 2진수 문자열 binaryA와 binaryB가 매개변수로 주어질 때, 두 2진수의 해밍 거리를 return 하도록 solution 함수를 작성했습니다. 이때, 위 구조를 참고하여 중복되는 부분은 func_a라는 함수로 작성했습니다. 코드가 올바르게 동작할 수 있도록 빈칸을 알맞게 채워 전체 코드를 완성해주세요. 매개변수 설명두 2진수 문자열 binaryA와 binaryB가 solution 함수의 매개변수로 주어집니다. binaryA의 길이는 1 이상 10 이하입니다. binaryA는 0 또는 1로만 이루어진 문자열이며, 0으로 시작하지 않습니다. binaryB의 길이는 1 이상 10 이하입니다. binaryB는 0 또는 1로만 이루어진 문자열이며, 0으로 시작하지 않습니다. return 값 설명두 2진수 문자열의 해밍 거리를 return 해주세요. 예시 binaryA binaryB return “10010” “110” 2 예시 설명두 2진수의 자릿수는 각각 5와 3입니다. 자릿수를 맞추기 위해 “110” 앞에 0 두 개를 채워주면 “00110”이 됩니다. 이제 두 2진수 문자열의 해밍 거리를 구하면 다음과 같습니다. 1001 0 0011 0 위와 같이 첫 번째와 세 번째 문자가 서로 다르므로, 해밍 거리는 2가 됩니다. 정답코드123456def func_a(bstr, bstr_len): # 문자열의 부족한 길이만큼의 0문자열을 추가하다 padZero = &quot;&quot; #빈 문자열 padSize = bstr_len - len(bstr) #매개변수로 주어진 문자열과, 주어진 문자열 만큼의 차이 for i in range(padSize): #위의 차이 만큼의 문자열 &quot;0&quot;을 빈 문자열 안에 넣어준다. padZero += &quot;0&quot; return padZero + bstr # 변수로 주어진 문자열에, &quot;0&quot;으로 채워진 문자열을 추가한다. 123456789101112def solution(binaryA, binaryB): max_length = max(len(binaryA), len(binaryB)) #두 문자열의 길이 중 더 큰 것을 고른다. if max_length &gt; len(binaryA): #binaryA가 더 작을 경우에, func_a를 이용해서 &quot;0&quot; 문자열을 추가해서 반환해준다. binaryA = func_a(binaryA, max_length) if max_length &gt; len(binaryB): #binaryB가 더 작을 경우. 위와동일. binaryB = fucn_a(binaryB, max_length) hamming_distance = 0 for i in range(max_length): #두 매개변수의 요소들을 훑기 위한 for문. if binaryA[i] != binaryB[i]: hamming_distance += 1 #해밍거리 count return hamming_distance","link":"/2020/09/19/1%EA%B8%89-1%EC%B0%A8-02%EB%B2%88/"},{"title":"Cos Pro Python 1급 1차 03번 문제풀이","text":"Contents of table: 문제내용 정답코드 Topic:계산기 by 문자열 Review:‘enumerate()’ 함수를 적극적으로 활용함. 개인적으로 func_c의 slicing 아이디어가 좋았다. 문제내용문자열 형태의 식을 계산하려 합니다. 식은 2개의 자연수와 1개의 연산자(‘+’, ‘-‘, ‘*’ 중 하나)로 이루어져 있습니다. 예를 들어 주어진 식이 “123+12”라면 이를 계산한 결과는 135입니다. 문자열로 이루어진 식을 계산하기 위해 다음과 같이 간단히 프로그램 구조를 작성했습니다. 1단계. 주어진 식에서 연산자의 위치를 찾습니다.2단계. 연산자의 앞과 뒤에 있는 문자열을 각각 숫자로 변환합니다.3단계. 주어진 연산자에 맞게 연산을 수행합니다. 문자열 형태의 식 expression이 매개변수로 주어질 때, 식을 계산한 결과를 return 하도록 solution 함수를 작성하려 합니다. 위 구조를 참고하여 코드가 올바르게 동작할 수 있도록 빈칸에 주어진 func_a, func_b, func_c 함수와 매개변수를 알맞게 채워주세요. 매개변수 설명문자열 형태의 식 expression이 solution 함수의 매개변수로 주어집니다. expression은 연산자 1개와 숫자 2개가 결합한 형태입니다. 연산자는 ‘+’, ‘-‘, ‘*’만 사용됩니다. 숫자는 1 이상 10,000 이하의 자연수입니다. return 값 설명expression을 계산한 결과를 return 해주세요. 계산 결과는 문자열로 변환하지 않아도 됩니다. 예시 expression return “123+12” 135 예시 설명‘+’를 기준으로 앞의 숫자는 123이고 뒤의 숫자는 12이므로 두 숫자를 더하면 135가 됩니다. 정답코드12345678910111213def func_a(numA, numB, exp): #주어진 문자열에서 사칙연산 기호를 찾아서, 해당 연산실행 if exp == '+': return numA + numB elif exp == '-': return numA - numB elif exp == '*': return numA * numB~~~ ~~~pythondef func_b(exp): #리스트 변수를 idx 와 val로 나누고, 연산기호의 idx 값을 return for index, value in enumerate(exp): if value == '+' or value == '-' or value == '*': return index 1234def func_c(exp, idx): #주어진 연산자의 앞과 뒤의 문자열을 숫자로 반환 numA = int(exp[:idx]) numB = int(exp[idx + 1:]) return numA, numB 12345def solution(expression): exp_index = func_b(expression) #연산 기호의 idx 값을 exp_index 변수에 저장 first_num, second_num = func_c(expression, exp_index) result = func_a(first_num, second_num, expression[exp_index]) return result","link":"/2020/09/20/1%EA%B8%89-1%EC%B0%A8-03%EB%B2%88/"},{"title":"Cos Pro Python 1급 1차 04번 문제풀이","text":"Contents of table: 문제내용 정답코드 Topic:타임머신 Review:#point 1) 연산자의 활용능력. 기호 //, % 을 활용할 수 있는가.#point 2) 숫자에 0을 포함하지 않도록 하는 로직. 정답코드에서는 num // digit % 10 == 0 으로 판단기준을 잡았다. 0이 포함된 숫자는 10의 배수라는 아이디어를 기본으로 num +=1, digit *= 10 을 반복하며 각 자릿수마다 0 이 없도록 while 문을 반복시킴. 문제내용어느 누군가가 타임머신을 타고 과거로 가서 숫자 0이 없는 수 체계를 전파했습니다. 역사가 바뀌어 이제 사람들의 의식 속엔 0이란 숫자가 사라졌습니다. 따라서, 현재의 수 체계는 1, 2, 3, …, 8, 9, 11, 12, …와 같이 0이 없게 바뀌었습니다. 0을 포함하지 않은 자연수 num이 매개변수로 주어질 때, 이 수에 1을 더한 수를 return 하도록 solution 함수를 완성해주세요. 매개변수 설명자연수 num이 solution 함수의 매개변수로 주어집니다. num은 1 이상 999,999,999,999,999,999 이하의 0을 포함하지 않는 자연수입니다. return 값 설명자연수 num에 1을 더한 수를 return 해주세요. 예시 num return 9949999 9951111 예시 설명9,949,999에 1을 더하면 9,950,000이지만 0은 존재하지 않으므로 9,951,111이 됩니다. 정답코드 def solution(num): num += 1 #100을 넣었다고 가정해 볼 때, 101 즉 끝자리 수의 0을 1로 바꿔준다. digit = 1 #임의의 digit 값, 아래 함수 참조하면, 1 =&gt; 10 =&gt; 100, 10의 거듭제곱 형태. while num // digit % 10 == 0: # num 값이 10의 배수라고 가정 했을 때, num += digit digit *= 10 return num ~~~python ## **부가설명** num에 99라는 값을 임의로 넣었다고 가정했을 때, 아래와 같은 연산이 진행이 된다. num = 99 digit =1 num += 1 =&gt; num = 100 #99라는 숫자에 1을 더했을 때, 10의 배수가 되는데, 10의 배수가 되지 않도록 만드는 것이 목표. num // digit % 10 == 0 (true) #100은 10의 배수 이므로 (num//digit) 값의 나머지가 0이 된다. 100 + 1 = 101 = num #100에서 바꾸어야 할 숫자는 10**0자리와 10**1자리 둘. 두 개중 하나가 해결되었다. 1 * 10 = 10 = digit #10**0의 자리수가 해결이 되었으므로 10**1자리로 포인트 이동. 101 // 10 % 10 == 0 (true) #101은 여전히 10의 배수, 몫의 정수 부분이 10이다. 101 + 10 = 111 = num #한 자리 수 올라간 digit 부분이 10*1 부분에 1을 삽입. 10 * 10 = 100 = digit 111 // 10 % 10 == 0 (false) #루프 break","link":"/2020/09/21/1%EA%B8%89-1%EC%B0%A8-04%EB%B2%88/"},{"title":"Cos Pro Python 1급 1차 06번 문제풀이","text":"Contents of table: 문제내용 정답코드 Topic:체스의 나이트 Review:2차원 panel 위에서 포인트 이동문제.소용돌이 알고리즘의 해결방법과 유사하게, direction 리스트를 정하고, point 를 정해진 조건에따라서 이동시켜서 결과값을 얻어낸다. 정답코드 12345678910111213def solution(pos): dx = [1,1,-1,-1,2,2,-2,-2] #체스말이 움직이는 거리와 방향을 나타낸다. dy = [2,-2,-2,2,1,-1,-1,1] #ex) (x,y) =&gt; (x+dx, y+dy) cx = ord(pos[0]) - ord(&quot;A&quot;) #ord()함수는 문자열의 unicode 값을 리턴해준다. #체스판 구조상, A와 1을 기준으로 잡고, unicode의 차이만큼을 current position으로 보겠다는 의미. cy = ord(pos[1]) - ord(&quot;0&quot;) - 1 ans = 0 for i in range(8): #나이트가 움직일 수 있는 전방위는 8가지이다. nx = cx + dx[i] #ex) (nx,ny) =&gt; (x+dx, y+dy) ny = cy + dy[i] if nx &gt;= 0 and nx &lt; 8 and ny &gt;= 0 and ny &lt; 8: # 0 &lt;= x,y &lt; 8 이어야지 체스판을 벗어나지 않는다. ans += 1 # nx, ny 가 체스판을 벗어나지 않을 경우, ans 변수에 카운트누적. return ans 문제내용체스에서 나이트(knight)는 아래 그림과 같이 동그라미로 표시된 8개의 방향중 한 곳으로 한 번에 이동이 가능합니다. 단, 나이트는 체스판 밖으로는 이동할 수 없습니다. 체스판의 각 칸의 위치는 다음과 같이 표기합니다.예를 들어, A번줄과 1번줄이 겹치는 부분은 ‘A1’이라고 합니다. 나이트의 위치 pos가 매개변수로 주어질 때, 나이트를 한 번 움직여서 이동할 수 있는 칸은 몇개인지 return 하도록 solution 함수를 완성해주세요. 매개변수 설명나이트의 위치 pos가 solution 함수의 매개변수로 주어집니다. pos는 A부터 H까지의 대문자 알파벳 하나와 1 이상 8이하의 정수 하나로 이루어진 두 글자 문자열입니다. 잘못된 위치가 주어지는 경우는 없습니다. return 값 설명나이트를 한 번 움직여서 이동할 수 있는 칸의 개수를 return 해주세요. 예시 pos return “A7” 3 예시 설명나이트가 A7 위치에 있으면 아래 그림과 같이 왼쪽으로는 이동하지 못하고, 오른쪽으로는 맨 위를 제외한 나머지 세 칸으로 이동 가능합니다.따라서, 3을 return 하면 됩니다.","link":"/2020/09/21/1%EA%B8%89-1%EC%B0%A8-06%EB%B2%88/"},{"title":"Project_Dacon_Corona_EDA(benchmark)","text":"제주 신용카드 빅데이터 경진대회▶ 해결해야 하는 문제 2020.04, 2020.07 기간 내 지역, 업종 별 월간 총 카드 사용 금액 예측 ▶ 데이터 요약 사용지역, 업종, 거주지역 등 준식별자로 구성된 신용카드 사용 내역 (출처: BC카드) 2020.07.28 09:00, 2020.04 기간 내 신용카드 사용 내역 데이터 공개 201901-202003.csv (2.07 GB) 2019.01 ~ 2020.03 기간 내 신용카드 사용 내역 데이터 202004.csv (116 MB) 2020.04 기간 내 신용카드 사용 내역 데이터 submission.csv (64 KB) 제출 양식 12import pandas as pd import numpy as np 123456import ospath = &quot;C:\\\\Eric\\\\Projects\\\\Dacon_Corona_TimeSeries&quot;file_list = os.listdir(path)print (&quot;file_list: {}&quot;.format(file_list)) file_list: ['201901-202003.csv', '202004.csv', 'submission.csv'] 123card = pd.read_csv(&quot;C:\\\\Eric\\\\Projects\\\\Dacon_Corona_TimeSeries\\\\201901-202003.csv&quot;)card_new = pd.read_csv(&quot;C:\\\\Eric\\\\Projects\\\\Dacon_Corona_TimeSeries\\\\202004.csv&quot;)sub = pd.read_csv(&quot;C:\\\\Eric\\\\Projects\\\\Dacon_Corona_TimeSeries\\\\submission.csv&quot;) 1card.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 24697792 entries, 0 to 24697791 Data columns (total 12 columns): REG_YYMM int64 CARD_SIDO_NM object CARD_CCG_NM object STD_CLSS_NM object HOM_SIDO_NM object HOM_CCG_NM object AGE object SEX_CTGO_CD int64 FLC int64 CSTMR_CNT int64 AMT int64 CNT int64 dtypes: int64(6), object(6) memory usage: 2.2+ GB 12# 01번째 열 : 2019년 1월부터 2020년 3월까지 월 단위. card['REG_YYMM'].unique() array([201901, 201902, 201903, 201904, 201905, 201906, 201907, 201908, 201909, 201910, 201911, 201912, 202001, 202002, 202003], dtype=int64) 12# 02번째 열 : 전국 시도 17개len(card['CARD_SIDO_NM'].unique()) 17 12# 03번째 열 : 전국 시군구 227개len(card['CARD_CCG_NM'].unique()) 227 12# 04번째 열 : 업종 41개 len(card['STD_CLSS_NM'].unique()) 41 12# 05번째 열 : 17개len(card['HOM_SIDO_NM'].unique()) 17 12# 06번째 열 : 227개 len(card['HOM_CCG_NM'].unique()) 227 12# 07번째 열 : 고객 나이대 card['AGE'].unique() array(['20s', '30s', '40s', '50s', '60s', '70s', '10s'], dtype=object) 12# 08번째 열 : 성별card['SEX_CTGO_CD'].unique() array([1, 2], dtype=int64) 12# 09번째 열 : 생애주기에 따른 구분 card['FLC'].unique() array([1, 2, 3, 4, 5], dtype=int64) 12# 10번째 열 : 이용고객수 (명)card['CSTMR_CNT'].describe() count 2.469779e+07 mean 6.196855e+01 std 3.559175e+02 min 3.000000e+00 25% 4.000000e+00 50% 8.000000e+00 75% 2.400000e+01 max 3.281300e+04 Name: CSTMR_CNT, dtype: float64 1card.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } REG_YYMM CARD_SIDO_NM CARD_CCG_NM STD_CLSS_NM HOM_SIDO_NM HOM_CCG_NM AGE SEX_CTGO_CD FLC CSTMR_CNT AMT CNT 0 201901 강원 강릉시 건강보조식품 소매업 강원 강릉시 20s 1 1 4 311200 4 1 201901 강원 강릉시 건강보조식품 소매업 강원 강릉시 30s 1 2 7 1374500 8 2 201901 강원 강릉시 건강보조식품 소매업 강원 강릉시 30s 2 2 6 818700 6 3 201901 강원 강릉시 건강보조식품 소매업 강원 강릉시 40s 1 3 4 1717000 5 4 201901 강원 강릉시 건강보조식품 소매업 강원 강릉시 40s 1 4 3 1047300 3 1 1 1card_new.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 1350322 entries, 0 to 1350321 Data columns (total 12 columns): REG_YYMM 1350322 non-null int64 CARD_SIDO_NM 1350322 non-null object CARD_CCG_NM 1345164 non-null object STD_CLSS_NM 1350322 non-null object HOM_SIDO_NM 1350322 non-null object HOM_CCG_NM 1342875 non-null object AGE 1350322 non-null object SEX_CTGO_CD 1350322 non-null int64 FLC 1350322 non-null int64 CSTMR_CNT 1350322 non-null int64 AMT 1350322 non-null int64 CNT 1350322 non-null int64 dtypes: int64(6), object(6) memory usage: 123.6+ MB 1sub.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 1394 entries, 0 to 1393 Data columns (total 5 columns): id 1394 non-null int64 REG_YYMM 1394 non-null int64 CARD_SIDO_NM 1394 non-null object STD_CLSS_NM 1394 non-null object AMT 1394 non-null int64 dtypes: int64(3), object(2) memory usage: 54.5+ KB 1 1 1 1 1","link":"/2020/08/24/Project_Dacon_Corona_TimeSeries/"},{"title":"Cos Pro Python 1급 1차 05번 문제풀이","text":"Contents of table: 문제내용 정답코드 Topic:소용돌이 수 Review:유명한 소용돌이 알고리즘 문제이다. 2차원 리스트 안에서, 원하는 대로 point 이동 알고리즘을 짤 수 있는가를 테스트하는 문제이다. 정답코드 12def in_range(i, j, n): # i와 j값이 0보다크고 n보다 작은지 확인 return 0 &lt;= i and i &lt; n and 0 &lt;= j and j &lt; n 12345678910111213141516171819202122def solution(n): pane = [[0 for j in range(n)] for i in range(n)] # n * n 크기의 panel 을 만든다. dy = [0, 1, 0, -1] # 2차원 리스트 panel 에서, (dy,dx)의 움직임은, (right, down, left, up)을 의미한다. dx = [1, 0, -1, 0] ci, cj = 0, 0 # 시작 포인트는 (0,0) num = 1 while in_range(ci, cj, n) and pane[ci][cj] == 0: # 1) in_range 함수를 활용, current point가 panel을 벗어나지 않음. 2) 이동하는 칸에 숫자가 채워져 있지 않음. for k in range(4): # dy, dx의 이동순서 if not in_range(ci, cj, n) or pane[ci][cj] != 0: break while True: # 1) pannel 범위안 2) 다음칸의 숫자가 비어있을 때, 무한루프 pane[ci][cj] = num # k = 0 일 때, panel의 범위 안에서 num +=1 만큼의 숫자를 각 칸에 입력 num += 1 ni = ci + dy[k] # next point는 current point 에서 (dy,dx) 만큼의 이동값을 가져간 결과이다. nj = cj + dx[k] if not in_range(ni, nj, n) or pane[ni][nj] != 0: # next point 가 pannel 을 벗어났을 경우, (down, left, up, right) 순서로 이동. ci += dy[(k + 1) % 4] # (k+1) % 4 는 1 2 3 0 cj += dx[(k + 1) % 4] break ci = ni cj = nj return pane 문제내용#문제5다음과 같이 n x n 크기의 격자에 1부터 n x n까지의 수가 하나씩 있습니다. 이때 수가 다음과 같은 순서로 배치되어있다면 이것을 n-소용돌이 수라고 부릅니다. 소용돌이 수에서 1행 1열부터 n 행 n 열까지 대각선상에 존재하는 수들의 합을 구해야 합니다. 위의 예에서 대각선상에 존재하는 수의 합은 15입니다.격자의 크기 n이 주어질 때 n-소용돌이 수의 대각선상에 존재하는 수들의 합을 return 하도록 solution 함수를 완성해주세요. 매개변수 설명격자의 크기 n이 solution 함수의 매개변수로 주어집니다. n은 1 이상 100 이하의 자연수입니다. return 값 설명n-소용돌이 수의 대각선상에 존재하는 수들의 합을 return 해주세요. 예시 n return 3 15 2 4 예시 설명예시 #1문제의 예와 같습니다. 예시 #21과 3을 더하여 4가 됩니다.","link":"/2020/09/21/1%EA%B8%89-1%EC%B0%A8-05%EB%B2%88/"},{"title":"Project_Kaggle_M5_EDA(benchmark)","text":"M5 Forecasting ChallengePart: Exploratory Data Analysis Topic: Wal-Mart Sales Prediction Details: US States( CA, TX, WI) which include item level, department, categories and store details. explanotory variables such as price, promotions, day of the week and speical events. 123456789import pandas as pdimport numpy as npimport matplotlib.pylab as pltimport seaborn as snsfrom itertools import cyclepd.set_option('max_columns', 50)plt.style.use('bmh')color_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color']) 12345678# # Read in the data# INPUT_DIR = '../input/m5-forecasting-accuracy'# cal = pd.read_csv(f'{INPUT_DIR}/calendar.csv')# stv = pd.read_csv(f'{INPUT_DIR}/sales_train_validation.csv')# ss = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')# sellp = pd.read_csv(f'{INPUT_DIR}/sell_prices.csv') 12345stv = pd.read_csv('C:\\\\Eric\\\\Projects\\\\Kaggle-M5\\Dataset\\\\sales_train_validation.csv')cal = pd.read_csv('C:\\\\Eric\\\\Projects\\\\Kaggle-M5\\Dataset\\\\calendar.csv')sellp = pd.read_csv('C:\\\\Eric\\\\Projects\\\\Kaggle-M5\\Dataset\\\\sell_prices.csv') We are given historic sales data in the sales_train_validation dataset. rows exist in this dataset for days d_1 to d_1913. We are given the department, category, state, and store id of the item. d_1914 - d_1941 represents the validation rows which we will predict in stage 1 d_1942 - d_1969 represents the evaluation rows which we will predict for the final competition standings. 1stv.head() Visualizing the data for a single item Lets take a random item that sell a lot and see how it’s sales look across the training data. FOODS_3_090_CA_3_validation sells a lot Note there are days where it appears the item is unavailable and sales flatline 1d_cols = [c for c in stv.columns if 'd_' in c] 12345678910111213141516171819# 아이템(행)들의 dayily sale 가 각 날짜 (열)에 입력되어 있다. d1~ d1941, d로 시작하는 모든 칼럼명을 list형태로 저장하였다. # Below we are chaining the following steps in pandas:# 1. Select the item.# 2. Set the id as the index, Keep only sales data columns# 3. Transform so it's a column# 4. Plot the data# 특정 id를 인덱스로 하고, 각 col이 가로로 정렬. # .T를 통해서, 가로행을 세로행으로 전환# .plot으로 plt 그래프를 그린다. stv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'] \\ .set_index('id')[d_cols] \\ .T \\ .plot(figsize=(15, 5), title='FOODS_3_090_CA_3 sales by &quot;d&quot; number', color=next(color_cycle))plt.legend('')plt.show() Merging the data with real dates We are given a calendar with additional information about past and future dates. The calendar data can be merged with our days data From this we can find weekly and annual trends 1234# Calendar data looks like this (only showing columns we care about for now)# Calendar 파일에서, 해당 리스트에 해당하는 col들만 표시 cal[['d','date','event_name_1','event_name_2', 'event_type_1','event_type_2', 'snap_CA']].head() 1stv.head() 12345678910# Merge calendar on our items' dataexample = stv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'][d_cols].Texample = example.rename(columns={8412:'FOODS_3_090_CA_3'}) # Name it correctlyexample = example.reset_index().rename(columns={'index': 'd'}) # make the index &quot;d&quot;example = example.merge(cal, how='left', validate='1:1')example.set_index('date')['FOODS_3_090_CA_3'] \\ .plot(figsize=(15, 5), color=next(color_cycle), title='FOODS_3_090_CA_3 sales by actual sale dates')plt.show() 123# FOODS_3_090_CA_3_validation id를 선택, col들을 호출, 해당 col 들을 가로에서 세로로 정렬 example = stv.loc[stv['id'] == 'FOODS_3_090_CA_3_validation'][d_cols].Texample.head() 12example = example.rename(columns={8412:'FOODS_3_090_CA_3'}) # Name it correctlyexample = example.reset_index().rename(columns={'index': 'd'}) # make the index &quot;d&quot; 1example.head() 12# Calendar 파일을 탐색 d 열과 date 열에 주목 cal.head() 12# Example 에 Calendar 를 Merge example = example.merge(cal, how='left', validate='1:1') 1example.head() 12345# Select more top selling examplesexample2 = stv.loc[stv['id'] == 'HOBBIES_1_234_CA_3_validation'][d_cols].Texample2 = example2.rename(columns={6324:'HOBBIES_1_234_CA_3'}) # Name it correctlyexample2 = example2.reset_index().rename(columns={'index': 'd'}) # make the index &quot;d&quot;example2 = example2.merge(cal, how='left', validate='1:1') 1234example3 = stv.loc[stv['id'] == 'HOUSEHOLD_1_118_CA_3_validation'][d_cols].Texample3 = example3.rename(columns={6776:'HOUSEHOLD_1_118_CA_3'}) # Name it correctlyexample3 = example3.reset_index().rename(columns={'index': 'd'}) # make the index &quot;d&quot;example3 = example3.merge(cal, how='left', validate='1:1') #Sales broken down by time variables Now that we have our example item lets see how it sells by: Day of the week Month Year 1234examples = ['FOODS_3_090_CA_3','HOBBIES_1_234_CA_3','HOUSEHOLD_1_118_CA_3']example_df = [example, example2, example3] 1example.head(5) 1234567891011121314151617181920212223242526272829examples = ['FOODS_3_090_CA_3','HOBBIES_1_234_CA_3','HOUSEHOLD_1_118_CA_3']example_df = [example, example2, example3]for i in [0, 1, 2]: fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3)) example_df[i].groupby('wday').mean()[examples[i]] \\ .plot(kind='line', title='average sale: day of week', lw=5, color=color_pal[0], ax=ax1) example_df[i].groupby('month').mean()[examples[i]] \\ .plot(kind='line', title='average sale: month', lw=5, color=color_pal[4], ax=ax2) example_df[i].groupby('year').mean()[examples[i]] \\ .plot(kind='line', lw=5, title='average sale: year', color=color_pal[2], ax=ax3) fig.suptitle(f'Trends for item: {examples[i]}', size=20, y=1.1) plt.tight_layout() plt.show() #Lets look at a lot of different items! Lets put it all together to plot 20 different items and their sales Some observations from these plots: It is common to see an item unavailable for a period of time. Some items only sell 1 or less in a day, making it very hard to predict. Other items show spikes in their demand (super bowl sunday?) possibly the “events” provided to us could help with these. 12345678twenty_examples = stv.sample(20, random_state=529) \\ .set_index('id')[d_cols] \\ .T \\ .merge(cal.set_index('d')['date'], left_index=True, right_index=True, validate='1:1') \\ .set_index('date') 1stv.sample(20, random_state=529).head(5) # 랜덤으로 샘플 id 고르기 12345678910fig, axs = plt.subplots(10, 2, figsize=(15, 20))axs = axs.flatten()ax_idx = 0for item in twenty_examples.columns: twenty_examples[item].plot(title=item, color=next(color_cycle), ax=axs[ax_idx]) ax_idx += 1plt.tight_layout()plt.show() 1fig, axs = plt.subplots(10,2, figsize=(15,20)) # 10행2열의 subplot 을 표시하기 1twenty_examples.columns # cols 는 현재 id list 와 같다. 1axs = axs.flatten() # axs 의 2차원 행열을 1차원으로 변환시켰다. 12345678ax_idx = 0 for item in twenty_examples.columns: twenty_examples[item].plot(title=item, color=next(color_cycle), ax=axs[ax_idx]) # ax_idx += 1 # ax_idx 를 활용해서 plot 각각에 번호를 부여할 것이다. plt.tight_layout()plt.show() Combined Sales over Time by Type We have several item types: Hobbies Household Foods Lets plot the total demand over time for each type 1stv['cat_id'].unique() 1234stv.groupby('cat_id').count()['id'] \\ .sort_values() \\ .plot(kind='barh', figsize=(15, 5), title='Count of Items by Category')plt.show() 1stv.columns # col 나열 1stv.groupby('cat_id').count() # cat id 종류를 찾아보기로 결정 1stv.groupby('cat_id').count()['id'] # groupby 활용해서 cat id 에 종류별 숫자 카운트 1234stv.groupby('cat_id').count()['id'] \\ .sort_values() \\ .plot(kind='barh', figsize=(15, 5), title='Count of Items by Category')plt.show() 123456789101112131415161718past_sales = stv.set_index('id')[d_cols] \\ .T \\ .merge(cal.set_index('d')['date'], left_index=True, right_index=True, validate='1:1') \\ .set_index('date')for i in stv['cat_id'].unique(): items_col = [c for c in past_sales.columns if i in c] past_sales[items_col] \\ .sum(axis=1) \\ .plot(figsize=(15, 5), alpha=0.8, title='Total Sales by Item Type')plt.legend(stv['cat_id'].unique())plt.show() 1past_sales.head() 123456789for i in stv['cat_id'].unique(): # 순서대로 category id Hobbies, Household, Foods 를 list items_col = [c for c in past_sales.columns if i in c] # past_sales 컬럼명에 있는 id 에 위에서 지정한 cat_id가 포함되어 있는 id만 선택 past_sales[items_col] \\ # 첫번째 카테고리인 Hobbies 가 포함된 id 들만 표시 .sum(axis=1) \\ # Hobbies 를 합계 .plot(figsize=(15, 5), alpha=0.8, title='Total Sales by Item Type')plt.legend(stv['cat_id'].unique())plt.show() Rollout of items being sold We can see the some items come into supply that previously didn’t exist. Similarly some items stop being sold completely. Lets plot the sales, but only count if item is selling or not selling (0 -&gt; not selling, &gt;0 -&gt; selling) This plot shows us that many items are being slowly introduced into inventory, so many of them will not register a sale at the beginning of the provided data. 1234567891011121314# clip() 함수 &lt;= np.clip(배열, 최소값 기준, 최대값 기준) 0보다 작은 수를 0으로 반환 # 아까의 sum 대신에 mean 함수를 넣어서 각 상품별 평균 sales 를 뽑았다. past_sales_clipped = past_sales.clip(0, 1)for i in stv['cat_id'].unique(): items_col = [c for c in past_sales.columns if i in c] (past_sales_clipped[items_col] \\ .mean(axis=1) * 100) \\ .plot(figsize=(15, 5), alpha=0.8, title='Inventory Sale Percentage by Date', style='.')plt.ylabel('% of Inventory with at least 1 sale')plt.legend(stv['cat_id'].unique())plt.show() Sales by StoreWe are provided data for 10 unique stores. What are the total sales by stores? Note that some stores are more steady than others. CA_2 seems to have a big change occur in 2015 1234567891011store_list = sellp['store_id'].unique() #앞서와 같은 방식으로, 이번에는 cat_id 가 아닌, store_id 로 정렬 for s in store_list: store_items = [c for c in past_sales.columns if s in c] past_sales[store_items] \\ .sum(axis=1) \\ .rolling(90).mean() \\ .plot(figsize=(15, 5), alpha=0.8, title='Rolling 90 Day Average Total Sales (10 stores)')plt.legend(store_list)plt.show() Looking at the same data a different way, we can plot a rolling 7 day total demand count by store. Note clearly that some stores have abrupt changes in their demand, it could be that the store expanded or a new competitor was built near by. Either way this is imporant to note when creating predictive models about demand pattern. 12345678910111213141516171819# week 기준으로 rolling mean 을 보기 위해서 rolling 7 을 하였다. fig, axes = plt.subplots(5, 2, figsize=(15, 10), sharex=True)axes = axes.flatten()ax_idx = 0for s in store_list: store_items = [c for c in past_sales.columns if s in c] past_sales[store_items] \\ .sum(axis=1) \\ .rolling(7).mean() \\ .plot(alpha=1, ax=axes[ax_idx], title=s, lw=3, color=next(color_cycle)) ax_idx += 1# plt.legend(store_list)plt.suptitle('Weekly Sale Trends by Store ID')plt.tight_layout()plt.show() Sale Prices We are given historical sale prices of each item. Lets take a look at our example item from before. It looks to me like the price of this item is growing. Different stores have different selling prices. 123456789101112131415fig, ax = plt.subplots(figsize=(15, 5))stores = []for store, d in sellp.query('item_id == &quot;FOODS_3_090&quot;').groupby('store_id'): d.plot(x='wm_yr_wk', y='sell_price', style='.', color=next(color_cycle), figsize=(15, 5), title='FOODS_3_090 sale price over time', ax=ax, legend=store) stores.append(store) plt.legend()plt.legend(stores)plt.show() 1sellp.head() # item 별 가격이 정리되어 있는 파일 1sellp['Category'] = sellp['item_id'].str.split('_', expand=True)[0] # Category 컬럼을 만들어서, 해당 칼럼에 아이템 id 의 unique value 를 부여한다. 1sellp.head() 12345678910111213sellp['Category'] = sellp['item_id'].str.split('_', expand=True)[0]fig, axs = plt.subplots(1, 3, figsize=(15, 4))i = 0for cat, d in sellp.groupby('Category'): ax = d['sell_price'].apply(np.log1p) \\ .plot(kind='hist', bins=20, title=f'Distribution of {cat} prices', ax=axs[i], color=next(color_cycle)) ax.set_xlabel('Log(price)') i += 1plt.tight_layout() 1sellp.groupby('Category').tail() 1sellp.Category.unique() 12for c in sellp.groupby('Category'): # groupby 함수와 for문을 함께 사용하면은 다음과 ('해당열의 고유값', '해당열의 고유값을 만족시키는 데이터의 집합') 으로 출력이 된다. print(c) Ref: https://www.kaggle.com/robikscube/m5-forecasting-starter-data-exploration/data Thanks you, Rob Mulla.","link":"/2020/09/17/Project_Kaggle_M5_EDA(benchmark)_02/"},{"title":"Kaggle M5 Competition","text":"Contents of table: 1. Fetch the data 2. Downcasting Kaggle M5 Competition Part 1 -EDA 미국 Wal-Mart 에서 주최한 매출예측 대회이다. 대회설명:M5는 월마트에서 제공하는 간헐적 수요가 지배하는 계층적 판매 데이터를 사용하여 향후 28 일 동안의 일일 판매를 예측하고 이러한 예측에 대한 불확실성 분포를 추정합니다.데이터에는 가격, 프로모션, 요일 및 특별 이벤트와 같은 설명 변수가 포함됩니다. 데이터셋:calendar.csv - 제품 판매 날짜에 대한 정보를 포함합니다.sales_train_validation.csv - 제품 및 매장 별 일일 판매량 기록 데이터 포함 [d_1-d_1913]sample_submission.csv - 제출 양식.sell_prices.csv - 상점 및 날짜별로 판매 된 제품의 가격에 대한 정보를 포함합니다.sales_train_evaluation.cs v - 제품 판매 포함 [d_1-d_1941] (공개 리더 보드에 사용되는 라벨) 12345678910111213import osimport pandas as pdimport numpy as npimport plotly_express as pximport plotly.graph_objects as gofrom plotly.subplots import make_subplotsimport matplotlib.pyplot as pltimport seaborn as snsimport gcimport warningswarnings.filterwarnings('ignore')from lightgbm import LGBMRegressorimport joblib 1. Fetch the data123456sales = pd.read_csv('C:\\\\Eric\\\\Projects\\\\Kaggle_M5\\Dataset\\\\sales_train_evaluation.csv')sales.name = 'sales'calendar = pd.read_csv('C:\\\\Eric\\\\Projects\\\\Kaggle_M5\\Dataset\\\\calendar.csv')calendar.name = 'calendar'prices = pd.read_csv('C:\\\\Eric\\\\Projects\\\\Kaggle_M5\\Dataset\\\\sell_prices.csv')prices.name = 'prices' 1sales.columns Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd_1', 'd_2', 'd_3', 'd_4', ... 'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937', 'd_1938', 'd_1939', 'd_1940', 'd_1941'], dtype='object', length=1947) 12345#빈 칸 처리되어있는 d 1942 ~ 1969 col들에 0 입력for d in range(1942,1970): col = 'd_' + str(d) sales[col] = 0 sales[col] = sales[col].astype(np.int16) 2. Downcasting1234#기본 데이터셋의 용량이 큰 만큼, 메모리 다운이 필요. sales_bd = np.round(sales.memory_usage().sum()/(1024*1024),1)calendar_bd = np.round(calendar.memory_usage().sum()/(1024*1024),1)prices_bd = np.round(prices.memory_usage().sum()/(1024*1024),1) 123456789101112131415161718192021222324252627#캐글의 memory downcasting 코드를 참고하여 아래와 같이 메모리 다운. def downcast(df): cols = df.dtypes.index.tolist() types = df.dtypes.values.tolist() for i,t in enumerate(types): if 'int' in str(t): if df[cols[i]].min() &gt; np.iinfo(np.int8).min and df[cols[i]].max() &lt; np.iinfo(np.int8).max: df[cols[i]] = df[cols[i]].astype(np.int8) elif df[cols[i]].min() &gt; np.iinfo(np.int16).min and df[cols[i]].max() &lt; np.iinfo(np.int16).max: df[cols[i]] = df[cols[i]].astype(np.int16) elif df[cols[i]].min() &gt; np.iinfo(np.int32).min and df[cols[i]].max() &lt; np.iinfo(np.int32).max: df[cols[i]] = df[cols[i]].astype(np.int32) else: df[cols[i]] = df[cols[i]].astype(np.int64) elif 'float' in str(t): if df[cols[i]].min() &gt; np.finfo(np.float16).min and df[cols[i]].max() &lt; np.finfo(np.float16).max: df[cols[i]] = df[cols[i]].astype(np.float16) elif df[cols[i]].min() &gt; np.finfo(np.float32).min and df[cols[i]].max() &lt; np.finfo(np.float32).max: df[cols[i]] = df[cols[i]].astype(np.float32) else: df[cols[i]] = df[cols[i]].astype(np.float64) elif t == np.object: if cols[i] == 'date': df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d') else: df[cols[i]] = df[cols[i]].astype('category') return df 123sales = downcast(sales)prices = downcast(prices)calendar = downcast(calendar) 1234#메모리 다운 후의 메모리 사용량 체크. sales_ad = np.round(sales.memory_usage().sum()/(1024*1024),1)calendar_ad = np.round(calendar.memory_usage().sum()/(1024*1024),1)prices_ad = np.round(prices.memory_usage().sum()/(1024*1024),1) 123456789101112#다운 캐스팅이 DataFrame의 메모리 사용량에 얼마나 많은 영향을 미쳤는지 시각화.1/4 미만으로 줄일 수 있음. dic = {'DataFrame':['sales','calendar','prices'], 'Before downcasting':[sales_bd,calendar_bd,prices_bd], 'After downcasting':[sales_ad,calendar_ad,prices_ad]}memory = pd.DataFrame(dic)memory = pd.melt(memory, id_vars='DataFrame', var_name='Status', value_name='Memory (MB)')memory.sort_values('Memory (MB)',inplace=True)fig = px.bar(memory, x='DataFrame', y='Memory (MB)', color='Status', barmode='group', text='Memory (MB)')fig.update_traces(texttemplate='%{text} MB', textposition='outside')fig.update_layout(template='seaborn', title='Effect of Downcasting')fig.show() 4. Exploratory Data AnalysisThe M5 dataset, generously made available by Walmart, involves the unit sales of various products sold in the USA, organized in the form of grouped time series. More specifically, the dataset involves the unit sales of 3,049 products, classified in 3 product categories (Hobbies, Foods, and Household) and 7 product departments, in which the above-mentioned categories are disaggregated. The products are sold across ten stores, located in three States (CA, TX, and WI). 12345678# walmart 에서 제공하는 세일즈 데이터는, wrt, 즉 with respect to [ cols ]# State: CA, WI, TX (3)# Store: CA_1, CA_2, TX_1, WI_1, ... (10)# Category: FOOD, HOBBIES, HOUSEHOLD (3) # Department:FOOD_1,2,3 , HOBBIES_1,2, ... (7)# item_id:: each unique id # (3,049) 1234567891011#plotly_express 에서 제공하는 treemap 을 활용해서, 각 제품 id 를 count var로 잡고, data col 들의 관계를 directory 형태로 시각화.group = sales.groupby(['state_id','store_id','cat_id','dept_id'],as_index=False)['item_id'].count().dropna()group['USA'] = 'United States of America'group.rename(columns={'state_id':'State','store_id':'Store','cat_id':'Category','dept_id':'Department','item_id':'Count'},inplace=True)fig = px.treemap(group, path=['USA', 'State', 'Store', 'Category', 'Department'], values='Count', color='Count', color_continuous_scale= px.colors.sequential.Sunset, title='Walmart: Distribution of items')fig.update_layout(template='seaborn')fig.show() 5. Melting the dataCurrently, the data is in three dataframes: sales, prices &amp; calendar. The sales dataframe contains daily sales data with days(d_1 - d_1969) as columns. The prices dataframe contains items’ price details and calendar contains data about the days d. #5.1 Convert from wide to long format 12#머신러닝 포맷에 적합시키기 위해서는 와이드 형식의 판매 데이터 프레임을 긴 형식으로 변환이 필요하다. sales 데이터셋의 row 는 30490(== # of items), 데이터셋을 melt하게되면은#sales, calendar 30490 x 1969 = 60034810 개의 row 를 가지게 된다. 1df = pd.melt(sales, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='sold').dropna() 12df = pd.merge(df, calendar, on='d', how='left')df = pd.merge(df, prices, on=['store_id','item_id','wm_yr_wk'], how='left') 1234567#Store 별로 매출액합계를 violin plot 을 활용해서 시각화. group = df.groupby(['year','date','state_id','store_id'], as_index=False)['sold'].sum().dropna()fig = px.violin(group, x='store_id', color='state_id', y='sold',box=True)fig.update_xaxes(title_text='Store')fig.update_yaxes(title_text='Total items sold')fig.update_layout(template='seaborn',title='Distribution of Items sold wrt Stores',legend_title_text='State')fig.show() 5. Feature Engineering1#5.1 Label Encoding 1234567#id, department, category, store, state 를 코드값으로 저장 d_id = dict(zip(df.id.cat.codes, df.id))d_item_id = dict(zip(df.item_id.cat.codes, df.item_id))d_dept_id = dict(zip(df.dept_id.cat.codes, df.dept_id))d_cat_id = dict(zip(df.cat_id.cat.codes, df.cat_id))d_store_id = dict(zip(df.store_id.cat.codes, df.store_id))d_state_id = dict(zip(df.state_id.cat.codes, df.state_id)) 12345678910111213#1gc.collect()#2df.d = df['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)cols = df.dtypes.index.tolist()types = df.dtypes.values.tolist()for i,type in enumerate(types): if type.name == 'category': df[cols[i]] = df[cols[i]].cat.codes #3df.drop('date',axis=1,inplace=True) 1import time 1#5.2 introduce lags 1234#lag col들을 추가lags = [1,2,3,6,12,24,36]for lag in lags: df['sold_lag_'+str(lag)] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['sold'].shift(lag).astype(np.float16) 1#5.3 Mean Encoding 1234567891011121314%time#판매량 평균을 wrt item, state, store, category, department 별로 col 생성 df['iteam_sold_avg'] = df.groupby('item_id')['sold'].transform('mean').astype(np.float16)df['state_sold_avg'] = df.groupby('state_id')['sold'].transform('mean').astype(np.float16)df['store_sold_avg'] = df.groupby('store_id')['sold'].transform('mean').astype(np.float16)df['cat_sold_avg'] = df.groupby('cat_id')['sold'].transform('mean').astype(np.float16)df['dept_sold_avg'] = df.groupby('dept_id')['sold'].transform('mean').astype(np.float16)df['cat_dept_sold_avg'] = df.groupby(['cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)df['store_item_sold_avg'] = df.groupby(['store_id','item_id'])['sold'].transform('mean').astype(np.float16)df['cat_item_sold_avg'] = df.groupby(['cat_id','item_id'])['sold'].transform('mean').astype(np.float16)df['dept_item_sold_avg'] = df.groupby(['dept_id','item_id'])['sold'].transform('mean').astype(np.float16)df['state_store_sold_avg'] = df.groupby(['state_id','store_id'])['sold'].transform('mean').astype(np.float16)df['state_store_cat_sold_avg'] = df.groupby(['state_id','store_id','cat_id'])['sold'].transform('mean').astype(np.float16)df['store_cat_dept_sold_avg'] = df.groupby(['store_id','cat_id','dept_id'])['sold'].transform('mean').astype(np.float16) Wall time: 1 ms 1#5.4 Rolling Window Statistics 1df['rolling_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.rolling(window=7).mean()).astype(np.float16) 1#5.5 Expanding Window Statistics 1df['expanding_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.expanding(2).mean()).astype(np.float16) 1#5.6 Trends 12345#Selling Trend는 간단하게, 평균보다 큰지 작은지 만을 비교. df['daily_avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','d'])['sold'].transform('mean').astype(np.float16)df['avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform('mean').astype(np.float16)df['selling_trend'] = (df['daily_avg_sold'] - df['avg_sold']).astype(np.float16)df.drop(['daily_avg_sold','avg_sold'],axis=1,inplace=True) 1#5.7 Save the data 12#lag 추가로 인해서, d 35까지 빈 row 들이 많이 발생했으므로 해당기간을 제외. df = df[df['d']&gt;=36] 1df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; Int64Index: 58967660 entries, 1067150 to 60034809 Data columns (total 43 columns): id int16 item_id int16 dept_id int8 cat_id int8 store_id int8 state_id int8 d int16 sold int16 wm_yr_wk int16 weekday int8 wday int8 month int8 year int16 event_name_1 int8 event_type_1 int8 event_name_2 int8 event_type_2 int8 snap_CA int8 snap_TX int8 snap_WI int8 sell_price float16 sold_lag_1 float16 sold_lag_2 float16 sold_lag_3 float16 sold_lag_6 float16 sold_lag_12 float16 sold_lag_24 float16 sold_lag_36 float16 iteam_sold_avg float16 state_sold_avg float16 store_sold_avg float16 cat_sold_avg float16 dept_sold_avg float16 cat_dept_sold_avg float16 store_item_sold_avg float16 cat_item_sold_avg float16 dept_item_sold_avg float16 state_store_sold_avg float16 state_store_cat_sold_avg float16 store_cat_dept_sold_avg float16 rolling_sold_mean float16 expanding_sold_mean float16 selling_trend float16 dtypes: float16(23), int16(6), int8(14) memory usage: 4.4 GB 123df.to_pickle('data.pkl')del dfgc.collect() 59 6. Modeling and Prediction1import time 123456%time data = pd.read_pickle('data.pkl') # FE후에 pickle 형태로 저장시켰던 데이터를 로드. valid = data[(data['d']&gt;=1914) &amp; (data['d']&lt;1942)][['id','d','sold']] # 1914 ~ 1942 validation periodtest = data[data['d']&gt;=1942][['id','d','sold']] # d &gt;= 1942 test and eval period eval_preds = test['sold'] # eval = testvalid_preds = valid['sold'] # val = val Wall time: 0 ns 12345678910111213141516171819202122232425262728293031#Get the store idsstores = sales.store_id.cat.codes.unique().tolist()for store in stores: #store 별로 나눠서 prediction 진행 df = data[data['store_id']==store] #Split the data X_train, y_train = df[df['d']&lt;1914].drop('sold',axis=1), df[df['d']&lt;1914]['sold'] X_valid, y_valid = df[(df['d']&gt;=1914) &amp; (df['d']&lt;1942)].drop('sold',axis=1), df[(df['d']&gt;=1914) &amp; (df['d']&lt;1942)]['sold'] X_test = df[df['d']&gt;=1942].drop('sold',axis=1) #Train and validate model = LGBMRegressor( n_estimators=1000, learning_rate=0.3, subsample=0.8, colsample_bytree=0.8, max_depth=8, num_leaves=50, min_child_weight=300 ) print('*****Prediction for Store: {}*****'.format(d_store_id[store])) model.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_valid,y_valid)], eval_metric='rmse', verbose=20, early_stopping_rounds=20) valid_preds[X_valid.index] = model.predict(X_valid) eval_preds[X_test.index] = model.predict(X_test) filename = 'model'+str(d_store_id[store])+'.pkl' # save model joblib.dump(model, filename) del model, X_train, y_train, X_valid, y_valid gc.collect() *****Prediction for Store: CA_1***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.843923 training's l2: 0.712206 valid_1's rmse: 0.556612 valid_1's l2: 0.309817 [40] training's rmse: 0.805702 training's l2: 0.649156 valid_1's rmse: 0.536648 valid_1's l2: 0.287992 [60] training's rmse: 0.782521 training's l2: 0.612339 valid_1's rmse: 0.529075 valid_1's l2: 0.27992 [80] training's rmse: 0.765509 training's l2: 0.586004 valid_1's rmse: 0.519001 valid_1's l2: 0.269362 [100] training's rmse: 0.746824 training's l2: 0.557746 valid_1's rmse: 0.516391 valid_1's l2: 0.26666 [120] training's rmse: 0.736669 training's l2: 0.542682 valid_1's rmse: 0.512239 valid_1's l2: 0.262389 [140] training's rmse: 0.725183 training's l2: 0.52589 valid_1's rmse: 0.507517 valid_1's l2: 0.257574 [160] training's rmse: 0.71879 training's l2: 0.516659 valid_1's rmse: 0.503054 valid_1's l2: 0.253063 [180] training's rmse: 0.713246 training's l2: 0.508719 valid_1's rmse: 0.501668 valid_1's l2: 0.25167 Early stopping, best iteration is: [177] training's rmse: 0.713815 training's l2: 0.509531 valid_1's rmse: 0.501194 valid_1's l2: 0.251195 *****Prediction for Store: CA_2***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.509193 training's l2: 0.259277 valid_1's rmse: 0.488679 valid_1's l2: 0.238808 [40] training's rmse: 0.476985 training's l2: 0.227515 valid_1's rmse: 0.481392 valid_1's l2: 0.231738 [60] training's rmse: 0.459124 training's l2: 0.210795 valid_1's rmse: 0.469844 valid_1's l2: 0.220753 [80] training's rmse: 0.446454 training's l2: 0.199321 valid_1's rmse: 0.466131 valid_1's l2: 0.217278 [100] training's rmse: 0.44062 training's l2: 0.194146 valid_1's rmse: 0.465138 valid_1's l2: 0.216353 [120] training's rmse: 0.435579 training's l2: 0.189729 valid_1's rmse: 0.462275 valid_1's l2: 0.213698 [140] training's rmse: 0.433312 training's l2: 0.187759 valid_1's rmse: 0.46174 valid_1's l2: 0.213204 [160] training's rmse: 0.430487 training's l2: 0.185319 valid_1's rmse: 0.461825 valid_1's l2: 0.213283 Early stopping, best iteration is: [149] training's rmse: 0.431706 training's l2: 0.18637 valid_1's rmse: 0.461223 valid_1's l2: 0.212727 *****Prediction for Store: CA_3***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 1.31768 training's l2: 1.73629 valid_1's rmse: 0.620532 valid_1's l2: 0.38506 [40] training's rmse: 1.25016 training's l2: 1.56289 valid_1's rmse: 0.599518 valid_1's l2: 0.359422 [60] training's rmse: 1.21357 training's l2: 1.47275 valid_1's rmse: 0.583401 valid_1's l2: 0.340357 [80] training's rmse: 1.18962 training's l2: 1.41519 valid_1's rmse: 0.580415 valid_1's l2: 0.336882 [100] training's rmse: 1.16704 training's l2: 1.36198 valid_1's rmse: 0.573824 valid_1's l2: 0.329274 Early stopping, best iteration is: [83] training's rmse: 1.18341 training's l2: 1.40046 valid_1's rmse: 0.571149 valid_1's l2: 0.326211 *****Prediction for Store: CA_4***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.379545 training's l2: 0.144055 valid_1's rmse: 0.306421 valid_1's l2: 0.0938936 [40] training's rmse: 0.362723 training's l2: 0.131568 valid_1's rmse: 0.296737 valid_1's l2: 0.0880528 [60] training's rmse: 0.352526 training's l2: 0.124275 valid_1's rmse: 0.286469 valid_1's l2: 0.0820644 [80] training's rmse: 0.347152 training's l2: 0.120515 valid_1's rmse: 0.283419 valid_1's l2: 0.0803261 [100] training's rmse: 0.342128 training's l2: 0.117052 valid_1's rmse: 0.279012 valid_1's l2: 0.0778477 [120] training's rmse: 0.339248 training's l2: 0.115089 valid_1's rmse: 0.27756 valid_1's l2: 0.0770398 [140] training's rmse: 0.336076 training's l2: 0.112947 valid_1's rmse: 0.27745 valid_1's l2: 0.0769786 Early stopping, best iteration is: [129] training's rmse: 0.337326 training's l2: 0.113789 valid_1's rmse: 0.276789 valid_1's l2: 0.0766123 *****Prediction for Store: TX_1***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.779231 training's l2: 0.607202 valid_1's rmse: 0.495078 valid_1's l2: 0.245102 [40] training's rmse: 0.734945 training's l2: 0.540143 valid_1's rmse: 0.477927 valid_1's l2: 0.228414 [60] training's rmse: 0.715 training's l2: 0.511225 valid_1's rmse: 0.474993 valid_1's l2: 0.225618 [80] training's rmse: 0.700945 training's l2: 0.491324 valid_1's rmse: 0.471686 valid_1's l2: 0.222487 [100] training's rmse: 0.688138 training's l2: 0.473534 valid_1's rmse: 0.469721 valid_1's l2: 0.220638 [120] training's rmse: 0.671506 training's l2: 0.45092 valid_1's rmse: 0.468799 valid_1's l2: 0.219772 Early stopping, best iteration is: [111] training's rmse: 0.678168 training's l2: 0.459912 valid_1's rmse: 0.466017 valid_1's l2: 0.217172 *****Prediction for Store: TX_2***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.949797 training's l2: 0.902115 valid_1's rmse: 0.519843 valid_1's l2: 0.270237 [40] training's rmse: 0.901254 training's l2: 0.812259 valid_1's rmse: 0.50753 valid_1's l2: 0.257587 [60] training's rmse: 0.860935 training's l2: 0.741208 valid_1's rmse: 0.496691 valid_1's l2: 0.246702 [80] training's rmse: 0.837279 training's l2: 0.701036 valid_1's rmse: 0.500869 valid_1's l2: 0.25087 Early stopping, best iteration is: [60] training's rmse: 0.860935 training's l2: 0.741208 valid_1's rmse: 0.496691 valid_1's l2: 0.246702 *****Prediction for Store: TX_3***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.741642 training's l2: 0.550033 valid_1's rmse: 0.569192 valid_1's l2: 0.323979 [40] training's rmse: 0.71047 training's l2: 0.504767 valid_1's rmse: 0.557032 valid_1's l2: 0.310284 [60] training's rmse: 0.68682 training's l2: 0.471721 valid_1's rmse: 0.546532 valid_1's l2: 0.298697 [80] training's rmse: 0.672727 training's l2: 0.452562 valid_1's rmse: 0.541006 valid_1's l2: 0.292688 [100] training's rmse: 0.66163 training's l2: 0.437754 valid_1's rmse: 0.539347 valid_1's l2: 0.290895 [120] training's rmse: 0.650395 training's l2: 0.423014 valid_1's rmse: 0.534985 valid_1's l2: 0.286208 [140] training's rmse: 0.645165 training's l2: 0.416238 valid_1's rmse: 0.532259 valid_1's l2: 0.2833 Early stopping, best iteration is: [132] training's rmse: 0.646645 training's l2: 0.418149 valid_1's rmse: 0.531403 valid_1's l2: 0.28239 *****Prediction for Store: WI_1***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.40387 training's l2: 0.163111 valid_1's rmse: 0.351971 valid_1's l2: 0.123884 [40] training's rmse: 0.379547 training's l2: 0.144056 valid_1's rmse: 0.339714 valid_1's l2: 0.115405 [60] training's rmse: 0.370228 training's l2: 0.137069 valid_1's rmse: 0.338534 valid_1's l2: 0.114605 [80] training's rmse: 0.362681 training's l2: 0.131537 valid_1's rmse: 0.335793 valid_1's l2: 0.112757 Early stopping, best iteration is: [75] training's rmse: 0.363574 training's l2: 0.132186 valid_1's rmse: 0.335287 valid_1's l2: 0.112418 *****Prediction for Store: WI_2***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.798844 training's l2: 0.638151 valid_1's rmse: 0.99757 valid_1's l2: 0.995147 [40] training's rmse: 0.75986 training's l2: 0.577388 valid_1's rmse: 0.979328 valid_1's l2: 0.959084 [60] training's rmse: 0.729671 training's l2: 0.53242 valid_1's rmse: 0.968394 valid_1's l2: 0.937787 Early stopping, best iteration is: [57] training's rmse: 0.732588 training's l2: 0.536685 valid_1's rmse: 0.967836 valid_1's l2: 0.936707 *****Prediction for Store: WI_3***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.803068 training's l2: 0.644919 valid_1's rmse: 0.580289 valid_1's l2: 0.336735 [40] training's rmse: 0.762335 training's l2: 0.581154 valid_1's rmse: 0.573159 valid_1's l2: 0.328512 [60] training's rmse: 0.739142 training's l2: 0.546331 valid_1's rmse: 0.566164 valid_1's l2: 0.320541 Early stopping, best iteration is: [51] training's rmse: 0.748455 training's l2: 0.560184 valid_1's rmse: 0.563976 valid_1's l2: 0.318069 1234567891011121314151617181920212223242526#Set actual equal to false if you want to top in the public leaderboard :Pactual = Falseif actual == False: #Get the validation results(We already have them as less than one month left for competition to end) validation = sales[['id']+['d_' + str(i) for i in range(1914,1942)]] validation['id']=pd.read_csv('C:\\\\Eric\\\\Projects\\\\Kaggle_M5\\Dataset\\\\sales_train_validation.csv').id validation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]else: #Get the actual validation results valid['sold'] = valid_preds validation = valid[['id','d','sold']] validation = pd.pivot(validation, index='id', columns='d', values='sold').reset_index() validation.columns=['id'] + ['F' + str(i + 1) for i in range(28)] validation.id = validation.id.map(d_id).str.replace('evaluation','validation')#Get the evaluation resultstest['sold'] = eval_predsevaluation = test[['id','d','sold']]evaluation = pd.pivot(evaluation, index='id', columns='d', values='sold').reset_index()evaluation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]#Remap the category id to their respective categoriesevaluation.id = evaluation.id.map(d_id)#Prepare the submissionsubmit = pd.concat([validation,evaluation]).reset_index(drop=True)submit.to_csv('M5_submission.csv',index=False) 1","link":"/2020/09/22/project_kaggle_m5_0922%20Modeling/"},{"title":"Kaggle M5 Competition Part 1 -EDA","text":"Contents of table: Kaggle M5 Competition Part 1 -EDA 1. Fetch the data 2. Downcasting 3. Exploratory Data Analysis Kaggle M5 Competition Part 1 -EDA 미국 Wal-Mart 에서 주최한 매출예측 대회이다. 대회설명:M5는 월마트에서 제공하는 간헐적 수요가 지배하는 계층적 판매 데이터를 사용하여 향후 28 일 동안의 일일 판매를 예측하고 이러한 예측에 대한 불확실성 분포를 추정합니다.데이터에는 가격, 프로모션, 요일 및 특별 이벤트와 같은 설명 변수가 포함됩니다. 데이터셋:calendar.csv - 제품 판매 날짜에 대한 정보를 포함합니다.sales_train_validation.csv - 제품 및 매장 별 일일 판매량 기록 데이터 포함 [d_1-d_1913]sample_submission.csv - 제출 양식.sell_prices.csv - 상점 및 날짜별로 판매 된 제품의 가격에 대한 정보를 포함합니다.sales_train_evaluation.cs v - 제품 판매 포함 [d_1-d_1941] (공개 리더 보드에 사용되는 라벨) 12345678910111213import osimport pandas as pdimport numpy as npimport plotly_express as pximport plotly.graph_objects as gofrom plotly.subplots import make_subplotsimport matplotlib.pyplot as pltimport seaborn as snsimport gcimport warningswarnings.filterwarnings('ignore')from lightgbm import LGBMRegressorimport joblib 1. Fetch the data123456sales = pd.read_csv('C:\\\\Eric\\\\Projects\\\\Kaggle_M5\\Dataset\\\\sales_train_evaluation.csv')sales.name = 'sales'calendar = pd.read_csv('C:\\\\Eric\\\\Projects\\\\Kaggle_M5\\Dataset\\\\calendar.csv')calendar.name = 'calendar'prices = pd.read_csv('C:\\\\Eric\\\\Projects\\\\Kaggle_M5\\Dataset\\\\sell_prices.csv')prices.name = 'prices' 1sales.columns Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd_1', 'd_2', 'd_3', 'd_4', ... 'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937', 'd_1938', 'd_1939', 'd_1940', 'd_1941'], dtype='object', length=1947) 12345#빈 칸 처리되어있는 d 1942 ~ 1969 col들에 0 입력for d in range(1942,1970): col = 'd_' + str(d) sales[col] = 0 sales[col] = sales[col].astype(np.int16) 2. Downcasting1234#기본 데이터셋의 용량이 큰 만큼, 메모리 다운이 필요. sales_bd = np.round(sales.memory_usage().sum()/(1024*1024),1)calendar_bd = np.round(calendar.memory_usage().sum()/(1024*1024),1)prices_bd = np.round(prices.memory_usage().sum()/(1024*1024),1) 123456789101112131415161718192021222324252627#캐글의 memory downcasting 코드를 참고하여 아래와 같이 메모리 다운. def downcast(df): cols = df.dtypes.index.tolist() types = df.dtypes.values.tolist() for i,t in enumerate(types): if 'int' in str(t): if df[cols[i]].min() &gt; np.iinfo(np.int8).min and df[cols[i]].max() &lt; np.iinfo(np.int8).max: df[cols[i]] = df[cols[i]].astype(np.int8) elif df[cols[i]].min() &gt; np.iinfo(np.int16).min and df[cols[i]].max() &lt; np.iinfo(np.int16).max: df[cols[i]] = df[cols[i]].astype(np.int16) elif df[cols[i]].min() &gt; np.iinfo(np.int32).min and df[cols[i]].max() &lt; np.iinfo(np.int32).max: df[cols[i]] = df[cols[i]].astype(np.int32) else: df[cols[i]] = df[cols[i]].astype(np.int64) elif 'float' in str(t): if df[cols[i]].min() &gt; np.finfo(np.float16).min and df[cols[i]].max() &lt; np.finfo(np.float16).max: df[cols[i]] = df[cols[i]].astype(np.float16) elif df[cols[i]].min() &gt; np.finfo(np.float32).min and df[cols[i]].max() &lt; np.finfo(np.float32).max: df[cols[i]] = df[cols[i]].astype(np.float32) else: df[cols[i]] = df[cols[i]].astype(np.float64) elif t == np.object: if cols[i] == 'date': df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d') else: df[cols[i]] = df[cols[i]].astype('category') return df 123sales = downcast(sales)prices = downcast(prices)calendar = downcast(calendar) 1234#메모리 다운 후의 메모리 사용량 체크. sales_ad = np.round(sales.memory_usage().sum()/(1024*1024),1)calendar_ad = np.round(calendar.memory_usage().sum()/(1024*1024),1)prices_ad = np.round(prices.memory_usage().sum()/(1024*1024),1) 123456789101112#다운 캐스팅이 DataFrame의 메모리 사용량에 얼마나 많은 영향을 미쳤는지 시각화.1/4 미만으로 줄일 수 있음. dic = {'DataFrame':['sales','calendar','prices'], 'Before downcasting':[sales_bd,calendar_bd,prices_bd], 'After downcasting':[sales_ad,calendar_ad,prices_ad]}memory = pd.DataFrame(dic)memory = pd.melt(memory, id_vars='DataFrame', var_name='Status', value_name='Memory (MB)')memory.sort_values('Memory (MB)',inplace=True)fig = px.bar(memory, x='DataFrame', y='Memory (MB)', color='Status', barmode='group', text='Memory (MB)')fig.update_traces(texttemplate='%{text} MB', textposition='outside')fig.update_layout(template='seaborn', title='Effect of Downcasting')fig.show() ![“c”](effect of downcasting.png) 3. Exploratory Data AnalysisThe M5 dataset, generously made available by Walmart, involves the unit sales of various products sold in the USA, organized in the form of grouped time series. More specifically, the dataset involves the unit sales of 3,049 products, classified in 3 product categories (Hobbies, Foods, and Household) and 7 product departments, in which the above-mentioned categories are disaggregated. The products are sold across ten stores, located in three States (CA, TX, and WI). 12345678# walmart 에서 제공하는 세일즈 데이터는, wrt, 즉 with respect to [ cols ]# State: CA, WI, TX (3)# Store: CA_1, CA_2, TX_1, WI_1, ... (10)# Category: FOOD, HOBBIES, HOUSEHOLD (3) # Department:FOOD_1,2,3 , HOBBIES_1,2, ... (7)# item_id:: each unique id # (3,049) 1234567891011#plotly_express 에서 제공하는 treemap 을 활용해서, 각 제품 id 를 count var로 잡고, data col 들의 관계를 directory 형태로 시각화.group = sales.groupby(['state_id','store_id','cat_id','dept_id'],as_index=False)['item_id'].count().dropna()group['USA'] = 'United States of America'group.rename(columns={'state_id':'State','store_id':'Store','cat_id':'Category','dept_id':'Department','item_id':'Count'},inplace=True)fig = px.treemap(group, path=['USA', 'State', 'Store', 'Category', 'Department'], values='Count', color='Count', color_continuous_scale= px.colors.sequential.Sunset, title='Walmart: Distribution of items')fig.update_layout(template='seaborn')fig.show() ![](C:\\Archaon\\blog\\hexo_blog\\themes\\icarus\\source\\img\\project\\m5\\distribution of items.png) 5. Melting the dataCurrently, the data is in three dataframes: sales, prices &amp; calendar. The sales dataframe contains daily sales data with days(d_1 - d_1969) as columns. The prices dataframe contains items’ price details and calendar contains data about the days d. #5.1 Convert from wide to long format 12#머신러닝 포맷에 적합시키기 위해서는 와이드 형식의 판매 데이터 프레임을 긴 형식으로 변환이 필요하다. sales 데이터셋의 row 는 30490(== # of items), 데이터셋을 melt하게되면은#sales, calendar 30490 x 1969 = 60034810 개의 row 를 가지게 된다. 1df = pd.melt(sales, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='sold').dropna() 12df = pd.merge(df, calendar, on='d', how='left')df = pd.merge(df, prices, on=['store_id','item_id','wm_yr_wk'], how='left') 1234567#Store 별로 매출액합계를 violin plot 을 활용해서 시각화. group = df.groupby(['year','date','state_id','store_id'], as_index=False)['sold'].sum().dropna()fig = px.violin(group, x='store_id', color='state_id', y='sold',box=True)fig.update_xaxes(title_text='Store')fig.update_yaxes(title_text='Total items sold')fig.update_layout(template='seaborn',title='Distribution of Items sold wrt Stores',legend_title_text='State')fig.show() ![](C:\\Archaon\\blog\\hexo_blog\\themes\\icarus\\source\\img\\project\\m5\\distribution of items sold wrt stores.png) 6. Feature Engineering1#5.1 Label Encoding 1234567#id, department, category, store, state 를 코드값으로 저장 d_id = dict(zip(df.id.cat.codes, df.id))d_item_id = dict(zip(df.item_id.cat.codes, df.item_id))d_dept_id = dict(zip(df.dept_id.cat.codes, df.dept_id))d_cat_id = dict(zip(df.cat_id.cat.codes, df.cat_id))d_store_id = dict(zip(df.store_id.cat.codes, df.store_id))d_state_id = dict(zip(df.state_id.cat.codes, df.state_id)) 12345678910111213#1gc.collect()#2df.d = df['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)cols = df.dtypes.index.tolist()types = df.dtypes.values.tolist()for i,type in enumerate(types): if type.name == 'category': df[cols[i]] = df[cols[i]].cat.codes #3df.drop('date',axis=1,inplace=True) 1import time 1#5.2 introduce lags 1234#lag col들을 추가lags = [1,2,3,6,12,24,36]for lag in lags: df['sold_lag_'+str(lag)] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['sold'].shift(lag).astype(np.float16) 1#5.3 Mean Encoding 1234567891011121314%time#판매량 평균을 wrt item, state, store, category, department 별로 col 생성 df['iteam_sold_avg'] = df.groupby('item_id')['sold'].transform('mean').astype(np.float16)df['state_sold_avg'] = df.groupby('state_id')['sold'].transform('mean').astype(np.float16)df['store_sold_avg'] = df.groupby('store_id')['sold'].transform('mean').astype(np.float16)df['cat_sold_avg'] = df.groupby('cat_id')['sold'].transform('mean').astype(np.float16)df['dept_sold_avg'] = df.groupby('dept_id')['sold'].transform('mean').astype(np.float16)df['cat_dept_sold_avg'] = df.groupby(['cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)df['store_item_sold_avg'] = df.groupby(['store_id','item_id'])['sold'].transform('mean').astype(np.float16)df['cat_item_sold_avg'] = df.groupby(['cat_id','item_id'])['sold'].transform('mean').astype(np.float16)df['dept_item_sold_avg'] = df.groupby(['dept_id','item_id'])['sold'].transform('mean').astype(np.float16)df['state_store_sold_avg'] = df.groupby(['state_id','store_id'])['sold'].transform('mean').astype(np.float16)df['state_store_cat_sold_avg'] = df.groupby(['state_id','store_id','cat_id'])['sold'].transform('mean').astype(np.float16)df['store_cat_dept_sold_avg'] = df.groupby(['store_id','cat_id','dept_id'])['sold'].transform('mean').astype(np.float16) Wall time: 1 ms 1#5.4 Rolling Window Statistics 1df['rolling_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.rolling(window=7).mean()).astype(np.float16) 1#5.5 Expanding Window Statistics 1df['expanding_sold_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.expanding(2).mean()).astype(np.float16) 1#5.6 Trends 12345#Selling Trend는 간단하게, 평균보다 큰지 작은지 만을 비교. df['daily_avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','d'])['sold'].transform('mean').astype(np.float16)df['avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform('mean').astype(np.float16)df['selling_trend'] = (df['daily_avg_sold'] - df['avg_sold']).astype(np.float16)df.drop(['daily_avg_sold','avg_sold'],axis=1,inplace=True) 1#5.7 Save the data 12#lag 추가로 인해서, d 35까지 빈 row 들이 많이 발생했으므로 해당기간을 제외. df = df[df['d']&gt;=36] 1df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; Int64Index: 58967660 entries, 1067150 to 60034809 Data columns (total 43 columns): id int16 item_id int16 dept_id int8 cat_id int8 store_id int8 state_id int8 d int16 sold int16 wm_yr_wk int16 weekday int8 wday int8 month int8 year int16 event_name_1 int8 event_type_1 int8 event_name_2 int8 event_type_2 int8 snap_CA int8 snap_TX int8 snap_WI int8 sell_price float16 sold_lag_1 float16 sold_lag_2 float16 sold_lag_3 float16 sold_lag_6 float16 sold_lag_12 float16 sold_lag_24 float16 sold_lag_36 float16 iteam_sold_avg float16 state_sold_avg float16 store_sold_avg float16 cat_sold_avg float16 dept_sold_avg float16 cat_dept_sold_avg float16 store_item_sold_avg float16 cat_item_sold_avg float16 dept_item_sold_avg float16 state_store_sold_avg float16 state_store_cat_sold_avg float16 store_cat_dept_sold_avg float16 rolling_sold_mean float16 expanding_sold_mean float16 selling_trend float16 dtypes: float16(23), int16(6), int8(14) memory usage: 4.4 GB 123df.to_pickle('data.pkl')del dfgc.collect() 59 6. Modeling and Prediction1import time 123456%time data = pd.read_pickle('data.pkl') # FE후에 pickle 형태로 저장시켰던 데이터를 로드. valid = data[(data['d']&gt;=1914) &amp; (data['d']&lt;1942)][['id','d','sold']] # 1914 ~ 1942 validation periodtest = data[data['d']&gt;=1942][['id','d','sold']] # d &gt;= 1942 test and eval period eval_preds = test['sold'] # eval = testvalid_preds = valid['sold'] # val = val Wall time: 0 ns 12345678910111213141516171819202122232425262728293031#Get the store idsstores = sales.store_id.cat.codes.unique().tolist()for store in stores: #store 별로 나눠서 prediction 진행 df = data[data['store_id']==store] #Split the data X_train, y_train = df[df['d']&lt;1914].drop('sold',axis=1), df[df['d']&lt;1914]['sold'] X_valid, y_valid = df[(df['d']&gt;=1914) &amp; (df['d']&lt;1942)].drop('sold',axis=1), df[(df['d']&gt;=1914) &amp; (df['d']&lt;1942)]['sold'] X_test = df[df['d']&gt;=1942].drop('sold',axis=1) #Train and validate model = LGBMRegressor( n_estimators=1000, learning_rate=0.3, subsample=0.8, colsample_bytree=0.8, max_depth=8, num_leaves=50, min_child_weight=300 ) print('*****Prediction for Store: {}*****'.format(d_store_id[store])) model.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_valid,y_valid)], eval_metric='rmse', verbose=20, early_stopping_rounds=20) valid_preds[X_valid.index] = model.predict(X_valid) eval_preds[X_test.index] = model.predict(X_test) filename = 'model'+str(d_store_id[store])+'.pkl' # save model joblib.dump(model, filename) del model, X_train, y_train, X_valid, y_valid gc.collect() *****Prediction for Store: CA_1***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.843923 training's l2: 0.712206 valid_1's rmse: 0.556612 valid_1's l2: 0.309817 [40] training's rmse: 0.805702 training's l2: 0.649156 valid_1's rmse: 0.536648 valid_1's l2: 0.287992 [60] training's rmse: 0.782521 training's l2: 0.612339 valid_1's rmse: 0.529075 valid_1's l2: 0.27992 [80] training's rmse: 0.765509 training's l2: 0.586004 valid_1's rmse: 0.519001 valid_1's l2: 0.269362 [100] training's rmse: 0.746824 training's l2: 0.557746 valid_1's rmse: 0.516391 valid_1's l2: 0.26666 [120] training's rmse: 0.736669 training's l2: 0.542682 valid_1's rmse: 0.512239 valid_1's l2: 0.262389 [140] training's rmse: 0.725183 training's l2: 0.52589 valid_1's rmse: 0.507517 valid_1's l2: 0.257574 [160] training's rmse: 0.71879 training's l2: 0.516659 valid_1's rmse: 0.503054 valid_1's l2: 0.253063 [180] training's rmse: 0.713246 training's l2: 0.508719 valid_1's rmse: 0.501668 valid_1's l2: 0.25167 Early stopping, best iteration is: [177] training's rmse: 0.713815 training's l2: 0.509531 valid_1's rmse: 0.501194 valid_1's l2: 0.251195 *****Prediction for Store: CA_2***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.509193 training's l2: 0.259277 valid_1's rmse: 0.488679 valid_1's l2: 0.238808 [40] training's rmse: 0.476985 training's l2: 0.227515 valid_1's rmse: 0.481392 valid_1's l2: 0.231738 [60] training's rmse: 0.459124 training's l2: 0.210795 valid_1's rmse: 0.469844 valid_1's l2: 0.220753 [80] training's rmse: 0.446454 training's l2: 0.199321 valid_1's rmse: 0.466131 valid_1's l2: 0.217278 [100] training's rmse: 0.44062 training's l2: 0.194146 valid_1's rmse: 0.465138 valid_1's l2: 0.216353 [120] training's rmse: 0.435579 training's l2: 0.189729 valid_1's rmse: 0.462275 valid_1's l2: 0.213698 [140] training's rmse: 0.433312 training's l2: 0.187759 valid_1's rmse: 0.46174 valid_1's l2: 0.213204 [160] training's rmse: 0.430487 training's l2: 0.185319 valid_1's rmse: 0.461825 valid_1's l2: 0.213283 Early stopping, best iteration is: [149] training's rmse: 0.431706 training's l2: 0.18637 valid_1's rmse: 0.461223 valid_1's l2: 0.212727 *****Prediction for Store: CA_3***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 1.31768 training's l2: 1.73629 valid_1's rmse: 0.620532 valid_1's l2: 0.38506 [40] training's rmse: 1.25016 training's l2: 1.56289 valid_1's rmse: 0.599518 valid_1's l2: 0.359422 [60] training's rmse: 1.21357 training's l2: 1.47275 valid_1's rmse: 0.583401 valid_1's l2: 0.340357 [80] training's rmse: 1.18962 training's l2: 1.41519 valid_1's rmse: 0.580415 valid_1's l2: 0.336882 [100] training's rmse: 1.16704 training's l2: 1.36198 valid_1's rmse: 0.573824 valid_1's l2: 0.329274 Early stopping, best iteration is: [83] training's rmse: 1.18341 training's l2: 1.40046 valid_1's rmse: 0.571149 valid_1's l2: 0.326211 *****Prediction for Store: CA_4***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.379545 training's l2: 0.144055 valid_1's rmse: 0.306421 valid_1's l2: 0.0938936 [40] training's rmse: 0.362723 training's l2: 0.131568 valid_1's rmse: 0.296737 valid_1's l2: 0.0880528 [60] training's rmse: 0.352526 training's l2: 0.124275 valid_1's rmse: 0.286469 valid_1's l2: 0.0820644 [80] training's rmse: 0.347152 training's l2: 0.120515 valid_1's rmse: 0.283419 valid_1's l2: 0.0803261 [100] training's rmse: 0.342128 training's l2: 0.117052 valid_1's rmse: 0.279012 valid_1's l2: 0.0778477 [120] training's rmse: 0.339248 training's l2: 0.115089 valid_1's rmse: 0.27756 valid_1's l2: 0.0770398 [140] training's rmse: 0.336076 training's l2: 0.112947 valid_1's rmse: 0.27745 valid_1's l2: 0.0769786 Early stopping, best iteration is: [129] training's rmse: 0.337326 training's l2: 0.113789 valid_1's rmse: 0.276789 valid_1's l2: 0.0766123 *****Prediction for Store: TX_1***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.779231 training's l2: 0.607202 valid_1's rmse: 0.495078 valid_1's l2: 0.245102 [40] training's rmse: 0.734945 training's l2: 0.540143 valid_1's rmse: 0.477927 valid_1's l2: 0.228414 [60] training's rmse: 0.715 training's l2: 0.511225 valid_1's rmse: 0.474993 valid_1's l2: 0.225618 [80] training's rmse: 0.700945 training's l2: 0.491324 valid_1's rmse: 0.471686 valid_1's l2: 0.222487 [100] training's rmse: 0.688138 training's l2: 0.473534 valid_1's rmse: 0.469721 valid_1's l2: 0.220638 [120] training's rmse: 0.671506 training's l2: 0.45092 valid_1's rmse: 0.468799 valid_1's l2: 0.219772 Early stopping, best iteration is: [111] training's rmse: 0.678168 training's l2: 0.459912 valid_1's rmse: 0.466017 valid_1's l2: 0.217172 *****Prediction for Store: TX_2***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.949797 training's l2: 0.902115 valid_1's rmse: 0.519843 valid_1's l2: 0.270237 [40] training's rmse: 0.901254 training's l2: 0.812259 valid_1's rmse: 0.50753 valid_1's l2: 0.257587 [60] training's rmse: 0.860935 training's l2: 0.741208 valid_1's rmse: 0.496691 valid_1's l2: 0.246702 [80] training's rmse: 0.837279 training's l2: 0.701036 valid_1's rmse: 0.500869 valid_1's l2: 0.25087 Early stopping, best iteration is: [60] training's rmse: 0.860935 training's l2: 0.741208 valid_1's rmse: 0.496691 valid_1's l2: 0.246702 *****Prediction for Store: TX_3***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.741642 training's l2: 0.550033 valid_1's rmse: 0.569192 valid_1's l2: 0.323979 [40] training's rmse: 0.71047 training's l2: 0.504767 valid_1's rmse: 0.557032 valid_1's l2: 0.310284 [60] training's rmse: 0.68682 training's l2: 0.471721 valid_1's rmse: 0.546532 valid_1's l2: 0.298697 [80] training's rmse: 0.672727 training's l2: 0.452562 valid_1's rmse: 0.541006 valid_1's l2: 0.292688 [100] training's rmse: 0.66163 training's l2: 0.437754 valid_1's rmse: 0.539347 valid_1's l2: 0.290895 [120] training's rmse: 0.650395 training's l2: 0.423014 valid_1's rmse: 0.534985 valid_1's l2: 0.286208 [140] training's rmse: 0.645165 training's l2: 0.416238 valid_1's rmse: 0.532259 valid_1's l2: 0.2833 Early stopping, best iteration is: [132] training's rmse: 0.646645 training's l2: 0.418149 valid_1's rmse: 0.531403 valid_1's l2: 0.28239 *****Prediction for Store: WI_1***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.40387 training's l2: 0.163111 valid_1's rmse: 0.351971 valid_1's l2: 0.123884 [40] training's rmse: 0.379547 training's l2: 0.144056 valid_1's rmse: 0.339714 valid_1's l2: 0.115405 [60] training's rmse: 0.370228 training's l2: 0.137069 valid_1's rmse: 0.338534 valid_1's l2: 0.114605 [80] training's rmse: 0.362681 training's l2: 0.131537 valid_1's rmse: 0.335793 valid_1's l2: 0.112757 Early stopping, best iteration is: [75] training's rmse: 0.363574 training's l2: 0.132186 valid_1's rmse: 0.335287 valid_1's l2: 0.112418 *****Prediction for Store: WI_2***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.798844 training's l2: 0.638151 valid_1's rmse: 0.99757 valid_1's l2: 0.995147 [40] training's rmse: 0.75986 training's l2: 0.577388 valid_1's rmse: 0.979328 valid_1's l2: 0.959084 [60] training's rmse: 0.729671 training's l2: 0.53242 valid_1's rmse: 0.968394 valid_1's l2: 0.937787 Early stopping, best iteration is: [57] training's rmse: 0.732588 training's l2: 0.536685 valid_1's rmse: 0.967836 valid_1's l2: 0.936707 *****Prediction for Store: WI_3***** Training until validation scores don't improve for 20 rounds [20] training's rmse: 0.803068 training's l2: 0.644919 valid_1's rmse: 0.580289 valid_1's l2: 0.336735 [40] training's rmse: 0.762335 training's l2: 0.581154 valid_1's rmse: 0.573159 valid_1's l2: 0.328512 [60] training's rmse: 0.739142 training's l2: 0.546331 valid_1's rmse: 0.566164 valid_1's l2: 0.320541 Early stopping, best iteration is: [51] training's rmse: 0.748455 training's l2: 0.560184 valid_1's rmse: 0.563976 valid_1's l2: 0.318069 1234567891011121314151617181920212223242526#Set actual equal to false if you want to top in the public leaderboard :Pactual = Falseif actual == False: #Get the validation results(We already have them as less than one month left for competition to end) validation = sales[['id']+['d_' + str(i) for i in range(1914,1942)]] validation['id']=pd.read_csv('C:\\\\Eric\\\\Projects\\\\Kaggle_M5\\Dataset\\\\sales_train_validation.csv').id validation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]else: #Get the actual validation results valid['sold'] = valid_preds validation = valid[['id','d','sold']] validation = pd.pivot(validation, index='id', columns='d', values='sold').reset_index() validation.columns=['id'] + ['F' + str(i + 1) for i in range(28)] validation.id = validation.id.map(d_id).str.replace('evaluation','validation')#Get the evaluation resultstest['sold'] = eval_predsevaluation = test[['id','d','sold']]evaluation = pd.pivot(evaluation, index='id', columns='d', values='sold').reset_index()evaluation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]#Remap the category id to their respective categoriesevaluation.id = evaluation.id.map(d_id)#Prepare the submissionsubmit = pd.concat([validation,evaluation]).reset_index(drop=True)submit.to_csv('M5_submission.csv',index=False) 1","link":"/2020/09/22/project_kaggle_m5_0922/"}],"tags":[{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Corona","slug":"Corona","link":"/tags/Corona/"}],"categories":[{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Cos Pro 1급","slug":"Python/Cos-Pro-1급","link":"/categories/Python/Cos-Pro-1%EA%B8%89/"},{"name":"Dacon","slug":"Dacon","link":"/categories/Dacon/"},{"name":"Kaggle","slug":"Kaggle","link":"/categories/Kaggle/"},{"name":"Corona","slug":"Dacon/Corona","link":"/categories/Dacon/Corona/"},{"name":"M5","slug":"Kaggle/M5","link":"/categories/Kaggle/M5/"}]}